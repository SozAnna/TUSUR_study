{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFyxekuoxd9H",
        "outputId": "23f11adb-b759-4bdf-dda7-291f8d387665"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VkX7dfDHSiUm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import tree\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "import xgboost as xgb\n",
        "!pip install catboost\n",
        "import catboost\n",
        "!pip install -U imbalanced-learn\n",
        "import imblearn\n",
        "\n",
        "import pylab\n",
        "pylab.rcParams['figure.figsize'] = (15, 10)\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ae_HidDexLUW"
      },
      "outputs": [],
      "source": [
        "itog = np.loadtxt('/content/drive/MyDrive/Small.csv', delimiter=\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-rycosTpdlp",
        "outputId": "68319cff-7195-422c-9eef-c45ab2994083"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(100000, 289)\n"
          ]
        }
      ],
      "source": [
        "print(itog.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13pMVVIWrG9j"
      },
      "source": [
        "Итак, мы имеем 100 тысяч объектов с 288 признаками"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "uXGU7s9gpqUl",
        "outputId": "66b7b266-0d60-4c10-ab97-9107c3cd63ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        0       1       2       3       4       5       6       7         8    \\\n",
              "0  281.1261  5.2019  4.9755  4.8077  4.6789  4.6716  4.4699  4.4669  272.5268   \n",
              "1  336.6726  5.0464  4.8182  4.7258  4.5225  4.5546  4.4657  4.5582  328.6995   \n",
              "2  290.2059  5.0463  4.7287  4.6207  4.5631  4.5707  4.4580  4.3681  255.7782   \n",
              "3  340.9563  5.1230  4.8917  4.5914  4.6239  4.5577  4.4663  4.4310  319.4096   \n",
              "4  276.8253  5.1806  4.8622  4.7099  4.5290  4.6399  4.3291  4.2316  257.5264   \n",
              "\n",
              "      9    ...     279     280     281     282     283     284     285  \\\n",
              "0  4.6808  ...  0.9458  0.0000 -0.6543  0.0427  0.4482  0.7899  1.0149   \n",
              "1  4.1517  ...  1.0008  0.0095  1.3937  1.3188  1.3582  1.2059  1.3286   \n",
              "2  3.7374  ...  1.5204 -0.0156  1.3963  1.3386  1.4358  1.4311  1.3677   \n",
              "3  4.6115  ...  1.7294  0.0000 -0.0617  1.1793  1.1271  1.3000  1.3077   \n",
              "4  3.8630  ...  1.6827  0.0000  0.3654  1.2722  1.4114  1.5722  1.5142   \n",
              "\n",
              "      286     287  288  \n",
              "0  1.1537  1.4483  0.0  \n",
              "1  1.4715  0.8529  0.0  \n",
              "2  1.6846  1.7238  0.0  \n",
              "3  1.5584  1.6281  0.0  \n",
              "4  1.1071  1.5082  0.0  \n",
              "\n",
              "[5 rows x 289 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-11ba911a-2426-479f-9168-f5f5269fd175\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "      <th>281</th>\n",
              "      <th>282</th>\n",
              "      <th>283</th>\n",
              "      <th>284</th>\n",
              "      <th>285</th>\n",
              "      <th>286</th>\n",
              "      <th>287</th>\n",
              "      <th>288</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>281.1261</td>\n",
              "      <td>5.2019</td>\n",
              "      <td>4.9755</td>\n",
              "      <td>4.8077</td>\n",
              "      <td>4.6789</td>\n",
              "      <td>4.6716</td>\n",
              "      <td>4.4699</td>\n",
              "      <td>4.4669</td>\n",
              "      <td>272.5268</td>\n",
              "      <td>4.6808</td>\n",
              "      <td>...</td>\n",
              "      <td>0.9458</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.6543</td>\n",
              "      <td>0.0427</td>\n",
              "      <td>0.4482</td>\n",
              "      <td>0.7899</td>\n",
              "      <td>1.0149</td>\n",
              "      <td>1.1537</td>\n",
              "      <td>1.4483</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>336.6726</td>\n",
              "      <td>5.0464</td>\n",
              "      <td>4.8182</td>\n",
              "      <td>4.7258</td>\n",
              "      <td>4.5225</td>\n",
              "      <td>4.5546</td>\n",
              "      <td>4.4657</td>\n",
              "      <td>4.5582</td>\n",
              "      <td>328.6995</td>\n",
              "      <td>4.1517</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0008</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>1.3937</td>\n",
              "      <td>1.3188</td>\n",
              "      <td>1.3582</td>\n",
              "      <td>1.2059</td>\n",
              "      <td>1.3286</td>\n",
              "      <td>1.4715</td>\n",
              "      <td>0.8529</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>290.2059</td>\n",
              "      <td>5.0463</td>\n",
              "      <td>4.7287</td>\n",
              "      <td>4.6207</td>\n",
              "      <td>4.5631</td>\n",
              "      <td>4.5707</td>\n",
              "      <td>4.4580</td>\n",
              "      <td>4.3681</td>\n",
              "      <td>255.7782</td>\n",
              "      <td>3.7374</td>\n",
              "      <td>...</td>\n",
              "      <td>1.5204</td>\n",
              "      <td>-0.0156</td>\n",
              "      <td>1.3963</td>\n",
              "      <td>1.3386</td>\n",
              "      <td>1.4358</td>\n",
              "      <td>1.4311</td>\n",
              "      <td>1.3677</td>\n",
              "      <td>1.6846</td>\n",
              "      <td>1.7238</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>340.9563</td>\n",
              "      <td>5.1230</td>\n",
              "      <td>4.8917</td>\n",
              "      <td>4.5914</td>\n",
              "      <td>4.6239</td>\n",
              "      <td>4.5577</td>\n",
              "      <td>4.4663</td>\n",
              "      <td>4.4310</td>\n",
              "      <td>319.4096</td>\n",
              "      <td>4.6115</td>\n",
              "      <td>...</td>\n",
              "      <td>1.7294</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0617</td>\n",
              "      <td>1.1793</td>\n",
              "      <td>1.1271</td>\n",
              "      <td>1.3000</td>\n",
              "      <td>1.3077</td>\n",
              "      <td>1.5584</td>\n",
              "      <td>1.6281</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>276.8253</td>\n",
              "      <td>5.1806</td>\n",
              "      <td>4.8622</td>\n",
              "      <td>4.7099</td>\n",
              "      <td>4.5290</td>\n",
              "      <td>4.6399</td>\n",
              "      <td>4.3291</td>\n",
              "      <td>4.2316</td>\n",
              "      <td>257.5264</td>\n",
              "      <td>3.8630</td>\n",
              "      <td>...</td>\n",
              "      <td>1.6827</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.3654</td>\n",
              "      <td>1.2722</td>\n",
              "      <td>1.4114</td>\n",
              "      <td>1.5722</td>\n",
              "      <td>1.5142</td>\n",
              "      <td>1.1071</td>\n",
              "      <td>1.5082</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 289 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-11ba911a-2426-479f-9168-f5f5269fd175')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-11ba911a-2426-479f-9168-f5f5269fd175 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-11ba911a-2426-479f-9168-f5f5269fd175');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df = pd.DataFrame(itog, columns=None)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8_ig5BDsOww"
      },
      "source": [
        "Бросается в глаза то, что признаки имеют значения в разных диапазонах, а значит нам понадобится нормирование, если мы хотим использовать что-то, кроме деревьев (а мы хотим)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pYJv-CfrvpN",
        "outputId": "cf24d559-f395-4756-e4b6-af55d6ea0cd7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0      0\n",
              "1      0\n",
              "2      0\n",
              "3      0\n",
              "4      0\n",
              "      ..\n",
              "284    0\n",
              "285    0\n",
              "286    0\n",
              "287    0\n",
              "288    0\n",
              "Length: 289, dtype: int64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wnEMStIsK6b"
      },
      "source": [
        "Кажется, пропусков нет.\n",
        "\n",
        "Посмотрим еще на баланс классов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKy2p9xp6HeR",
        "outputId": "642789a7-4573-4fd6-aa47-8824e7432b8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Examples:\n",
            "    Total: 100000\n",
            "    Positive: 8665 (8.66% of total)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "neg, pos = df[288].value_counts()\n",
        "total = neg + pos\n",
        "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
        "    total, pos, 100 * pos / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "cLPEY8_AwoH-",
        "outputId": "a4c265ba-3251-428f-ef06-c1ae6ab9f49b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-1974dea6f1a4>:4: MatplotlibDeprecationWarning: Using a string of single character colors as a color sequence is deprecated. Use an explicit list instead.\n",
            "  ax.bar(np.arange(len(column)), column, color='cm')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x720 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "column = df[288].value_counts()\n",
        "names = df[288].value_counts().index.tolist()\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(np.arange(len(column)), column, color='cm')\n",
        "ax.set_xticks(np.arange(len(names)))\n",
        "ax.set_xticklabels(names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlxGrTSZxNZm"
      },
      "source": [
        "Мы видим, что объектов класса 0 примерно в 10 раз больше, чем объектов класса 1, т.е. наблюдается дисбаланс классов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RpkkJ6HpLv6R"
      },
      "outputs": [],
      "source": [
        "out = itog[:,288]\n",
        "itog = itog[:,:288]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2BW6dPbmOGcU"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = tf.cast(K.sum(K.round(K.clip(y_true * y_pred, 0, 1))), tf.float32)\n",
        "    possible_positives = tf.cast(K.sum(K.round(K.clip(y_true, 0, 1))), tf.float32)\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = tf.cast(K.sum(K.round(K.clip(y_true * y_pred, 0, 1))), tf.float32)\n",
        "    predicted_positives = tf.cast(K.sum(K.round(K.clip(y_pred, 0, 1))), tf.float32)\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GMZmCL_c5S8-"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(itog, out, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wOCz4pf9vKk"
      },
      "source": [
        "Вроде по науке нормализацию надо проводить по тренировочным данным и fit к тестовым применять на основе обученного на тренировочных данных скейлера."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Tb7QqQkT9uvb"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8Xa7Znj6mnn"
      },
      "source": [
        "Посчитаем веса для классов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqNgW2-m0A-o",
        "outputId": "2eeef44d-e1a7-496f-fd5f-a9926cbf9985"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight for class 0: 0.55\n",
            "Weight for class 1: 5.77\n"
          ]
        }
      ],
      "source": [
        "# вдохновлено этим\n",
        "# https://www.tensorflow.org/tutorials/structured_data/imbalanced_data\n",
        "\n",
        "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
        "# The sum of the weights of all examples stays the same.\n",
        "weight_for_0 = (1 / neg) * (total / 2.0)\n",
        "weight_for_1 = (1 / pos) * (total / 2.0)\n",
        "\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
        "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCW-VvgFFxYL"
      },
      "source": [
        "Начнем с нейронной сети с линейными слоями. Для борьбы с переобучением добавим слой Dropout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CFn-kNHwEHJF"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_shape=(288,)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=[f1_m])\n",
        "\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=\"mymodel_{epoch}\", monitor='val_loss', verbose=1, save_best_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq5hh9Xj7jt2"
      },
      "source": [
        "В fit добавим веса классов.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CyS0GPTHti0",
        "outputId": "1f260810-bb14-4422-8825-2d36782685ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.6987 - f1_m: 0.2086\n",
            "Epoch 1: val_loss improved from inf to 0.49224, saving model to mymodel_1\n",
            "80/80 [==============================] - 4s 30ms/step - loss: 0.6981 - f1_m: 0.2088 - val_loss: 0.4922 - val_f1_m: 0.4796\n",
            "Epoch 2/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.5235 - f1_m: 0.3225\n",
            "Epoch 2: val_loss improved from 0.49224 to 0.34553, saving model to mymodel_2\n",
            "80/80 [==============================] - 2s 28ms/step - loss: 0.5230 - f1_m: 0.3234 - val_loss: 0.3455 - val_f1_m: 0.5806\n",
            "Epoch 3/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.4254 - f1_m: 0.4067\n",
            "Epoch 3: val_loss improved from 0.34553 to 0.25663, saving model to mymodel_3\n",
            "80/80 [==============================] - 2s 25ms/step - loss: 0.4254 - f1_m: 0.4067 - val_loss: 0.2566 - val_f1_m: 0.6477\n",
            "Epoch 4/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.3577 - f1_m: 0.4749\n",
            "Epoch 4: val_loss improved from 0.25663 to 0.21548, saving model to mymodel_4\n",
            "80/80 [==============================] - 2s 25ms/step - loss: 0.3577 - f1_m: 0.4749 - val_loss: 0.2155 - val_f1_m: 0.6849\n",
            "Epoch 5/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.3154 - f1_m: 0.5255\n",
            "Epoch 5: val_loss improved from 0.21548 to 0.18946, saving model to mymodel_5\n",
            "80/80 [==============================] - 2s 24ms/step - loss: 0.3149 - f1_m: 0.5255 - val_loss: 0.1895 - val_f1_m: 0.7109\n",
            "Epoch 6/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.2768 - f1_m: 0.5666\n",
            "Epoch 6: val_loss improved from 0.18946 to 0.15733, saving model to mymodel_6\n",
            "80/80 [==============================] - 2s 24ms/step - loss: 0.2768 - f1_m: 0.5666 - val_loss: 0.1573 - val_f1_m: 0.7571\n",
            "Epoch 7/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.2527 - f1_m: 0.5925\n",
            "Epoch 7: val_loss improved from 0.15733 to 0.12964, saving model to mymodel_7\n",
            "80/80 [==============================] - 2s 24ms/step - loss: 0.2520 - f1_m: 0.5935 - val_loss: 0.1296 - val_f1_m: 0.7962\n",
            "Epoch 8/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.2281 - f1_m: 0.6232\n",
            "Epoch 8: val_loss improved from 0.12964 to 0.10479, saving model to mymodel_8\n",
            "80/80 [==============================] - 2s 24ms/step - loss: 0.2281 - f1_m: 0.6232 - val_loss: 0.1048 - val_f1_m: 0.8418\n",
            "Epoch 9/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.2058 - f1_m: 0.6510\n",
            "Epoch 9: val_loss did not improve from 0.10479\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.2055 - f1_m: 0.6525 - val_loss: 0.1126 - val_f1_m: 0.8201\n",
            "Epoch 10/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.1934 - f1_m: 0.6726\n",
            "Epoch 10: val_loss improved from 0.10479 to 0.10158, saving model to mymodel_10\n",
            "80/80 [==============================] - 2s 26ms/step - loss: 0.1934 - f1_m: 0.6733 - val_loss: 0.1016 - val_f1_m: 0.8405\n",
            "Epoch 11/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.1787 - f1_m: 0.6909\n",
            "Epoch 11: val_loss improved from 0.10158 to 0.09053, saving model to mymodel_11\n",
            "80/80 [==============================] - 2s 26ms/step - loss: 0.1787 - f1_m: 0.6913 - val_loss: 0.0905 - val_f1_m: 0.8576\n",
            "Epoch 12/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.1668 - f1_m: 0.7107\n",
            "Epoch 12: val_loss improved from 0.09053 to 0.08219, saving model to mymodel_12\n",
            "80/80 [==============================] - 2s 30ms/step - loss: 0.1668 - f1_m: 0.7107 - val_loss: 0.0822 - val_f1_m: 0.8727\n",
            "Epoch 13/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.1587 - f1_m: 0.7212\n",
            "Epoch 13: val_loss improved from 0.08219 to 0.08082, saving model to mymodel_13\n",
            "80/80 [==============================] - 2s 25ms/step - loss: 0.1586 - f1_m: 0.7206 - val_loss: 0.0808 - val_f1_m: 0.8718\n",
            "Epoch 14/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.1434 - f1_m: 0.7461\n",
            "Epoch 14: val_loss improved from 0.08082 to 0.07070, saving model to mymodel_14\n",
            "80/80 [==============================] - 2s 24ms/step - loss: 0.1433 - f1_m: 0.7452 - val_loss: 0.0707 - val_f1_m: 0.8859\n",
            "Epoch 15/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.1378 - f1_m: 0.7502\n",
            "Epoch 15: val_loss did not improve from 0.07070\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.1381 - f1_m: 0.7504 - val_loss: 0.0726 - val_f1_m: 0.8811\n",
            "Epoch 16/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.1306 - f1_m: 0.7683\n",
            "Epoch 16: val_loss improved from 0.07070 to 0.06393, saving model to mymodel_16\n",
            "80/80 [==============================] - 2s 25ms/step - loss: 0.1308 - f1_m: 0.7684 - val_loss: 0.0639 - val_f1_m: 0.8999\n",
            "Epoch 17/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.1245 - f1_m: 0.7685\n",
            "Epoch 17: val_loss improved from 0.06393 to 0.06161, saving model to mymodel_17\n",
            "80/80 [==============================] - 2s 25ms/step - loss: 0.1235 - f1_m: 0.7702 - val_loss: 0.0616 - val_f1_m: 0.9021\n",
            "Epoch 18/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.1205 - f1_m: 0.7780\n",
            "Epoch 18: val_loss improved from 0.06161 to 0.05862, saving model to mymodel_18\n",
            "80/80 [==============================] - 2s 24ms/step - loss: 0.1203 - f1_m: 0.7782 - val_loss: 0.0586 - val_f1_m: 0.9135\n",
            "Epoch 19/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.1085 - f1_m: 0.7975\n",
            "Epoch 19: val_loss improved from 0.05862 to 0.05796, saving model to mymodel_19\n",
            "80/80 [==============================] - 2s 25ms/step - loss: 0.1082 - f1_m: 0.7971 - val_loss: 0.0580 - val_f1_m: 0.9014\n",
            "Epoch 20/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.1095 - f1_m: 0.7988\n",
            "Epoch 20: val_loss improved from 0.05796 to 0.05574, saving model to mymodel_20\n",
            "80/80 [==============================] - 2s 25ms/step - loss: 0.1095 - f1_m: 0.7987 - val_loss: 0.0557 - val_f1_m: 0.9122\n",
            "Epoch 21/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.1043 - f1_m: 0.8045\n",
            "Epoch 21: val_loss improved from 0.05574 to 0.04804, saving model to mymodel_21\n",
            "80/80 [==============================] - 3s 33ms/step - loss: 0.1043 - f1_m: 0.8045 - val_loss: 0.0480 - val_f1_m: 0.9253\n",
            "Epoch 22/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0971 - f1_m: 0.8203\n",
            "Epoch 22: val_loss improved from 0.04804 to 0.04793, saving model to mymodel_22\n",
            "80/80 [==============================] - 2s 25ms/step - loss: 0.0969 - f1_m: 0.8204 - val_loss: 0.0479 - val_f1_m: 0.9253\n",
            "Epoch 23/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0961 - f1_m: 0.8186\n",
            "Epoch 23: val_loss improved from 0.04793 to 0.04788, saving model to mymodel_23\n",
            "80/80 [==============================] - 2s 25ms/step - loss: 0.0964 - f1_m: 0.8181 - val_loss: 0.0479 - val_f1_m: 0.9279\n",
            "Epoch 24/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0899 - f1_m: 0.8302\n",
            "Epoch 24: val_loss improved from 0.04788 to 0.04198, saving model to mymodel_24\n",
            "80/80 [==============================] - 2s 24ms/step - loss: 0.0895 - f1_m: 0.8306 - val_loss: 0.0420 - val_f1_m: 0.9316\n",
            "Epoch 25/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0876 - f1_m: 0.8324\n",
            "Epoch 25: val_loss did not improve from 0.04198\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0875 - f1_m: 0.8328 - val_loss: 0.0426 - val_f1_m: 0.9287\n",
            "Epoch 26/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.0862 - f1_m: 0.8356\n",
            "Epoch 26: val_loss improved from 0.04198 to 0.04145, saving model to mymodel_26\n",
            "80/80 [==============================] - 2s 25ms/step - loss: 0.0857 - f1_m: 0.8354 - val_loss: 0.0415 - val_f1_m: 0.9331\n",
            "Epoch 27/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0811 - f1_m: 0.8469\n",
            "Epoch 27: val_loss improved from 0.04145 to 0.04139, saving model to mymodel_27\n",
            "80/80 [==============================] - 2s 24ms/step - loss: 0.0814 - f1_m: 0.8463 - val_loss: 0.0414 - val_f1_m: 0.9311\n",
            "Epoch 28/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0768 - f1_m: 0.8529\n",
            "Epoch 28: val_loss improved from 0.04139 to 0.03877, saving model to mymodel_28\n",
            "80/80 [==============================] - 2s 25ms/step - loss: 0.0770 - f1_m: 0.8526 - val_loss: 0.0388 - val_f1_m: 0.9382\n",
            "Epoch 29/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.0745 - f1_m: 0.8551\n",
            "Epoch 29: val_loss improved from 0.03877 to 0.03512, saving model to mymodel_29\n",
            "80/80 [==============================] - 2s 24ms/step - loss: 0.0740 - f1_m: 0.8558 - val_loss: 0.0351 - val_f1_m: 0.9459\n",
            "Epoch 30/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.0719 - f1_m: 0.8573\n",
            "Epoch 30: val_loss improved from 0.03512 to 0.03446, saving model to mymodel_30\n",
            "80/80 [==============================] - 2s 29ms/step - loss: 0.0720 - f1_m: 0.8577 - val_loss: 0.0345 - val_f1_m: 0.9440\n",
            "Epoch 31/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0747 - f1_m: 0.8559\n",
            "Epoch 31: val_loss did not improve from 0.03446\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0747 - f1_m: 0.8559 - val_loss: 0.0363 - val_f1_m: 0.9420\n",
            "Epoch 32/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.0672 - f1_m: 0.8654\n",
            "Epoch 32: val_loss did not improve from 0.03446\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0677 - f1_m: 0.8645 - val_loss: 0.0354 - val_f1_m: 0.9457\n",
            "Epoch 33/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0662 - f1_m: 0.8723\n",
            "Epoch 33: val_loss improved from 0.03446 to 0.03420, saving model to mymodel_33\n",
            "80/80 [==============================] - 2s 25ms/step - loss: 0.0666 - f1_m: 0.8721 - val_loss: 0.0342 - val_f1_m: 0.9460\n",
            "Epoch 34/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0654 - f1_m: 0.8657\n",
            "Epoch 34: val_loss improved from 0.03420 to 0.03144, saving model to mymodel_34\n",
            "80/80 [==============================] - 2s 26ms/step - loss: 0.0653 - f1_m: 0.8659 - val_loss: 0.0314 - val_f1_m: 0.9500\n",
            "Epoch 35/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.0644 - f1_m: 0.8752\n",
            "Epoch 35: val_loss improved from 0.03144 to 0.03075, saving model to mymodel_35\n",
            "80/80 [==============================] - 2s 25ms/step - loss: 0.0649 - f1_m: 0.8749 - val_loss: 0.0308 - val_f1_m: 0.9501\n",
            "Epoch 36/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0619 - f1_m: 0.8748\n",
            "Epoch 36: val_loss did not improve from 0.03075\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0628 - f1_m: 0.8747 - val_loss: 0.0314 - val_f1_m: 0.9466\n",
            "Epoch 37/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0643 - f1_m: 0.8698\n",
            "Epoch 37: val_loss improved from 0.03075 to 0.02916, saving model to mymodel_37\n",
            "80/80 [==============================] - 2s 25ms/step - loss: 0.0643 - f1_m: 0.8698 - val_loss: 0.0292 - val_f1_m: 0.9534\n",
            "Epoch 38/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0576 - f1_m: 0.8863\n",
            "Epoch 38: val_loss improved from 0.02916 to 0.02907, saving model to mymodel_38\n",
            "80/80 [==============================] - 2s 24ms/step - loss: 0.0583 - f1_m: 0.8861 - val_loss: 0.0291 - val_f1_m: 0.9517\n",
            "Epoch 39/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.0619 - f1_m: 0.8834\n",
            "Epoch 39: val_loss did not improve from 0.02907\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0613 - f1_m: 0.8836 - val_loss: 0.0330 - val_f1_m: 0.9443\n",
            "Epoch 40/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0589 - f1_m: 0.8831\n",
            "Epoch 40: val_loss did not improve from 0.02907\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0591 - f1_m: 0.8825 - val_loss: 0.0302 - val_f1_m: 0.9492\n",
            "Epoch 41/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0545 - f1_m: 0.8868\n",
            "Epoch 41: val_loss improved from 0.02907 to 0.02802, saving model to mymodel_41\n",
            "80/80 [==============================] - 2s 26ms/step - loss: 0.0548 - f1_m: 0.8856 - val_loss: 0.0280 - val_f1_m: 0.9533\n",
            "Epoch 42/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.0558 - f1_m: 0.8888\n",
            "Epoch 42: val_loss improved from 0.02802 to 0.02670, saving model to mymodel_42\n",
            "80/80 [==============================] - 2s 25ms/step - loss: 0.0557 - f1_m: 0.8897 - val_loss: 0.0267 - val_f1_m: 0.9551\n",
            "Epoch 43/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0531 - f1_m: 0.8961\n",
            "Epoch 43: val_loss did not improve from 0.02670\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0531 - f1_m: 0.8961 - val_loss: 0.0279 - val_f1_m: 0.9536\n",
            "Epoch 44/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0495 - f1_m: 0.9004\n",
            "Epoch 44: val_loss did not improve from 0.02670\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0495 - f1_m: 0.9004 - val_loss: 0.0269 - val_f1_m: 0.9539\n",
            "Epoch 45/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0524 - f1_m: 0.8981\n",
            "Epoch 45: val_loss improved from 0.02670 to 0.02647, saving model to mymodel_45\n",
            "80/80 [==============================] - 2s 30ms/step - loss: 0.0524 - f1_m: 0.8981 - val_loss: 0.0265 - val_f1_m: 0.9558\n",
            "Epoch 46/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0529 - f1_m: 0.8950\n",
            "Epoch 46: val_loss improved from 0.02647 to 0.02543, saving model to mymodel_46\n",
            "80/80 [==============================] - 2s 25ms/step - loss: 0.0526 - f1_m: 0.8955 - val_loss: 0.0254 - val_f1_m: 0.9576\n",
            "Epoch 47/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.0513 - f1_m: 0.8991\n",
            "Epoch 47: val_loss did not improve from 0.02543\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0511 - f1_m: 0.8993 - val_loss: 0.0255 - val_f1_m: 0.9580\n",
            "Epoch 48/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0498 - f1_m: 0.9022\n",
            "Epoch 48: val_loss did not improve from 0.02543\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0498 - f1_m: 0.9022 - val_loss: 0.0274 - val_f1_m: 0.9550\n",
            "Epoch 49/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0485 - f1_m: 0.9014\n",
            "Epoch 49: val_loss did not improve from 0.02543\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0484 - f1_m: 0.9020 - val_loss: 0.0254 - val_f1_m: 0.9562\n",
            "Epoch 50/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0489 - f1_m: 0.9034\n",
            "Epoch 50: val_loss did not improve from 0.02543\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0490 - f1_m: 0.9030 - val_loss: 0.0266 - val_f1_m: 0.9534\n",
            "Epoch 51/200\n",
            "76/80 [===========================>..] - ETA: 0s - loss: 0.0469 - f1_m: 0.9043\n",
            "Epoch 51: val_loss did not improve from 0.02543\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0466 - f1_m: 0.9048 - val_loss: 0.0257 - val_f1_m: 0.9564\n",
            "Epoch 52/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0449 - f1_m: 0.9099\n",
            "Epoch 52: val_loss did not improve from 0.02543\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0448 - f1_m: 0.9099 - val_loss: 0.0256 - val_f1_m: 0.9568\n",
            "Epoch 53/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0455 - f1_m: 0.9084\n",
            "Epoch 53: val_loss improved from 0.02543 to 0.02376, saving model to mymodel_53\n",
            "80/80 [==============================] - 2s 25ms/step - loss: 0.0457 - f1_m: 0.9075 - val_loss: 0.0238 - val_f1_m: 0.9612\n",
            "Epoch 54/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.0431 - f1_m: 0.9135\n",
            "Epoch 54: val_loss improved from 0.02376 to 0.02352, saving model to mymodel_54\n",
            "80/80 [==============================] - 2s 25ms/step - loss: 0.0436 - f1_m: 0.9132 - val_loss: 0.0235 - val_f1_m: 0.9614\n",
            "Epoch 55/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0444 - f1_m: 0.9100\n",
            "Epoch 55: val_loss improved from 0.02352 to 0.02222, saving model to mymodel_55\n",
            "80/80 [==============================] - 2s 25ms/step - loss: 0.0443 - f1_m: 0.9100 - val_loss: 0.0222 - val_f1_m: 0.9608\n",
            "Epoch 56/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0408 - f1_m: 0.9175\n",
            "Epoch 56: val_loss did not improve from 0.02222\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0408 - f1_m: 0.9177 - val_loss: 0.0242 - val_f1_m: 0.9557\n",
            "Epoch 57/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0452 - f1_m: 0.9114\n",
            "Epoch 57: val_loss did not improve from 0.02222\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0455 - f1_m: 0.9104 - val_loss: 0.0247 - val_f1_m: 0.9580\n",
            "Epoch 58/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0408 - f1_m: 0.9161\n",
            "Epoch 58: val_loss did not improve from 0.02222\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0408 - f1_m: 0.9161 - val_loss: 0.0237 - val_f1_m: 0.9588\n",
            "Epoch 59/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0412 - f1_m: 0.9155\n",
            "Epoch 59: val_loss improved from 0.02222 to 0.02216, saving model to mymodel_59\n",
            "80/80 [==============================] - 2s 25ms/step - loss: 0.0412 - f1_m: 0.9155 - val_loss: 0.0222 - val_f1_m: 0.9561\n",
            "Epoch 60/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0442 - f1_m: 0.9115\n",
            "Epoch 60: val_loss did not improve from 0.02216\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0444 - f1_m: 0.9116 - val_loss: 0.0225 - val_f1_m: 0.9580\n",
            "Epoch 61/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0455 - f1_m: 0.9123\n",
            "Epoch 61: val_loss did not improve from 0.02216\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0454 - f1_m: 0.9123 - val_loss: 0.0235 - val_f1_m: 0.9589\n",
            "Epoch 62/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0411 - f1_m: 0.9161\n",
            "Epoch 62: val_loss did not improve from 0.02216\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0411 - f1_m: 0.9161 - val_loss: 0.0230 - val_f1_m: 0.9582\n",
            "Epoch 63/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0398 - f1_m: 0.9214\n",
            "Epoch 63: val_loss did not improve from 0.02216\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0397 - f1_m: 0.9213 - val_loss: 0.0229 - val_f1_m: 0.9618\n",
            "Epoch 64/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.0354 - f1_m: 0.9234\n",
            "Epoch 64: val_loss improved from 0.02216 to 0.02055, saving model to mymodel_64\n",
            "80/80 [==============================] - 2s 24ms/step - loss: 0.0353 - f1_m: 0.9242 - val_loss: 0.0205 - val_f1_m: 0.9638\n",
            "Epoch 65/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0394 - f1_m: 0.9211\n",
            "Epoch 65: val_loss did not improve from 0.02055\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0394 - f1_m: 0.9211 - val_loss: 0.0226 - val_f1_m: 0.9574\n",
            "Epoch 66/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.0319 - f1_m: 0.9314\n",
            "Epoch 66: val_loss did not improve from 0.02055\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0321 - f1_m: 0.9315 - val_loss: 0.0208 - val_f1_m: 0.9643\n",
            "Epoch 67/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.0359 - f1_m: 0.9324\n",
            "Epoch 67: val_loss did not improve from 0.02055\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0359 - f1_m: 0.9326 - val_loss: 0.0219 - val_f1_m: 0.9630\n",
            "Epoch 68/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0371 - f1_m: 0.9280\n",
            "Epoch 68: val_loss did not improve from 0.02055\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0371 - f1_m: 0.9280 - val_loss: 0.0211 - val_f1_m: 0.9630\n",
            "Epoch 69/200\n",
            "76/80 [===========================>..] - ETA: 0s - loss: 0.0340 - f1_m: 0.9294\n",
            "Epoch 69: val_loss improved from 0.02055 to 0.01975, saving model to mymodel_69\n",
            "80/80 [==============================] - 2s 26ms/step - loss: 0.0345 - f1_m: 0.9291 - val_loss: 0.0197 - val_f1_m: 0.9661\n",
            "Epoch 70/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0347 - f1_m: 0.9313\n",
            "Epoch 70: val_loss did not improve from 0.01975\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0349 - f1_m: 0.9314 - val_loss: 0.0208 - val_f1_m: 0.9650\n",
            "Epoch 71/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0358 - f1_m: 0.9290\n",
            "Epoch 71: val_loss did not improve from 0.01975\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0357 - f1_m: 0.9286 - val_loss: 0.0221 - val_f1_m: 0.9639\n",
            "Epoch 72/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0340 - f1_m: 0.9319\n",
            "Epoch 72: val_loss did not improve from 0.01975\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0342 - f1_m: 0.9311 - val_loss: 0.0210 - val_f1_m: 0.9642\n",
            "Epoch 73/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0350 - f1_m: 0.9333\n",
            "Epoch 73: val_loss did not improve from 0.01975\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0351 - f1_m: 0.9329 - val_loss: 0.0216 - val_f1_m: 0.9649\n",
            "Epoch 74/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0389 - f1_m: 0.9235\n",
            "Epoch 74: val_loss did not improve from 0.01975\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0389 - f1_m: 0.9236 - val_loss: 0.0214 - val_f1_m: 0.9639\n",
            "Epoch 75/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0358 - f1_m: 0.9331\n",
            "Epoch 75: val_loss did not improve from 0.01975\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0358 - f1_m: 0.9331 - val_loss: 0.0207 - val_f1_m: 0.9651\n",
            "Epoch 76/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0337 - f1_m: 0.9351\n",
            "Epoch 76: val_loss improved from 0.01975 to 0.01880, saving model to mymodel_76\n",
            "80/80 [==============================] - 2s 30ms/step - loss: 0.0337 - f1_m: 0.9351 - val_loss: 0.0188 - val_f1_m: 0.9674\n",
            "Epoch 77/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0335 - f1_m: 0.9345\n",
            "Epoch 77: val_loss did not improve from 0.01880\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0340 - f1_m: 0.9343 - val_loss: 0.0209 - val_f1_m: 0.9633\n",
            "Epoch 78/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0346 - f1_m: 0.9323\n",
            "Epoch 78: val_loss did not improve from 0.01880\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0346 - f1_m: 0.9323 - val_loss: 0.0202 - val_f1_m: 0.9654\n",
            "Epoch 79/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.0311 - f1_m: 0.9385\n",
            "Epoch 79: val_loss improved from 0.01880 to 0.01864, saving model to mymodel_79\n",
            "80/80 [==============================] - 2s 25ms/step - loss: 0.0312 - f1_m: 0.9385 - val_loss: 0.0186 - val_f1_m: 0.9680\n",
            "Epoch 80/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0323 - f1_m: 0.9389\n",
            "Epoch 80: val_loss did not improve from 0.01864\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0323 - f1_m: 0.9390 - val_loss: 0.0201 - val_f1_m: 0.9645\n",
            "Epoch 81/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.0310 - f1_m: 0.9405\n",
            "Epoch 81: val_loss did not improve from 0.01864\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0310 - f1_m: 0.9408 - val_loss: 0.0190 - val_f1_m: 0.9675\n",
            "Epoch 82/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0292 - f1_m: 0.9405\n",
            "Epoch 82: val_loss did not improve from 0.01864\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0292 - f1_m: 0.9405 - val_loss: 0.0192 - val_f1_m: 0.9676\n",
            "Epoch 83/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0339 - f1_m: 0.9348\n",
            "Epoch 83: val_loss did not improve from 0.01864\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0337 - f1_m: 0.9349 - val_loss: 0.0197 - val_f1_m: 0.9676\n",
            "Epoch 84/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0346 - f1_m: 0.9352\n",
            "Epoch 84: val_loss did not improve from 0.01864\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0346 - f1_m: 0.9354 - val_loss: 0.0196 - val_f1_m: 0.9681\n",
            "Epoch 85/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0320 - f1_m: 0.9368\n",
            "Epoch 85: val_loss did not improve from 0.01864\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0320 - f1_m: 0.9368 - val_loss: 0.0206 - val_f1_m: 0.9667\n",
            "Epoch 86/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0299 - f1_m: 0.9415\n",
            "Epoch 86: val_loss did not improve from 0.01864\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0299 - f1_m: 0.9415 - val_loss: 0.0194 - val_f1_m: 0.9685\n",
            "Epoch 87/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0300 - f1_m: 0.9421\n",
            "Epoch 87: val_loss did not improve from 0.01864\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0303 - f1_m: 0.9418 - val_loss: 0.0192 - val_f1_m: 0.9716\n",
            "Epoch 88/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0308 - f1_m: 0.9391\n",
            "Epoch 88: val_loss did not improve from 0.01864\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0308 - f1_m: 0.9392 - val_loss: 0.0195 - val_f1_m: 0.9713\n",
            "Epoch 89/200\n",
            "76/80 [===========================>..] - ETA: 0s - loss: 0.0288 - f1_m: 0.9441\n",
            "Epoch 89: val_loss did not improve from 0.01864\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0286 - f1_m: 0.9444 - val_loss: 0.0199 - val_f1_m: 0.9692\n",
            "Epoch 90/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0284 - f1_m: 0.9446\n",
            "Epoch 90: val_loss improved from 0.01864 to 0.01790, saving model to mymodel_90\n",
            "80/80 [==============================] - 2s 25ms/step - loss: 0.0284 - f1_m: 0.9446 - val_loss: 0.0179 - val_f1_m: 0.9726\n",
            "Epoch 91/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0317 - f1_m: 0.9402\n",
            "Epoch 91: val_loss did not improve from 0.01790\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0317 - f1_m: 0.9402 - val_loss: 0.0198 - val_f1_m: 0.9694\n",
            "Epoch 92/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0308 - f1_m: 0.9383\n",
            "Epoch 92: val_loss improved from 0.01790 to 0.01779, saving model to mymodel_92\n",
            "80/80 [==============================] - 2s 25ms/step - loss: 0.0307 - f1_m: 0.9386 - val_loss: 0.0178 - val_f1_m: 0.9709\n",
            "Epoch 93/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0284 - f1_m: 0.9461\n",
            "Epoch 93: val_loss did not improve from 0.01779\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0286 - f1_m: 0.9463 - val_loss: 0.0182 - val_f1_m: 0.9715\n",
            "Epoch 94/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0276 - f1_m: 0.9429\n",
            "Epoch 94: val_loss did not improve from 0.01779\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0276 - f1_m: 0.9429 - val_loss: 0.0184 - val_f1_m: 0.9720\n",
            "Epoch 95/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0255 - f1_m: 0.9489\n",
            "Epoch 95: val_loss improved from 0.01779 to 0.01746, saving model to mymodel_95\n",
            "80/80 [==============================] - 2s 25ms/step - loss: 0.0255 - f1_m: 0.9488 - val_loss: 0.0175 - val_f1_m: 0.9737\n",
            "Epoch 96/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0266 - f1_m: 0.9504\n",
            "Epoch 96: val_loss did not improve from 0.01746\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0266 - f1_m: 0.9504 - val_loss: 0.0181 - val_f1_m: 0.9702\n",
            "Epoch 97/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0269 - f1_m: 0.9461\n",
            "Epoch 97: val_loss improved from 0.01746 to 0.01713, saving model to mymodel_97\n",
            "80/80 [==============================] - 2s 26ms/step - loss: 0.0269 - f1_m: 0.9458 - val_loss: 0.0171 - val_f1_m: 0.9735\n",
            "Epoch 98/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0263 - f1_m: 0.9486\n",
            "Epoch 98: val_loss did not improve from 0.01713\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0265 - f1_m: 0.9480 - val_loss: 0.0185 - val_f1_m: 0.9702\n",
            "Epoch 99/200\n",
            "76/80 [===========================>..] - ETA: 0s - loss: 0.0303 - f1_m: 0.9417\n",
            "Epoch 99: val_loss did not improve from 0.01713\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0300 - f1_m: 0.9420 - val_loss: 0.0181 - val_f1_m: 0.9698\n",
            "Epoch 100/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0293 - f1_m: 0.9459\n",
            "Epoch 100: val_loss did not improve from 0.01713\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0293 - f1_m: 0.9459 - val_loss: 0.0199 - val_f1_m: 0.9683\n",
            "Epoch 101/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0295 - f1_m: 0.9467\n",
            "Epoch 101: val_loss did not improve from 0.01713\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0301 - f1_m: 0.9462 - val_loss: 0.0197 - val_f1_m: 0.9690\n",
            "Epoch 102/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0286 - f1_m: 0.9422\n",
            "Epoch 102: val_loss did not improve from 0.01713\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0286 - f1_m: 0.9422 - val_loss: 0.0184 - val_f1_m: 0.9703\n",
            "Epoch 103/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0248 - f1_m: 0.9530\n",
            "Epoch 103: val_loss did not improve from 0.01713\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0247 - f1_m: 0.9533 - val_loss: 0.0173 - val_f1_m: 0.9712\n",
            "Epoch 104/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0291 - f1_m: 0.9478\n",
            "Epoch 104: val_loss did not improve from 0.01713\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0291 - f1_m: 0.9478 - val_loss: 0.0174 - val_f1_m: 0.9690\n",
            "Epoch 105/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0257 - f1_m: 0.9476\n",
            "Epoch 105: val_loss did not improve from 0.01713\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0256 - f1_m: 0.9476 - val_loss: 0.0182 - val_f1_m: 0.9709\n",
            "Epoch 106/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0260 - f1_m: 0.9487\n",
            "Epoch 106: val_loss did not improve from 0.01713\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0263 - f1_m: 0.9484 - val_loss: 0.0185 - val_f1_m: 0.9701\n",
            "Epoch 107/200\n",
            "76/80 [===========================>..] - ETA: 0s - loss: 0.0305 - f1_m: 0.9457\n",
            "Epoch 107: val_loss did not improve from 0.01713\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0304 - f1_m: 0.9463 - val_loss: 0.0192 - val_f1_m: 0.9696\n",
            "Epoch 108/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0279 - f1_m: 0.9495\n",
            "Epoch 108: val_loss did not improve from 0.01713\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0279 - f1_m: 0.9495 - val_loss: 0.0181 - val_f1_m: 0.9692\n",
            "Epoch 109/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.0233 - f1_m: 0.9500\n",
            "Epoch 109: val_loss did not improve from 0.01713\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0233 - f1_m: 0.9502 - val_loss: 0.0175 - val_f1_m: 0.9681\n",
            "Epoch 110/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0280 - f1_m: 0.9496\n",
            "Epoch 110: val_loss did not improve from 0.01713\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0279 - f1_m: 0.9498 - val_loss: 0.0180 - val_f1_m: 0.9705\n",
            "Epoch 111/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.0285 - f1_m: 0.9463\n",
            "Epoch 111: val_loss did not improve from 0.01713\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0289 - f1_m: 0.9461 - val_loss: 0.0177 - val_f1_m: 0.9709\n",
            "Epoch 112/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0280 - f1_m: 0.9472\n",
            "Epoch 112: val_loss improved from 0.01713 to 0.01710, saving model to mymodel_112\n",
            "80/80 [==============================] - 2s 25ms/step - loss: 0.0280 - f1_m: 0.9472 - val_loss: 0.0171 - val_f1_m: 0.9713\n",
            "Epoch 113/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0269 - f1_m: 0.9500\n",
            "Epoch 113: val_loss did not improve from 0.01710\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0267 - f1_m: 0.9506 - val_loss: 0.0186 - val_f1_m: 0.9701\n",
            "Epoch 114/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0255 - f1_m: 0.9512\n",
            "Epoch 114: val_loss did not improve from 0.01710\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0258 - f1_m: 0.9514 - val_loss: 0.0192 - val_f1_m: 0.9687\n",
            "Epoch 115/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0267 - f1_m: 0.9490\n",
            "Epoch 115: val_loss did not improve from 0.01710\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0268 - f1_m: 0.9489 - val_loss: 0.0180 - val_f1_m: 0.9706\n",
            "Epoch 116/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0263 - f1_m: 0.9515\n",
            "Epoch 116: val_loss improved from 0.01710 to 0.01686, saving model to mymodel_116\n",
            "80/80 [==============================] - 2s 26ms/step - loss: 0.0262 - f1_m: 0.9518 - val_loss: 0.0169 - val_f1_m: 0.9709\n",
            "Epoch 117/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.0265 - f1_m: 0.9509\n",
            "Epoch 117: val_loss did not improve from 0.01686\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0262 - f1_m: 0.9511 - val_loss: 0.0180 - val_f1_m: 0.9695\n",
            "Epoch 118/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0235 - f1_m: 0.9580\n",
            "Epoch 118: val_loss did not improve from 0.01686\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0237 - f1_m: 0.9574 - val_loss: 0.0190 - val_f1_m: 0.9675\n",
            "Epoch 119/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.0255 - f1_m: 0.9517\n",
            "Epoch 119: val_loss did not improve from 0.01686\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0256 - f1_m: 0.9523 - val_loss: 0.0170 - val_f1_m: 0.9716\n",
            "Epoch 120/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.0228 - f1_m: 0.9556\n",
            "Epoch 120: val_loss did not improve from 0.01686\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0228 - f1_m: 0.9555 - val_loss: 0.0180 - val_f1_m: 0.9721\n",
            "Epoch 121/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0240 - f1_m: 0.9552\n",
            "Epoch 121: val_loss did not improve from 0.01686\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0239 - f1_m: 0.9552 - val_loss: 0.0177 - val_f1_m: 0.9719\n",
            "Epoch 122/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0233 - f1_m: 0.9556\n",
            "Epoch 122: val_loss did not improve from 0.01686\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0233 - f1_m: 0.9555 - val_loss: 0.0173 - val_f1_m: 0.9723\n",
            "Epoch 123/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0224 - f1_m: 0.9559\n",
            "Epoch 123: val_loss improved from 0.01686 to 0.01675, saving model to mymodel_123\n",
            "80/80 [==============================] - 2s 30ms/step - loss: 0.0222 - f1_m: 0.9562 - val_loss: 0.0167 - val_f1_m: 0.9727\n",
            "Epoch 124/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.0248 - f1_m: 0.9514\n",
            "Epoch 124: val_loss did not improve from 0.01675\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0248 - f1_m: 0.9512 - val_loss: 0.0169 - val_f1_m: 0.9708\n",
            "Epoch 125/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0222 - f1_m: 0.9580\n",
            "Epoch 125: val_loss did not improve from 0.01675\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0222 - f1_m: 0.9581 - val_loss: 0.0170 - val_f1_m: 0.9704\n",
            "Epoch 126/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0258 - f1_m: 0.9538\n",
            "Epoch 126: val_loss did not improve from 0.01675\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0259 - f1_m: 0.9533 - val_loss: 0.0183 - val_f1_m: 0.9686\n",
            "Epoch 127/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.0241 - f1_m: 0.9551\n",
            "Epoch 127: val_loss did not improve from 0.01675\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0244 - f1_m: 0.9553 - val_loss: 0.0172 - val_f1_m: 0.9710\n",
            "Epoch 128/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0242 - f1_m: 0.9538\n",
            "Epoch 128: val_loss did not improve from 0.01675\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0242 - f1_m: 0.9538 - val_loss: 0.0177 - val_f1_m: 0.9676\n",
            "Epoch 129/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.0231 - f1_m: 0.9566\n",
            "Epoch 129: val_loss did not improve from 0.01675\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0232 - f1_m: 0.9563 - val_loss: 0.0178 - val_f1_m: 0.9662\n",
            "Epoch 130/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0251 - f1_m: 0.9521\n",
            "Epoch 130: val_loss did not improve from 0.01675\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0252 - f1_m: 0.9518 - val_loss: 0.0175 - val_f1_m: 0.9711\n",
            "Epoch 131/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.0232 - f1_m: 0.9547\n",
            "Epoch 131: val_loss did not improve from 0.01675\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0231 - f1_m: 0.9547 - val_loss: 0.0173 - val_f1_m: 0.9688\n",
            "Epoch 132/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0250 - f1_m: 0.9546\n",
            "Epoch 132: val_loss improved from 0.01675 to 0.01648, saving model to mymodel_132\n",
            "80/80 [==============================] - 2s 26ms/step - loss: 0.0250 - f1_m: 0.9546 - val_loss: 0.0165 - val_f1_m: 0.9717\n",
            "Epoch 133/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0233 - f1_m: 0.9586\n",
            "Epoch 133: val_loss improved from 0.01648 to 0.01648, saving model to mymodel_133\n",
            "80/80 [==============================] - 2s 26ms/step - loss: 0.0233 - f1_m: 0.9586 - val_loss: 0.0165 - val_f1_m: 0.9715\n",
            "Epoch 134/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0241 - f1_m: 0.9539\n",
            "Epoch 134: val_loss did not improve from 0.01648\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0241 - f1_m: 0.9539 - val_loss: 0.0168 - val_f1_m: 0.9712\n",
            "Epoch 135/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.0238 - f1_m: 0.9593\n",
            "Epoch 135: val_loss did not improve from 0.01648\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0240 - f1_m: 0.9589 - val_loss: 0.0169 - val_f1_m: 0.9721\n",
            "Epoch 136/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0227 - f1_m: 0.9572\n",
            "Epoch 136: val_loss did not improve from 0.01648\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0233 - f1_m: 0.9571 - val_loss: 0.0182 - val_f1_m: 0.9682\n",
            "Epoch 137/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0256 - f1_m: 0.9529\n",
            "Epoch 137: val_loss did not improve from 0.01648\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0257 - f1_m: 0.9530 - val_loss: 0.0174 - val_f1_m: 0.9711\n",
            "Epoch 138/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0249 - f1_m: 0.9552\n",
            "Epoch 138: val_loss did not improve from 0.01648\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0249 - f1_m: 0.9552 - val_loss: 0.0188 - val_f1_m: 0.9717\n",
            "Epoch 139/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0236 - f1_m: 0.9562\n",
            "Epoch 139: val_loss did not improve from 0.01648\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0236 - f1_m: 0.9562 - val_loss: 0.0174 - val_f1_m: 0.9725\n",
            "Epoch 140/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0229 - f1_m: 0.9573\n",
            "Epoch 140: val_loss did not improve from 0.01648\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0227 - f1_m: 0.9573 - val_loss: 0.0182 - val_f1_m: 0.9704\n",
            "Epoch 141/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0230 - f1_m: 0.9573\n",
            "Epoch 141: val_loss did not improve from 0.01648\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0230 - f1_m: 0.9573 - val_loss: 0.0174 - val_f1_m: 0.9735\n",
            "Epoch 142/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0232 - f1_m: 0.9575\n",
            "Epoch 142: val_loss did not improve from 0.01648\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0229 - f1_m: 0.9580 - val_loss: 0.0176 - val_f1_m: 0.9706\n",
            "Epoch 143/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0225 - f1_m: 0.9581\n",
            "Epoch 143: val_loss did not improve from 0.01648\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0225 - f1_m: 0.9581 - val_loss: 0.0178 - val_f1_m: 0.9731\n",
            "Epoch 144/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.0224 - f1_m: 0.9584\n",
            "Epoch 144: val_loss did not improve from 0.01648\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0224 - f1_m: 0.9577 - val_loss: 0.0182 - val_f1_m: 0.9726\n",
            "Epoch 145/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.0200 - f1_m: 0.9622\n",
            "Epoch 145: val_loss did not improve from 0.01648\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0199 - f1_m: 0.9623 - val_loss: 0.0178 - val_f1_m: 0.9723\n",
            "Epoch 146/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0195 - f1_m: 0.9645\n",
            "Epoch 146: val_loss did not improve from 0.01648\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0195 - f1_m: 0.9642 - val_loss: 0.0186 - val_f1_m: 0.9719\n",
            "Epoch 147/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0227 - f1_m: 0.9594\n",
            "Epoch 147: val_loss did not improve from 0.01648\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.0227 - f1_m: 0.9594 - val_loss: 0.0168 - val_f1_m: 0.9743\n",
            "Epoch 148/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.0191 - f1_m: 0.9623\n",
            "Epoch 148: val_loss did not improve from 0.01648\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0190 - f1_m: 0.9620 - val_loss: 0.0170 - val_f1_m: 0.9748\n",
            "Epoch 149/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.0221 - f1_m: 0.9628\n",
            "Epoch 149: val_loss did not improve from 0.01648\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0223 - f1_m: 0.9627 - val_loss: 0.0202 - val_f1_m: 0.9719\n",
            "Epoch 150/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0233 - f1_m: 0.9559\n",
            "Epoch 150: val_loss did not improve from 0.01648\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0233 - f1_m: 0.9561 - val_loss: 0.0173 - val_f1_m: 0.9721\n",
            "Epoch 151/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0213 - f1_m: 0.9591\n",
            "Epoch 151: val_loss did not improve from 0.01648\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.0215 - f1_m: 0.9592 - val_loss: 0.0174 - val_f1_m: 0.9696\n",
            "Epoch 152/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0225 - f1_m: 0.9588\n",
            "Epoch 152: val_loss did not improve from 0.01648\n",
            "80/80 [==============================] - 2s 19ms/step - loss: 0.0223 - f1_m: 0.9591 - val_loss: 0.0171 - val_f1_m: 0.9717\n",
            "Epoch 153/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0234 - f1_m: 0.9580\n",
            "Epoch 153: val_loss did not improve from 0.01648\n",
            "80/80 [==============================] - 1s 19ms/step - loss: 0.0234 - f1_m: 0.9581 - val_loss: 0.0168 - val_f1_m: 0.9727\n",
            "Epoch 154/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0249 - f1_m: 0.9586\n",
            "Epoch 154: val_loss did not improve from 0.01648\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.0250 - f1_m: 0.9588 - val_loss: 0.0170 - val_f1_m: 0.9727\n",
            "Epoch 155/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0209 - f1_m: 0.9588\n",
            "Epoch 155: val_loss improved from 0.01648 to 0.01643, saving model to mymodel_155\n",
            "80/80 [==============================] - 2s 29ms/step - loss: 0.0207 - f1_m: 0.9591 - val_loss: 0.0164 - val_f1_m: 0.9735\n",
            "Epoch 156/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0224 - f1_m: 0.9614\n",
            "Epoch 156: val_loss did not improve from 0.01643\n",
            "80/80 [==============================] - 2s 19ms/step - loss: 0.0224 - f1_m: 0.9614 - val_loss: 0.0168 - val_f1_m: 0.9731\n",
            "Epoch 157/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.0211 - f1_m: 0.9596\n",
            "Epoch 157: val_loss did not improve from 0.01643\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0211 - f1_m: 0.9596 - val_loss: 0.0180 - val_f1_m: 0.9745\n",
            "Epoch 158/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0218 - f1_m: 0.9609\n",
            "Epoch 158: val_loss did not improve from 0.01643\n",
            "80/80 [==============================] - 1s 19ms/step - loss: 0.0218 - f1_m: 0.9609 - val_loss: 0.0166 - val_f1_m: 0.9741\n",
            "Epoch 159/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0205 - f1_m: 0.9631\n",
            "Epoch 159: val_loss did not improve from 0.01643\n",
            "80/80 [==============================] - 1s 19ms/step - loss: 0.0205 - f1_m: 0.9631 - val_loss: 0.0167 - val_f1_m: 0.9745\n",
            "Epoch 160/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0207 - f1_m: 0.9639\n",
            "Epoch 160: val_loss did not improve from 0.01643\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.0207 - f1_m: 0.9639 - val_loss: 0.0166 - val_f1_m: 0.9739\n",
            "Epoch 161/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0217 - f1_m: 0.9632\n",
            "Epoch 161: val_loss did not improve from 0.01643\n",
            "80/80 [==============================] - 2s 19ms/step - loss: 0.0217 - f1_m: 0.9631 - val_loss: 0.0178 - val_f1_m: 0.9738\n",
            "Epoch 162/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0225 - f1_m: 0.9601\n",
            "Epoch 162: val_loss did not improve from 0.01643\n",
            "80/80 [==============================] - 2s 19ms/step - loss: 0.0224 - f1_m: 0.9600 - val_loss: 0.0167 - val_f1_m: 0.9746\n",
            "Epoch 163/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0228 - f1_m: 0.9603\n",
            "Epoch 163: val_loss did not improve from 0.01643\n",
            "80/80 [==============================] - 2s 20ms/step - loss: 0.0227 - f1_m: 0.9603 - val_loss: 0.0167 - val_f1_m: 0.9737\n",
            "Epoch 164/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0192 - f1_m: 0.9648\n",
            "Epoch 164: val_loss improved from 0.01643 to 0.01532, saving model to mymodel_164\n",
            "80/80 [==============================] - 2s 30ms/step - loss: 0.0192 - f1_m: 0.9648 - val_loss: 0.0153 - val_f1_m: 0.9750\n",
            "Epoch 165/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0208 - f1_m: 0.9631\n",
            "Epoch 165: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.0208 - f1_m: 0.9631 - val_loss: 0.0162 - val_f1_m: 0.9746\n",
            "Epoch 166/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.0197 - f1_m: 0.9637\n",
            "Epoch 166: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 1s 19ms/step - loss: 0.0201 - f1_m: 0.9637 - val_loss: 0.0162 - val_f1_m: 0.9733\n",
            "Epoch 167/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0202 - f1_m: 0.9638\n",
            "Epoch 167: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 2s 20ms/step - loss: 0.0202 - f1_m: 0.9636 - val_loss: 0.0178 - val_f1_m: 0.9719\n",
            "Epoch 168/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0217 - f1_m: 0.9627\n",
            "Epoch 168: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 2s 19ms/step - loss: 0.0216 - f1_m: 0.9628 - val_loss: 0.0171 - val_f1_m: 0.9740\n",
            "Epoch 169/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0208 - f1_m: 0.9615\n",
            "Epoch 169: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0207 - f1_m: 0.9617 - val_loss: 0.0167 - val_f1_m: 0.9739\n",
            "Epoch 170/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0189 - f1_m: 0.9652\n",
            "Epoch 170: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0189 - f1_m: 0.9652 - val_loss: 0.0173 - val_f1_m: 0.9727\n",
            "Epoch 171/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0204 - f1_m: 0.9645\n",
            "Epoch 171: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.0204 - f1_m: 0.9643 - val_loss: 0.0181 - val_f1_m: 0.9693\n",
            "Epoch 172/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0206 - f1_m: 0.9633\n",
            "Epoch 172: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 2s 20ms/step - loss: 0.0206 - f1_m: 0.9631 - val_loss: 0.0170 - val_f1_m: 0.9710\n",
            "Epoch 173/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0192 - f1_m: 0.9666\n",
            "Epoch 173: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 2s 19ms/step - loss: 0.0189 - f1_m: 0.9667 - val_loss: 0.0164 - val_f1_m: 0.9718\n",
            "Epoch 174/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.0206 - f1_m: 0.9676\n",
            "Epoch 174: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 2s 19ms/step - loss: 0.0206 - f1_m: 0.9674 - val_loss: 0.0170 - val_f1_m: 0.9727\n",
            "Epoch 175/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0188 - f1_m: 0.9653\n",
            "Epoch 175: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 1s 19ms/step - loss: 0.0187 - f1_m: 0.9655 - val_loss: 0.0161 - val_f1_m: 0.9746\n",
            "Epoch 176/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0197 - f1_m: 0.9654\n",
            "Epoch 176: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 2s 19ms/step - loss: 0.0197 - f1_m: 0.9652 - val_loss: 0.0163 - val_f1_m: 0.9725\n",
            "Epoch 177/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0191 - f1_m: 0.9647\n",
            "Epoch 177: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 1s 19ms/step - loss: 0.0193 - f1_m: 0.9647 - val_loss: 0.0165 - val_f1_m: 0.9745\n",
            "Epoch 178/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0185 - f1_m: 0.9667\n",
            "Epoch 178: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 2s 19ms/step - loss: 0.0185 - f1_m: 0.9668 - val_loss: 0.0161 - val_f1_m: 0.9745\n",
            "Epoch 179/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0196 - f1_m: 0.9671\n",
            "Epoch 179: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 2s 19ms/step - loss: 0.0195 - f1_m: 0.9669 - val_loss: 0.0160 - val_f1_m: 0.9753\n",
            "Epoch 180/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0198 - f1_m: 0.9643\n",
            "Epoch 180: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 2s 20ms/step - loss: 0.0202 - f1_m: 0.9641 - val_loss: 0.0163 - val_f1_m: 0.9748\n",
            "Epoch 181/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0180 - f1_m: 0.9680\n",
            "Epoch 181: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 2s 20ms/step - loss: 0.0177 - f1_m: 0.9684 - val_loss: 0.0169 - val_f1_m: 0.9741\n",
            "Epoch 182/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0177 - f1_m: 0.9683\n",
            "Epoch 182: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 2s 19ms/step - loss: 0.0177 - f1_m: 0.9683 - val_loss: 0.0178 - val_f1_m: 0.9721\n",
            "Epoch 183/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0176 - f1_m: 0.9678\n",
            "Epoch 183: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 2s 19ms/step - loss: 0.0179 - f1_m: 0.9671 - val_loss: 0.0174 - val_f1_m: 0.9754\n",
            "Epoch 184/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0184 - f1_m: 0.9665\n",
            "Epoch 184: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 2s 20ms/step - loss: 0.0185 - f1_m: 0.9664 - val_loss: 0.0173 - val_f1_m: 0.9747\n",
            "Epoch 185/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0183 - f1_m: 0.9669\n",
            "Epoch 185: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 2s 20ms/step - loss: 0.0182 - f1_m: 0.9670 - val_loss: 0.0164 - val_f1_m: 0.9752\n",
            "Epoch 186/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0187 - f1_m: 0.9672\n",
            "Epoch 186: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 2s 20ms/step - loss: 0.0192 - f1_m: 0.9671 - val_loss: 0.0169 - val_f1_m: 0.9732\n",
            "Epoch 187/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0187 - f1_m: 0.9660\n",
            "Epoch 187: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.0186 - f1_m: 0.9659 - val_loss: 0.0166 - val_f1_m: 0.9733\n",
            "Epoch 188/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0195 - f1_m: 0.9663\n",
            "Epoch 188: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0194 - f1_m: 0.9665 - val_loss: 0.0170 - val_f1_m: 0.9720\n",
            "Epoch 189/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0188 - f1_m: 0.9664\n",
            "Epoch 189: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0188 - f1_m: 0.9664 - val_loss: 0.0160 - val_f1_m: 0.9751\n",
            "Epoch 190/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0219 - f1_m: 0.9603\n",
            "Epoch 190: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 2s 20ms/step - loss: 0.0219 - f1_m: 0.9603 - val_loss: 0.0155 - val_f1_m: 0.9747\n",
            "Epoch 191/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0206 - f1_m: 0.9645\n",
            "Epoch 191: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 2s 20ms/step - loss: 0.0207 - f1_m: 0.9644 - val_loss: 0.0163 - val_f1_m: 0.9733\n",
            "Epoch 192/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.0181 - f1_m: 0.9667\n",
            "Epoch 192: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.0182 - f1_m: 0.9663 - val_loss: 0.0166 - val_f1_m: 0.9728\n",
            "Epoch 193/200\n",
            "77/80 [===========================>..] - ETA: 0s - loss: 0.0185 - f1_m: 0.9670\n",
            "Epoch 193: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0186 - f1_m: 0.9671 - val_loss: 0.0176 - val_f1_m: 0.9733\n",
            "Epoch 194/200\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0162 - f1_m: 0.9706\n",
            "Epoch 194: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.0162 - f1_m: 0.9706 - val_loss: 0.0162 - val_f1_m: 0.9754\n",
            "Epoch 195/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0196 - f1_m: 0.9670\n",
            "Epoch 195: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0196 - f1_m: 0.9666 - val_loss: 0.0160 - val_f1_m: 0.9738\n",
            "Epoch 196/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0175 - f1_m: 0.9705\n",
            "Epoch 196: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.0173 - f1_m: 0.9707 - val_loss: 0.0162 - val_f1_m: 0.9750\n",
            "Epoch 197/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0186 - f1_m: 0.9666\n",
            "Epoch 197: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0186 - f1_m: 0.9666 - val_loss: 0.0172 - val_f1_m: 0.9709\n",
            "Epoch 198/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0193 - f1_m: 0.9671\n",
            "Epoch 198: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 2s 21ms/step - loss: 0.0192 - f1_m: 0.9671 - val_loss: 0.0172 - val_f1_m: 0.9744\n",
            "Epoch 199/200\n",
            "78/80 [============================>.] - ETA: 0s - loss: 0.0175 - f1_m: 0.9701\n",
            "Epoch 199: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 2s 19ms/step - loss: 0.0178 - f1_m: 0.9700 - val_loss: 0.0169 - val_f1_m: 0.9734\n",
            "Epoch 200/200\n",
            "79/80 [============================>.] - ETA: 0s - loss: 0.0182 - f1_m: 0.9686\n",
            "Epoch 200: val_loss did not improve from 0.01532\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.0181 - f1_m: 0.9686 - val_loss: 0.0169 - val_f1_m: 0.9739\n"
          ]
        }
      ],
      "source": [
        "results = model.fit(X_train, Y_train, validation_data = (X_test, Y_test), \n",
        "                    batch_size=1000, epochs=200, \n",
        "                    callbacks=[model_checkpoint], class_weight=class_weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HaWqeaJDxAf"
      },
      "source": [
        "Лучший результат из 200 относится к 164 эпохе."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXpBTfVYA58-"
      },
      "outputs": [],
      "source": [
        "def display_f1_m():\n",
        "    plt.plot(results.history['f1_m'])\n",
        "    plt.plot(results.history['val_f1_m'])\n",
        "\n",
        "    plt.title('Model accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "def display_loss():\n",
        "    plt.plot(results.history['loss'])\n",
        "    plt.plot(results.history['val_loss'])\n",
        "\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WKICTSz78tQm",
        "outputId": "c60493cc-92a8-4ff3-938a-a1d1da1c6153"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x720 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x720 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display_f1_m()\n",
        "display_loss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXJjx2M6CP82"
      },
      "source": [
        "Дропаут - вещь."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpUeSkGpEDVI"
      },
      "source": [
        "Кстати, раз уж у нас лучшая эпоха была не последняя, а 164, сохраним f1-меру для нее"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuLRIAriW6o6"
      },
      "outputs": [],
      "source": [
        "f1_1 = results.history['val_f1_m'][163]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JA2j7anEoRqo",
        "outputId": "bc3945bb-6659-4804-b425-642b899ac10e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9750153422355652\n"
          ]
        }
      ],
      "source": [
        "print(f1_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dNriIfu52Z9Z"
      },
      "outputs": [],
      "source": [
        "# на случай последующих экспериментов, чтобы не считать много раз модель\n",
        "f1_1 = 0.9750153422355652"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgD6VGW07-dF"
      },
      "source": [
        "Взвешенный случайный лес"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyQc28XGFOB6"
      },
      "outputs": [],
      "source": [
        "model_RF_bal = RandomForestClassifier(n_estimators=500, \n",
        "                                      class_weight=class_weight, random_state=42)\n",
        "# model_RF_bal = RandomForestClassifier(class_weight=\"balanced\", random_state=42)\n",
        "model_RF_bal.fit(X_train, Y_train)\n",
        "predictions_rf = model_RF_bal.predict_proba(X_test)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOsKKhhiEmwS",
        "outputId": "2f6d743b-8386-4d66-ad12-43ff790fc316"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7803508639335632\n"
          ]
        }
      ],
      "source": [
        "f1_2 = float(f1_m(Y_test, K.round(predictions_rf)))\n",
        "print(f1_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UotyKSqkXIM4"
      },
      "source": [
        "Лес без взвешивания"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgEMZb4uUVJ_"
      },
      "outputs": [],
      "source": [
        "model_RF = RandomForestClassifier(n_estimators=500, random_state=42)\n",
        "model_RF.fit(X_train, Y_train)\n",
        "predictions_rf = model_RF.predict_proba(X_test)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Rgi1O2RUYmx",
        "outputId": "2ed80646-3d9f-4955-cced-ca9fc402c557"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7857391238212585\n"
          ]
        }
      ],
      "source": [
        "f1_2 = float(f1_m(Y_test, K.round(predictions_rf)))\n",
        "print(f1_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqvjDo3VUnpl"
      },
      "source": [
        "Одинокое дерево ниже и того лучше"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gUsnUVz8DT8"
      },
      "source": [
        "Взвешенный градиентный бустинг"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUh-cYMs-PIK"
      },
      "outputs": [],
      "source": [
        "# model_XGB = xgb.XGBClassifier(sample_weight=class_weight, random_state=42)\n",
        "model_XGB = xgb.XGBClassifier(n_estimators=1500, \n",
        "                              class_weight=class_weight, random_state=42)\n",
        "model_XGB.fit(X_train, Y_train)\n",
        "predictions_xgb = model_XGB.predict_proba(X_test)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GowUQYSVGVZi",
        "outputId": "dc9c4c5d-c1b6-4339-8a66-8db8b194bd8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9414575099945068\n"
          ]
        }
      ],
      "source": [
        "f1_3 = float(f1_m(Y_test, K.round(predictions_xgb)))\n",
        "print(f1_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY8BOqwfiuuM"
      },
      "source": [
        "Попробуем градиентный бустинг из sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O5f5HBdQix21"
      },
      "outputs": [],
      "source": [
        "model_gb = GradientBoostingClassifier(n_estimators=1500, random_state=42) # тут как будто не предполагается учет несбалансированных классов\n",
        "model_gb.fit(X_train, Y_train) # работает больше часа\n",
        "predictions_gb = model_gb.predict_proba(X_test)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "s6qtVf4vjdAw",
        "outputId": "4b9787df-907f-443c-8f4d-209e1e6cbbc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9244713187217712\n"
          ]
        }
      ],
      "source": [
        "f1_16 = float(f1_m(Y_test, K.round(predictions_gb)))\n",
        "print(f1_16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wA_z-TKl7MRr"
      },
      "source": [
        "Раз у нас хорошо работает нейросеть с линейными слоями, возникла идея попробовать линейный классификатор из методов классического машинного обучения. Попробуем метод опорных векторов, за нелинейность в нем будет отвечать ядро (радиально-базисная функция)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7cXFRDcJ1AF"
      },
      "outputs": [],
      "source": [
        "svm = SVC(kernel='rbf', class_weight=class_weight, random_state=42)\n",
        "svm.fit(X_train, Y_train)\n",
        "predictions_svm = svm.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BN12fb8825qF",
        "outputId": "aeec682f-5097-4184-9981-5b36d9995a7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9391303658485413\n"
          ]
        }
      ],
      "source": [
        "f1_4 = float(f1_m(Y_test, K.round(predictions_svm)))\n",
        "print(f1_4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wr1XYF5c6eaM"
      },
      "outputs": [],
      "source": [
        "svm = SVC(kernel='rbf', probability=True, \n",
        "          class_weight=class_weight, random_state=42) \n",
        "svm.fit(X_train, Y_train) # больше часа\n",
        "predictions_svm_ = svm.predict_proba(X_test)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9y7Qmzv-6lEx",
        "outputId": "0a88f341-e527-4c61-94a3-ae94e79a54a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9503176808357239\n"
          ]
        }
      ],
      "source": [
        "f1_5 = float(f1_m(Y_test, K.round(predictions_svm_)))\n",
        "print(f1_5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHn8BKCNE6vZ"
      },
      "source": [
        "Расчет вероятностей существенно увеличивает время работы...\n",
        "\n",
        "Мягкий классификатор оказался неплох"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "DVNC3ibVQg6z"
      },
      "outputs": [],
      "source": [
        "# на случай последующих экспериментов, чтобы не считать много раз модель\n",
        "f1_5 = 0.9503176808357239"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH9JCpiyZ86H"
      },
      "source": [
        "Попробуем с ядром попроще"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZ7JRKwsYAh-"
      },
      "outputs": [],
      "source": [
        "svc_lin = SVC(kernel='linear', probability=True, class_weight=class_weight, random_state=42)\n",
        "svc_lin.fit(X_train, Y_train)\n",
        "predictions_svc_lin = svc_lin.predict_proba(X_test)[:, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f3Q8uO2u-nk"
      },
      "source": [
        "Ядро попроще, время выполнения - нет... На пятом часу я решила, что, возможно, выборка линейно неразделима..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAdrzknMZ_6M"
      },
      "outputs": [],
      "source": [
        "f1_12 = float(f1_m(Y_test, K.round(predictions_svm_)))\n",
        "print(f1_12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXrEF-HR75-I"
      },
      "source": [
        "Из линейных классификаторов у нас также есть логистическая регрессия. Попробуем и ее.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ru8QV-HD7_Ij",
        "outputId": "cfd9c198-5623-454f-81b8-3f9c410130c9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "logreg = LogisticRegression(penalty='l2', class_weight=class_weight, \n",
        "                            random_state=42, solver='saga')\n",
        "logreg.fit(X_train, Y_train)\n",
        "predictions_logreg = logreg.predict_proba(X_test)[:, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1iKKk0dN2IH"
      },
      "source": [
        "Не сошлась что ли"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cleWcvOsE0b9",
        "outputId": "471d47d4-7809-483f-a080-4e88c399357d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.206183522939682\n"
          ]
        }
      ],
      "source": [
        "f1_6 = float(f1_m(Y_test, K.round(predictions_logreg)))\n",
        "print(f1_6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVdTcxKrOhCY"
      },
      "source": [
        "Выглядит не очень "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U110D--a3fgG"
      },
      "source": [
        "Может, мне удастся настроить градиентный бустинг в катбуст...\n",
        "\n",
        "Ниже в модели количество деревьев увеличивалось до тех пор, пока значения f-меры на тренировочных данных не перестали изменяться."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "boosting_model = catboost.CatBoostClassifier(n_estimators=4000, \n",
        "                                             learning_rate=0.1,\n",
        "                                             depth=4,\n",
        "                                             eval_metric='F1',\n",
        "                                             random_seed=42,\n",
        "                                             class_weights=class_weight)\n",
        "boosting_model.fit(X_train, Y_train)\n",
        "\n",
        "predictions_cat = boosting_model.predict_proba(X_test)[:, 1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Er9x5vglgfH-",
        "outputId": "a4efd90a-2a13-4b62-8803-ea2c74c3bff1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\tlearn: 0.6852146\ttotal: 91.7ms\tremaining: 6m 6s\n",
            "1:\tlearn: 0.7196323\ttotal: 154ms\tremaining: 5m 7s\n",
            "2:\tlearn: 0.8070603\ttotal: 227ms\tremaining: 5m 2s\n",
            "3:\tlearn: 0.8449248\ttotal: 299ms\tremaining: 4m 58s\n",
            "4:\tlearn: 0.8640227\ttotal: 385ms\tremaining: 5m 7s\n",
            "5:\tlearn: 0.8629636\ttotal: 457ms\tremaining: 5m 3s\n",
            "6:\tlearn: 0.8638052\ttotal: 531ms\tremaining: 5m 3s\n",
            "7:\tlearn: 0.8727026\ttotal: 608ms\tremaining: 5m 3s\n",
            "8:\tlearn: 0.8728451\ttotal: 673ms\tremaining: 4m 58s\n",
            "9:\tlearn: 0.8726766\ttotal: 746ms\tremaining: 4m 57s\n",
            "10:\tlearn: 0.8721249\ttotal: 825ms\tremaining: 4m 59s\n",
            "11:\tlearn: 0.8783926\ttotal: 886ms\tremaining: 4m 54s\n",
            "12:\tlearn: 0.8889472\ttotal: 967ms\tremaining: 4m 56s\n",
            "13:\tlearn: 0.8886508\ttotal: 1.05s\tremaining: 4m 57s\n",
            "14:\tlearn: 0.8941621\ttotal: 1.13s\tremaining: 5m\n",
            "15:\tlearn: 0.8960845\ttotal: 1.21s\tremaining: 5m\n",
            "16:\tlearn: 0.8943879\ttotal: 1.27s\tremaining: 4m 57s\n",
            "17:\tlearn: 0.8950489\ttotal: 1.34s\tremaining: 4m 56s\n",
            "18:\tlearn: 0.8965052\ttotal: 1.45s\tremaining: 5m 4s\n",
            "19:\tlearn: 0.8990970\ttotal: 1.53s\tremaining: 5m 3s\n",
            "20:\tlearn: 0.9001968\ttotal: 1.59s\tremaining: 5m 1s\n",
            "21:\tlearn: 0.9040617\ttotal: 1.65s\tremaining: 4m 58s\n",
            "22:\tlearn: 0.9082278\ttotal: 1.72s\tremaining: 4m 57s\n",
            "23:\tlearn: 0.9195396\ttotal: 1.79s\tremaining: 4m 56s\n",
            "24:\tlearn: 0.9196046\ttotal: 1.87s\tremaining: 4m 57s\n",
            "25:\tlearn: 0.9209651\ttotal: 1.95s\tremaining: 4m 58s\n",
            "26:\tlearn: 0.9229698\ttotal: 2.02s\tremaining: 4m 56s\n",
            "27:\tlearn: 0.9225202\ttotal: 2.09s\tremaining: 4m 57s\n",
            "28:\tlearn: 0.9247018\ttotal: 2.18s\tremaining: 4m 58s\n",
            "29:\tlearn: 0.9262187\ttotal: 2.26s\tremaining: 4m 58s\n",
            "30:\tlearn: 0.9311478\ttotal: 2.32s\tremaining: 4m 57s\n",
            "31:\tlearn: 0.9331959\ttotal: 2.41s\tremaining: 4m 58s\n",
            "32:\tlearn: 0.9345879\ttotal: 2.49s\tremaining: 4m 59s\n",
            "33:\tlearn: 0.9345444\ttotal: 2.55s\tremaining: 4m 57s\n",
            "34:\tlearn: 0.9349848\ttotal: 2.63s\tremaining: 4m 57s\n",
            "35:\tlearn: 0.9376389\ttotal: 2.69s\tremaining: 4m 56s\n",
            "36:\tlearn: 0.9371479\ttotal: 2.77s\tremaining: 4m 57s\n",
            "37:\tlearn: 0.9373436\ttotal: 2.84s\tremaining: 4m 55s\n",
            "38:\tlearn: 0.9384336\ttotal: 2.9s\tremaining: 4m 54s\n",
            "39:\tlearn: 0.9380329\ttotal: 2.97s\tremaining: 4m 54s\n",
            "40:\tlearn: 0.9387119\ttotal: 3.05s\tremaining: 4m 54s\n",
            "41:\tlearn: 0.9387109\ttotal: 3.11s\tremaining: 4m 53s\n",
            "42:\tlearn: 0.9401874\ttotal: 3.17s\tremaining: 4m 52s\n",
            "43:\tlearn: 0.9398113\ttotal: 3.24s\tremaining: 4m 50s\n",
            "44:\tlearn: 0.9395502\ttotal: 3.32s\tremaining: 4m 51s\n",
            "45:\tlearn: 0.9393846\ttotal: 3.38s\tremaining: 4m 50s\n",
            "46:\tlearn: 0.9401487\ttotal: 3.45s\tremaining: 4m 50s\n",
            "47:\tlearn: 0.9399302\ttotal: 3.53s\tremaining: 4m 50s\n",
            "48:\tlearn: 0.9409922\ttotal: 3.6s\tremaining: 4m 50s\n",
            "49:\tlearn: 0.9413843\ttotal: 3.66s\tremaining: 4m 49s\n",
            "50:\tlearn: 0.9414298\ttotal: 3.73s\tremaining: 4m 48s\n",
            "51:\tlearn: 0.9418901\ttotal: 3.8s\tremaining: 4m 48s\n",
            "52:\tlearn: 0.9417469\ttotal: 3.87s\tremaining: 4m 48s\n",
            "53:\tlearn: 0.9428776\ttotal: 3.96s\tremaining: 4m 49s\n",
            "54:\tlearn: 0.9428162\ttotal: 4.03s\tremaining: 4m 48s\n",
            "55:\tlearn: 0.9433637\ttotal: 4.09s\tremaining: 4m 48s\n",
            "56:\tlearn: 0.9432848\ttotal: 4.16s\tremaining: 4m 47s\n",
            "57:\tlearn: 0.9438434\ttotal: 4.23s\tremaining: 4m 47s\n",
            "58:\tlearn: 0.9440936\ttotal: 4.29s\tremaining: 4m 46s\n",
            "59:\tlearn: 0.9446862\ttotal: 4.37s\tremaining: 4m 46s\n",
            "60:\tlearn: 0.9445804\ttotal: 4.43s\tremaining: 4m 46s\n",
            "61:\tlearn: 0.9446734\ttotal: 4.52s\tremaining: 4m 47s\n",
            "62:\tlearn: 0.9441373\ttotal: 4.58s\tremaining: 4m 46s\n",
            "63:\tlearn: 0.9447211\ttotal: 4.65s\tremaining: 4m 45s\n",
            "64:\tlearn: 0.9455593\ttotal: 4.73s\tremaining: 4m 46s\n",
            "65:\tlearn: 0.9462476\ttotal: 4.8s\tremaining: 4m 45s\n",
            "66:\tlearn: 0.9463679\ttotal: 4.86s\tremaining: 4m 45s\n",
            "67:\tlearn: 0.9475585\ttotal: 4.94s\tremaining: 4m 45s\n",
            "68:\tlearn: 0.9481767\ttotal: 5.01s\tremaining: 4m 45s\n",
            "69:\tlearn: 0.9486328\ttotal: 5.09s\tremaining: 4m 45s\n",
            "70:\tlearn: 0.9485918\ttotal: 5.17s\tremaining: 4m 45s\n",
            "71:\tlearn: 0.9491021\ttotal: 5.24s\tremaining: 4m 45s\n",
            "72:\tlearn: 0.9495397\ttotal: 5.3s\tremaining: 4m 45s\n",
            "73:\tlearn: 0.9495849\ttotal: 5.39s\tremaining: 4m 45s\n",
            "74:\tlearn: 0.9501771\ttotal: 5.46s\tremaining: 4m 45s\n",
            "75:\tlearn: 0.9502673\ttotal: 5.54s\tremaining: 4m 46s\n",
            "76:\tlearn: 0.9508568\ttotal: 5.6s\tremaining: 4m 45s\n",
            "77:\tlearn: 0.9506471\ttotal: 5.66s\tremaining: 4m 44s\n",
            "78:\tlearn: 0.9510454\ttotal: 5.74s\tremaining: 4m 44s\n",
            "79:\tlearn: 0.9512896\ttotal: 5.83s\tremaining: 4m 45s\n",
            "80:\tlearn: 0.9518382\ttotal: 5.89s\tremaining: 4m 45s\n",
            "81:\tlearn: 0.9519258\ttotal: 5.96s\tremaining: 4m 44s\n",
            "82:\tlearn: 0.9520975\ttotal: 6.02s\tremaining: 4m 44s\n",
            "83:\tlearn: 0.9517579\ttotal: 6.11s\tremaining: 4m 44s\n",
            "84:\tlearn: 0.9518909\ttotal: 6.19s\tremaining: 4m 45s\n",
            "85:\tlearn: 0.9522628\ttotal: 6.26s\tremaining: 4m 44s\n",
            "86:\tlearn: 0.9530571\ttotal: 6.35s\tremaining: 4m 45s\n",
            "87:\tlearn: 0.9526295\ttotal: 6.42s\tremaining: 4m 45s\n",
            "88:\tlearn: 0.9532927\ttotal: 6.5s\tremaining: 4m 45s\n",
            "89:\tlearn: 0.9536333\ttotal: 6.58s\tremaining: 4m 45s\n",
            "90:\tlearn: 0.9539055\ttotal: 6.65s\tremaining: 4m 45s\n",
            "91:\tlearn: 0.9537985\ttotal: 6.73s\tremaining: 4m 45s\n",
            "92:\tlearn: 0.9536038\ttotal: 6.81s\tremaining: 4m 46s\n",
            "93:\tlearn: 0.9537395\ttotal: 6.88s\tremaining: 4m 46s\n",
            "94:\tlearn: 0.9538365\ttotal: 6.96s\tremaining: 4m 45s\n",
            "95:\tlearn: 0.9540471\ttotal: 7.04s\tremaining: 4m 46s\n",
            "96:\tlearn: 0.9545012\ttotal: 7.1s\tremaining: 4m 45s\n",
            "97:\tlearn: 0.9549627\ttotal: 7.17s\tremaining: 4m 45s\n",
            "98:\tlearn: 0.9548129\ttotal: 7.23s\tremaining: 4m 44s\n",
            "99:\tlearn: 0.9545889\ttotal: 7.3s\tremaining: 4m 44s\n",
            "100:\tlearn: 0.9548572\ttotal: 7.38s\tremaining: 4m 44s\n",
            "101:\tlearn: 0.9549704\ttotal: 7.45s\tremaining: 4m 44s\n",
            "102:\tlearn: 0.9549544\ttotal: 7.52s\tremaining: 4m 44s\n",
            "103:\tlearn: 0.9551319\ttotal: 7.59s\tremaining: 4m 44s\n",
            "104:\tlearn: 0.9554561\ttotal: 7.65s\tremaining: 4m 43s\n",
            "105:\tlearn: 0.9566540\ttotal: 7.71s\tremaining: 4m 43s\n",
            "106:\tlearn: 0.9568876\ttotal: 7.78s\tremaining: 4m 43s\n",
            "107:\tlearn: 0.9568358\ttotal: 7.88s\tremaining: 4m 43s\n",
            "108:\tlearn: 0.9570664\ttotal: 7.94s\tremaining: 4m 43s\n",
            "109:\tlearn: 0.9572777\ttotal: 8.02s\tremaining: 4m 43s\n",
            "110:\tlearn: 0.9573201\ttotal: 8.11s\tremaining: 4m 43s\n",
            "111:\tlearn: 0.9578076\ttotal: 8.18s\tremaining: 4m 43s\n",
            "112:\tlearn: 0.9578562\ttotal: 8.25s\tremaining: 4m 43s\n",
            "113:\tlearn: 0.9582647\ttotal: 8.32s\tremaining: 4m 43s\n",
            "114:\tlearn: 0.9580310\ttotal: 8.38s\tremaining: 4m 43s\n",
            "115:\tlearn: 0.9580375\ttotal: 8.46s\tremaining: 4m 43s\n",
            "116:\tlearn: 0.9582007\ttotal: 8.54s\tremaining: 4m 43s\n",
            "117:\tlearn: 0.9586415\ttotal: 8.61s\tremaining: 4m 43s\n",
            "118:\tlearn: 0.9590416\ttotal: 8.69s\tremaining: 4m 43s\n",
            "119:\tlearn: 0.9595388\ttotal: 8.75s\tremaining: 4m 42s\n",
            "120:\tlearn: 0.9598340\ttotal: 8.82s\tremaining: 4m 42s\n",
            "121:\tlearn: 0.9603457\ttotal: 8.88s\tremaining: 4m 42s\n",
            "122:\tlearn: 0.9607450\ttotal: 8.96s\tremaining: 4m 42s\n",
            "123:\tlearn: 0.9615197\ttotal: 9.02s\tremaining: 4m 41s\n",
            "124:\tlearn: 0.9615483\ttotal: 9.08s\tremaining: 4m 41s\n",
            "125:\tlearn: 0.9615514\ttotal: 9.15s\tremaining: 4m 41s\n",
            "126:\tlearn: 0.9617155\ttotal: 9.22s\tremaining: 4m 41s\n",
            "127:\tlearn: 0.9621384\ttotal: 9.29s\tremaining: 4m 40s\n",
            "128:\tlearn: 0.9625703\ttotal: 9.38s\tremaining: 4m 41s\n",
            "129:\tlearn: 0.9626968\ttotal: 9.45s\tremaining: 4m 41s\n",
            "130:\tlearn: 0.9631679\ttotal: 9.51s\tremaining: 4m 41s\n",
            "131:\tlearn: 0.9637023\ttotal: 9.58s\tremaining: 4m 40s\n",
            "132:\tlearn: 0.9638702\ttotal: 9.64s\tremaining: 4m 40s\n",
            "133:\tlearn: 0.9644260\ttotal: 9.72s\tremaining: 4m 40s\n",
            "134:\tlearn: 0.9645897\ttotal: 9.79s\tremaining: 4m 40s\n",
            "135:\tlearn: 0.9647165\ttotal: 9.85s\tremaining: 4m 39s\n",
            "136:\tlearn: 0.9648758\ttotal: 9.91s\tremaining: 4m 39s\n",
            "137:\tlearn: 0.9651222\ttotal: 9.97s\tremaining: 4m 39s\n",
            "138:\tlearn: 0.9654847\ttotal: 10s\tremaining: 4m 38s\n",
            "139:\tlearn: 0.9659297\ttotal: 10.1s\tremaining: 4m 38s\n",
            "140:\tlearn: 0.9660113\ttotal: 10.2s\tremaining: 4m 38s\n",
            "141:\tlearn: 0.9662381\ttotal: 10.2s\tremaining: 4m 37s\n",
            "142:\tlearn: 0.9664303\ttotal: 10.3s\tremaining: 4m 37s\n",
            "143:\tlearn: 0.9666142\ttotal: 10.4s\tremaining: 4m 37s\n",
            "144:\tlearn: 0.9667062\ttotal: 10.4s\tremaining: 4m 37s\n",
            "145:\tlearn: 0.9668756\ttotal: 10.5s\tremaining: 4m 37s\n",
            "146:\tlearn: 0.9670564\ttotal: 10.6s\tremaining: 4m 37s\n",
            "147:\tlearn: 0.9671089\ttotal: 10.6s\tremaining: 4m 36s\n",
            "148:\tlearn: 0.9671007\ttotal: 10.7s\tremaining: 4m 36s\n",
            "149:\tlearn: 0.9673488\ttotal: 10.8s\tremaining: 4m 36s\n",
            "150:\tlearn: 0.9673866\ttotal: 10.8s\tremaining: 4m 36s\n",
            "151:\tlearn: 0.9675936\ttotal: 10.9s\tremaining: 4m 35s\n",
            "152:\tlearn: 0.9674622\ttotal: 11s\tremaining: 4m 35s\n",
            "153:\tlearn: 0.9676955\ttotal: 11s\tremaining: 4m 35s\n",
            "154:\tlearn: 0.9679287\ttotal: 11.1s\tremaining: 4m 35s\n",
            "155:\tlearn: 0.9680305\ttotal: 11.2s\tremaining: 4m 34s\n",
            "156:\tlearn: 0.9680652\ttotal: 11.2s\tremaining: 4m 35s\n",
            "157:\tlearn: 0.9682439\ttotal: 11.3s\tremaining: 4m 35s\n",
            "158:\tlearn: 0.9684675\ttotal: 11.4s\tremaining: 4m 35s\n",
            "159:\tlearn: 0.9684675\ttotal: 11.4s\tremaining: 4m 34s\n",
            "160:\tlearn: 0.9686583\ttotal: 11.5s\tremaining: 4m 34s\n",
            "161:\tlearn: 0.9690100\ttotal: 11.6s\tremaining: 4m 34s\n",
            "162:\tlearn: 0.9691417\ttotal: 11.6s\tremaining: 4m 34s\n",
            "163:\tlearn: 0.9692169\ttotal: 11.7s\tremaining: 4m 33s\n",
            "164:\tlearn: 0.9692827\ttotal: 11.8s\tremaining: 4m 33s\n",
            "165:\tlearn: 0.9692696\ttotal: 11.8s\tremaining: 4m 33s\n",
            "166:\tlearn: 0.9695765\ttotal: 11.9s\tremaining: 4m 33s\n",
            "167:\tlearn: 0.9698887\ttotal: 12s\tremaining: 4m 33s\n",
            "168:\tlearn: 0.9702166\ttotal: 12s\tremaining: 4m 32s\n",
            "169:\tlearn: 0.9705443\ttotal: 12.1s\tremaining: 4m 32s\n",
            "170:\tlearn: 0.9704155\ttotal: 12.2s\tremaining: 4m 32s\n",
            "171:\tlearn: 0.9705780\ttotal: 12.2s\tremaining: 4m 32s\n",
            "172:\tlearn: 0.9708034\ttotal: 12.3s\tremaining: 4m 31s\n",
            "173:\tlearn: 0.9709629\ttotal: 12.3s\tremaining: 4m 31s\n",
            "174:\tlearn: 0.9711355\ttotal: 12.4s\tremaining: 4m 31s\n",
            "175:\tlearn: 0.9712675\ttotal: 12.5s\tremaining: 4m 31s\n",
            "176:\tlearn: 0.9716800\ttotal: 12.5s\tremaining: 4m 30s\n",
            "177:\tlearn: 0.9716856\ttotal: 12.6s\tremaining: 4m 30s\n",
            "178:\tlearn: 0.9721058\ttotal: 12.7s\tremaining: 4m 30s\n",
            "179:\tlearn: 0.9722643\ttotal: 12.8s\tremaining: 4m 30s\n",
            "180:\tlearn: 0.9722502\ttotal: 12.8s\tremaining: 4m 30s\n",
            "181:\tlearn: 0.9724155\ttotal: 12.9s\tremaining: 4m 30s\n",
            "182:\tlearn: 0.9725817\ttotal: 12.9s\tremaining: 4m 29s\n",
            "183:\tlearn: 0.9727025\ttotal: 13s\tremaining: 4m 29s\n",
            "184:\tlearn: 0.9729687\ttotal: 13.1s\tremaining: 4m 29s\n",
            "185:\tlearn: 0.9732680\ttotal: 13.1s\tremaining: 4m 29s\n",
            "186:\tlearn: 0.9734401\ttotal: 13.2s\tremaining: 4m 28s\n",
            "187:\tlearn: 0.9735402\ttotal: 13.3s\tremaining: 4m 28s\n",
            "188:\tlearn: 0.9735468\ttotal: 13.3s\tremaining: 4m 28s\n",
            "189:\tlearn: 0.9740661\ttotal: 13.4s\tremaining: 4m 28s\n",
            "190:\tlearn: 0.9740866\ttotal: 13.4s\tremaining: 4m 28s\n",
            "191:\tlearn: 0.9742661\ttotal: 13.5s\tremaining: 4m 28s\n",
            "192:\tlearn: 0.9747515\ttotal: 13.6s\tremaining: 4m 28s\n",
            "193:\tlearn: 0.9751917\ttotal: 13.7s\tremaining: 4m 27s\n",
            "194:\tlearn: 0.9751906\ttotal: 13.7s\tremaining: 4m 27s\n",
            "195:\tlearn: 0.9754232\ttotal: 13.8s\tremaining: 4m 27s\n",
            "196:\tlearn: 0.9757434\ttotal: 13.9s\tremaining: 4m 27s\n",
            "197:\tlearn: 0.9757562\ttotal: 13.9s\tremaining: 4m 27s\n",
            "198:\tlearn: 0.9757160\ttotal: 14s\tremaining: 4m 27s\n",
            "199:\tlearn: 0.9755692\ttotal: 14.1s\tremaining: 4m 27s\n",
            "200:\tlearn: 0.9758628\ttotal: 14.1s\tremaining: 4m 27s\n",
            "201:\tlearn: 0.9761825\ttotal: 14.2s\tremaining: 4m 27s\n",
            "202:\tlearn: 0.9761957\ttotal: 14.3s\tremaining: 4m 27s\n",
            "203:\tlearn: 0.9762820\ttotal: 14.3s\tremaining: 4m 26s\n",
            "204:\tlearn: 0.9764151\ttotal: 14.4s\tremaining: 4m 26s\n",
            "205:\tlearn: 0.9764081\ttotal: 14.5s\tremaining: 4m 26s\n",
            "206:\tlearn: 0.9763812\ttotal: 14.5s\tremaining: 4m 26s\n",
            "207:\tlearn: 0.9765479\ttotal: 14.6s\tremaining: 4m 26s\n",
            "208:\tlearn: 0.9764679\ttotal: 14.7s\tremaining: 4m 26s\n",
            "209:\tlearn: 0.9767407\ttotal: 14.8s\tremaining: 4m 26s\n",
            "210:\tlearn: 0.9767939\ttotal: 14.8s\tremaining: 4m 26s\n",
            "211:\tlearn: 0.9771536\ttotal: 14.9s\tremaining: 4m 26s\n",
            "212:\tlearn: 0.9771469\ttotal: 15s\tremaining: 4m 25s\n",
            "213:\tlearn: 0.9772669\ttotal: 15s\tremaining: 4m 25s\n",
            "214:\tlearn: 0.9773201\ttotal: 15.1s\tremaining: 4m 25s\n",
            "215:\tlearn: 0.9774732\ttotal: 15.1s\tremaining: 4m 25s\n",
            "216:\tlearn: 0.9777928\ttotal: 15.2s\tremaining: 4m 25s\n",
            "217:\tlearn: 0.9779325\ttotal: 15.3s\tremaining: 4m 24s\n",
            "218:\tlearn: 0.9778859\ttotal: 15.3s\tremaining: 4m 24s\n",
            "219:\tlearn: 0.9781786\ttotal: 15.4s\tremaining: 4m 24s\n",
            "220:\tlearn: 0.9780653\ttotal: 15.5s\tremaining: 4m 24s\n",
            "221:\tlearn: 0.9784511\ttotal: 15.5s\tremaining: 4m 24s\n",
            "222:\tlearn: 0.9785376\ttotal: 15.6s\tremaining: 4m 24s\n",
            "223:\tlearn: 0.9785974\ttotal: 15.7s\tremaining: 4m 24s\n",
            "224:\tlearn: 0.9785110\ttotal: 15.7s\tremaining: 4m 24s\n",
            "225:\tlearn: 0.9783114\ttotal: 15.8s\tremaining: 4m 24s\n",
            "226:\tlearn: 0.9786440\ttotal: 15.9s\tremaining: 4m 23s\n",
            "227:\tlearn: 0.9786706\ttotal: 15.9s\tremaining: 4m 23s\n",
            "228:\tlearn: 0.9787038\ttotal: 16s\tremaining: 4m 23s\n",
            "229:\tlearn: 0.9788036\ttotal: 16.1s\tremaining: 4m 23s\n",
            "230:\tlearn: 0.9788634\ttotal: 16.2s\tremaining: 4m 23s\n",
            "231:\tlearn: 0.9791495\ttotal: 16.2s\tremaining: 4m 23s\n",
            "232:\tlearn: 0.9793290\ttotal: 16.3s\tremaining: 4m 24s\n",
            "233:\tlearn: 0.9794154\ttotal: 16.4s\tremaining: 4m 24s\n",
            "234:\tlearn: 0.9797079\ttotal: 16.5s\tremaining: 4m 24s\n",
            "235:\tlearn: 0.9797147\ttotal: 16.6s\tremaining: 4m 24s\n",
            "236:\tlearn: 0.9796681\ttotal: 16.6s\tremaining: 4m 24s\n",
            "237:\tlearn: 0.9799209\ttotal: 16.7s\tremaining: 4m 24s\n",
            "238:\tlearn: 0.9797946\ttotal: 16.8s\tremaining: 4m 24s\n",
            "239:\tlearn: 0.9799742\ttotal: 16.9s\tremaining: 4m 24s\n",
            "240:\tlearn: 0.9800075\ttotal: 16.9s\tremaining: 4m 23s\n",
            "241:\tlearn: 0.9800806\ttotal: 17s\tremaining: 4m 23s\n",
            "242:\tlearn: 0.9803002\ttotal: 17.1s\tremaining: 4m 23s\n",
            "243:\tlearn: 0.9802804\ttotal: 17.1s\tremaining: 4m 23s\n",
            "244:\tlearn: 0.9803337\ttotal: 17.2s\tremaining: 4m 23s\n",
            "245:\tlearn: 0.9803670\ttotal: 17.3s\tremaining: 4m 23s\n",
            "246:\tlearn: 0.9807525\ttotal: 17.3s\tremaining: 4m 23s\n",
            "247:\tlearn: 0.9809852\ttotal: 17.4s\tremaining: 4m 23s\n",
            "248:\tlearn: 0.9810452\ttotal: 17.5s\tremaining: 4m 23s\n",
            "249:\tlearn: 0.9809789\ttotal: 17.5s\tremaining: 4m 22s\n",
            "250:\tlearn: 0.9811049\ttotal: 17.6s\tremaining: 4m 23s\n",
            "251:\tlearn: 0.9812912\ttotal: 17.7s\tremaining: 4m 23s\n",
            "252:\tlearn: 0.9812249\ttotal: 17.8s\tremaining: 4m 23s\n",
            "253:\tlearn: 0.9811452\ttotal: 17.8s\tremaining: 4m 22s\n",
            "254:\tlearn: 0.9812445\ttotal: 17.9s\tremaining: 4m 22s\n",
            "255:\tlearn: 0.9813579\ttotal: 18s\tremaining: 4m 22s\n",
            "256:\tlearn: 0.9814509\ttotal: 18s\tremaining: 4m 22s\n",
            "257:\tlearn: 0.9817569\ttotal: 18.1s\tremaining: 4m 22s\n",
            "258:\tlearn: 0.9815113\ttotal: 18.2s\tremaining: 4m 22s\n",
            "259:\tlearn: 0.9817969\ttotal: 18.2s\tremaining: 4m 22s\n",
            "260:\tlearn: 0.9818303\ttotal: 18.3s\tremaining: 4m 22s\n",
            "261:\tlearn: 0.9818966\ttotal: 18.4s\tremaining: 4m 21s\n",
            "262:\tlearn: 0.9819962\ttotal: 18.4s\tremaining: 4m 21s\n",
            "263:\tlearn: 0.9819500\ttotal: 18.5s\tremaining: 4m 21s\n",
            "264:\tlearn: 0.9820835\ttotal: 18.6s\tremaining: 4m 21s\n",
            "265:\tlearn: 0.9822694\ttotal: 18.6s\tremaining: 4m 21s\n",
            "266:\tlearn: 0.9822961\ttotal: 18.7s\tremaining: 4m 21s\n",
            "267:\tlearn: 0.9821641\ttotal: 18.8s\tremaining: 4m 21s\n",
            "268:\tlearn: 0.9823233\ttotal: 18.8s\tremaining: 4m 21s\n",
            "269:\tlearn: 0.9824363\ttotal: 18.9s\tremaining: 4m 20s\n",
            "270:\tlearn: 0.9825293\ttotal: 18.9s\tremaining: 4m 20s\n",
            "271:\tlearn: 0.9824898\ttotal: 19s\tremaining: 4m 20s\n",
            "272:\tlearn: 0.9825961\ttotal: 19.1s\tremaining: 4m 20s\n",
            "273:\tlearn: 0.9826228\ttotal: 19.1s\tremaining: 4m 19s\n",
            "274:\tlearn: 0.9826495\ttotal: 19.2s\tremaining: 4m 19s\n",
            "275:\tlearn: 0.9828421\ttotal: 19.2s\tremaining: 4m 19s\n",
            "276:\tlearn: 0.9829690\ttotal: 19.3s\tremaining: 4m 19s\n",
            "277:\tlearn: 0.9830486\ttotal: 19.4s\tremaining: 4m 19s\n",
            "278:\tlearn: 0.9831749\ttotal: 19.5s\tremaining: 4m 19s\n",
            "279:\tlearn: 0.9833875\ttotal: 19.5s\tremaining: 4m 19s\n",
            "280:\tlearn: 0.9834938\ttotal: 19.6s\tremaining: 4m 19s\n",
            "281:\tlearn: 0.9835205\ttotal: 19.7s\tremaining: 4m 19s\n",
            "282:\tlearn: 0.9836595\ttotal: 19.8s\tremaining: 4m 19s\n",
            "283:\tlearn: 0.9838058\ttotal: 19.8s\tremaining: 4m 19s\n",
            "284:\tlearn: 0.9838318\ttotal: 19.9s\tremaining: 4m 19s\n",
            "285:\tlearn: 0.9837791\ttotal: 19.9s\tremaining: 4m 18s\n",
            "286:\tlearn: 0.9838526\ttotal: 20s\tremaining: 4m 18s\n",
            "287:\tlearn: 0.9840191\ttotal: 20.1s\tremaining: 4m 18s\n",
            "288:\tlearn: 0.9840324\ttotal: 20.1s\tremaining: 4m 18s\n",
            "289:\tlearn: 0.9841446\ttotal: 20.2s\tremaining: 4m 18s\n",
            "290:\tlearn: 0.9842115\ttotal: 20.3s\tremaining: 4m 18s\n",
            "291:\tlearn: 0.9842984\ttotal: 20.3s\tremaining: 4m 17s\n",
            "292:\tlearn: 0.9842725\ttotal: 20.4s\tremaining: 4m 17s\n",
            "293:\tlearn: 0.9845184\ttotal: 20.4s\tremaining: 4m 17s\n",
            "294:\tlearn: 0.9845251\ttotal: 20.5s\tremaining: 4m 17s\n",
            "295:\tlearn: 0.9845452\ttotal: 20.6s\tremaining: 4m 17s\n",
            "296:\tlearn: 0.9845853\ttotal: 20.6s\tremaining: 4m 17s\n",
            "297:\tlearn: 0.9845987\ttotal: 20.7s\tremaining: 4m 17s\n",
            "298:\tlearn: 0.9846447\ttotal: 20.8s\tremaining: 4m 17s\n",
            "299:\tlearn: 0.9846255\ttotal: 20.9s\tremaining: 4m 17s\n",
            "300:\tlearn: 0.9847451\ttotal: 20.9s\tremaining: 4m 16s\n",
            "301:\tlearn: 0.9851288\ttotal: 21s\tremaining: 4m 16s\n",
            "302:\tlearn: 0.9853603\ttotal: 21s\tremaining: 4m 16s\n",
            "303:\tlearn: 0.9853871\ttotal: 21.1s\tremaining: 4m 16s\n",
            "304:\tlearn: 0.9854540\ttotal: 21.2s\tremaining: 4m 16s\n",
            "305:\tlearn: 0.9855066\ttotal: 21.2s\tremaining: 4m 16s\n",
            "306:\tlearn: 0.9857781\ttotal: 21.3s\tremaining: 4m 15s\n",
            "307:\tlearn: 0.9857848\ttotal: 21.3s\tremaining: 4m 15s\n",
            "308:\tlearn: 0.9856921\ttotal: 21.4s\tremaining: 4m 15s\n",
            "309:\tlearn: 0.9858507\ttotal: 21.5s\tremaining: 4m 15s\n",
            "310:\tlearn: 0.9857858\ttotal: 21.5s\tremaining: 4m 15s\n",
            "311:\tlearn: 0.9860305\ttotal: 21.6s\tremaining: 4m 15s\n",
            "312:\tlearn: 0.9859121\ttotal: 21.7s\tremaining: 4m 15s\n",
            "313:\tlearn: 0.9860852\ttotal: 21.7s\tremaining: 4m 15s\n",
            "314:\tlearn: 0.9859935\ttotal: 21.8s\tremaining: 4m 15s\n",
            "315:\tlearn: 0.9860471\ttotal: 21.9s\tremaining: 4m 15s\n",
            "316:\tlearn: 0.9860471\ttotal: 21.9s\tremaining: 4m 15s\n",
            "317:\tlearn: 0.9862728\ttotal: 22s\tremaining: 4m 14s\n",
            "318:\tlearn: 0.9862527\ttotal: 22.1s\tremaining: 4m 14s\n",
            "319:\tlearn: 0.9864180\ttotal: 22.1s\tremaining: 4m 14s\n",
            "320:\tlearn: 0.9864381\ttotal: 22.2s\tremaining: 4m 14s\n",
            "321:\tlearn: 0.9866101\ttotal: 22.3s\tremaining: 4m 14s\n",
            "322:\tlearn: 0.9866827\ttotal: 22.3s\tremaining: 4m 14s\n",
            "323:\tlearn: 0.9867687\ttotal: 22.4s\tremaining: 4m 14s\n",
            "324:\tlearn: 0.9867095\ttotal: 22.5s\tremaining: 4m 13s\n",
            "325:\tlearn: 0.9867632\ttotal: 22.5s\tremaining: 4m 13s\n",
            "326:\tlearn: 0.9868448\ttotal: 22.6s\tremaining: 4m 13s\n",
            "327:\tlearn: 0.9869387\ttotal: 22.6s\tremaining: 4m 13s\n",
            "328:\tlearn: 0.9869521\ttotal: 22.7s\tremaining: 4m 13s\n",
            "329:\tlearn: 0.9869924\ttotal: 22.8s\tremaining: 4m 13s\n",
            "330:\tlearn: 0.9870137\ttotal: 22.8s\tremaining: 4m 12s\n",
            "331:\tlearn: 0.9868807\ttotal: 22.9s\tremaining: 4m 12s\n",
            "332:\tlearn: 0.9870863\ttotal: 22.9s\tremaining: 4m 12s\n",
            "333:\tlearn: 0.9870808\ttotal: 23s\tremaining: 4m 12s\n",
            "334:\tlearn: 0.9871010\ttotal: 23.1s\tremaining: 4m 12s\n",
            "335:\tlearn: 0.9870875\ttotal: 23.1s\tremaining: 4m 12s\n",
            "336:\tlearn: 0.9871010\ttotal: 23.2s\tremaining: 4m 12s\n",
            "337:\tlearn: 0.9871748\ttotal: 23.3s\tremaining: 4m 12s\n",
            "338:\tlearn: 0.9872352\ttotal: 23.4s\tremaining: 4m 12s\n",
            "339:\tlearn: 0.9875268\ttotal: 23.4s\tremaining: 4m 12s\n",
            "340:\tlearn: 0.9875603\ttotal: 23.5s\tremaining: 4m 12s\n",
            "341:\tlearn: 0.9875402\ttotal: 23.6s\tremaining: 4m 11s\n",
            "342:\tlearn: 0.9876342\ttotal: 23.6s\tremaining: 4m 11s\n",
            "343:\tlearn: 0.9877135\ttotal: 23.7s\tremaining: 4m 11s\n",
            "344:\tlearn: 0.9877403\ttotal: 23.8s\tremaining: 4m 11s\n",
            "345:\tlearn: 0.9879056\ttotal: 23.8s\tremaining: 4m 11s\n",
            "346:\tlearn: 0.9878720\ttotal: 23.9s\tremaining: 4m 11s\n",
            "347:\tlearn: 0.9878921\ttotal: 24s\tremaining: 4m 11s\n",
            "348:\tlearn: 0.9878330\ttotal: 24s\tremaining: 4m 11s\n",
            "349:\tlearn: 0.9878854\ttotal: 24.1s\tremaining: 4m 11s\n",
            "350:\tlearn: 0.9879593\ttotal: 24.2s\tremaining: 4m 11s\n",
            "351:\tlearn: 0.9879660\ttotal: 24.2s\tremaining: 4m 11s\n",
            "352:\tlearn: 0.9879459\ttotal: 24.3s\tremaining: 4m 11s\n",
            "353:\tlearn: 0.9881110\ttotal: 24.4s\tremaining: 4m 10s\n",
            "354:\tlearn: 0.9880775\ttotal: 24.4s\tremaining: 4m 10s\n",
            "355:\tlearn: 0.9881567\ttotal: 24.5s\tremaining: 4m 10s\n",
            "356:\tlearn: 0.9882238\ttotal: 24.6s\tremaining: 4m 10s\n",
            "357:\tlearn: 0.9883299\ttotal: 24.6s\tremaining: 4m 10s\n",
            "358:\tlearn: 0.9883165\ttotal: 24.7s\tremaining: 4m 10s\n",
            "359:\tlearn: 0.9883299\ttotal: 24.8s\tremaining: 4m 10s\n",
            "360:\tlearn: 0.9883366\ttotal: 24.8s\tremaining: 4m 10s\n",
            "361:\tlearn: 0.9885099\ttotal: 24.9s\tremaining: 4m 10s\n",
            "362:\tlearn: 0.9884374\ttotal: 25s\tremaining: 4m 10s\n",
            "363:\tlearn: 0.9884441\ttotal: 25s\tremaining: 4m 9s\n",
            "364:\tlearn: 0.9884979\ttotal: 25.1s\tremaining: 4m 9s\n",
            "365:\tlearn: 0.9885517\ttotal: 25.2s\tremaining: 4m 9s\n",
            "366:\tlearn: 0.9886592\ttotal: 25.2s\tremaining: 4m 9s\n",
            "367:\tlearn: 0.9886861\ttotal: 25.3s\tremaining: 4m 9s\n",
            "368:\tlearn: 0.9886271\ttotal: 25.4s\tremaining: 4m 9s\n",
            "369:\tlearn: 0.9886271\ttotal: 25.4s\tremaining: 4m 9s\n",
            "370:\tlearn: 0.9886472\ttotal: 25.5s\tremaining: 4m 9s\n",
            "371:\tlearn: 0.9887414\ttotal: 25.6s\tremaining: 4m 9s\n",
            "372:\tlearn: 0.9887347\ttotal: 25.6s\tremaining: 4m 9s\n",
            "373:\tlearn: 0.9887481\ttotal: 25.7s\tremaining: 4m 9s\n",
            "374:\tlearn: 0.9887952\ttotal: 25.8s\tremaining: 4m 8s\n",
            "375:\tlearn: 0.9887615\ttotal: 25.8s\tremaining: 4m 9s\n",
            "376:\tlearn: 0.9887817\ttotal: 25.9s\tremaining: 4m 8s\n",
            "377:\tlearn: 0.9888086\ttotal: 26s\tremaining: 4m 9s\n",
            "378:\tlearn: 0.9889013\ttotal: 26s\tremaining: 4m 8s\n",
            "379:\tlearn: 0.9889282\ttotal: 26.1s\tremaining: 4m 8s\n",
            "380:\tlearn: 0.9890007\ttotal: 26.2s\tremaining: 4m 8s\n",
            "381:\tlearn: 0.9890074\ttotal: 26.3s\tremaining: 4m 8s\n",
            "382:\tlearn: 0.9890276\ttotal: 26.3s\tremaining: 4m 8s\n",
            "383:\tlearn: 0.9889551\ttotal: 26.4s\tremaining: 4m 8s\n",
            "384:\tlearn: 0.9890679\ttotal: 26.5s\tremaining: 4m 8s\n",
            "385:\tlearn: 0.9890881\ttotal: 26.5s\tremaining: 4m 8s\n",
            "386:\tlearn: 0.9891150\ttotal: 26.6s\tremaining: 4m 8s\n",
            "387:\tlearn: 0.9891689\ttotal: 26.7s\tremaining: 4m 8s\n",
            "388:\tlearn: 0.9891487\ttotal: 26.7s\tremaining: 4m 8s\n",
            "389:\tlearn: 0.9891487\ttotal: 26.8s\tremaining: 4m 8s\n",
            "390:\tlearn: 0.9891487\ttotal: 26.9s\tremaining: 4m 8s\n",
            "391:\tlearn: 0.9891958\ttotal: 26.9s\tremaining: 4m 8s\n",
            "392:\tlearn: 0.9892160\ttotal: 27s\tremaining: 4m 7s\n",
            "393:\tlearn: 0.9891502\ttotal: 27.1s\tremaining: 4m 7s\n",
            "394:\tlearn: 0.9891637\ttotal: 27.2s\tremaining: 4m 7s\n",
            "395:\tlearn: 0.9892310\ttotal: 27.2s\tremaining: 4m 7s\n",
            "396:\tlearn: 0.9893842\ttotal: 27.3s\tremaining: 4m 7s\n",
            "397:\tlearn: 0.9893371\ttotal: 27.4s\tremaining: 4m 7s\n",
            "398:\tlearn: 0.9894365\ttotal: 27.4s\tremaining: 4m 7s\n",
            "399:\tlearn: 0.9894888\ttotal: 27.5s\tremaining: 4m 7s\n",
            "400:\tlearn: 0.9895814\ttotal: 27.6s\tremaining: 4m 7s\n",
            "401:\tlearn: 0.9896539\ttotal: 27.6s\tremaining: 4m 7s\n",
            "402:\tlearn: 0.9897801\ttotal: 27.7s\tremaining: 4m 7s\n",
            "403:\tlearn: 0.9898862\ttotal: 27.7s\tremaining: 4m 6s\n",
            "404:\tlearn: 0.9898391\ttotal: 27.8s\tremaining: 4m 7s\n",
            "405:\tlearn: 0.9898744\ttotal: 27.9s\tremaining: 4m 7s\n",
            "406:\tlearn: 0.9899872\ttotal: 28s\tremaining: 4m 6s\n",
            "407:\tlearn: 0.9899939\ttotal: 28s\tremaining: 4m 6s\n",
            "408:\tlearn: 0.9900815\ttotal: 28.1s\tremaining: 4m 6s\n",
            "409:\tlearn: 0.9901472\ttotal: 28.2s\tremaining: 4m 6s\n",
            "410:\tlearn: 0.9901000\ttotal: 28.2s\tremaining: 4m 6s\n",
            "411:\tlearn: 0.9901068\ttotal: 28.3s\tremaining: 4m 6s\n",
            "412:\tlearn: 0.9901270\ttotal: 28.4s\tremaining: 4m 6s\n",
            "413:\tlearn: 0.9901606\ttotal: 28.4s\tremaining: 4m 6s\n",
            "414:\tlearn: 0.9902011\ttotal: 28.5s\tremaining: 4m 6s\n",
            "415:\tlearn: 0.9902011\ttotal: 28.6s\tremaining: 4m 6s\n",
            "416:\tlearn: 0.9902937\ttotal: 28.6s\tremaining: 4m 6s\n",
            "417:\tlearn: 0.9903273\ttotal: 28.7s\tremaining: 4m 5s\n",
            "418:\tlearn: 0.9903071\ttotal: 28.8s\tremaining: 4m 5s\n",
            "419:\tlearn: 0.9903408\ttotal: 28.8s\tremaining: 4m 5s\n",
            "420:\tlearn: 0.9904065\ttotal: 28.9s\tremaining: 4m 5s\n",
            "421:\tlearn: 0.9904267\ttotal: 29s\tremaining: 4m 5s\n",
            "422:\tlearn: 0.9903660\ttotal: 29s\tremaining: 4m 5s\n",
            "423:\tlearn: 0.9903458\ttotal: 29.1s\tremaining: 4m 5s\n",
            "424:\tlearn: 0.9903610\ttotal: 29.2s\tremaining: 4m 5s\n",
            "425:\tlearn: 0.9904536\ttotal: 29.2s\tremaining: 4m 5s\n",
            "426:\tlearn: 0.9905597\ttotal: 29.3s\tremaining: 4m 5s\n",
            "427:\tlearn: 0.9906860\ttotal: 29.4s\tremaining: 4m 5s\n",
            "428:\tlearn: 0.9906927\ttotal: 29.4s\tremaining: 4m 5s\n",
            "429:\tlearn: 0.9907803\ttotal: 29.5s\tremaining: 4m 4s\n",
            "430:\tlearn: 0.9907668\ttotal: 29.6s\tremaining: 4m 4s\n",
            "431:\tlearn: 0.9908796\ttotal: 29.6s\tremaining: 4m 4s\n",
            "432:\tlearn: 0.9908661\ttotal: 29.7s\tremaining: 4m 4s\n",
            "433:\tlearn: 0.9908729\ttotal: 29.8s\tremaining: 4m 4s\n",
            "434:\tlearn: 0.9909538\ttotal: 29.8s\tremaining: 4m 4s\n",
            "435:\tlearn: 0.9910144\ttotal: 29.9s\tremaining: 4m 4s\n",
            "436:\tlearn: 0.9910482\ttotal: 30s\tremaining: 4m 4s\n",
            "437:\tlearn: 0.9911205\ttotal: 30s\tremaining: 4m 4s\n",
            "438:\tlearn: 0.9911475\ttotal: 30.1s\tremaining: 4m 3s\n",
            "439:\tlearn: 0.9911928\ttotal: 30.2s\tremaining: 4m 3s\n",
            "440:\tlearn: 0.9912131\ttotal: 30.2s\tremaining: 4m 3s\n",
            "441:\tlearn: 0.9911996\ttotal: 30.3s\tremaining: 4m 4s\n",
            "442:\tlearn: 0.9912333\ttotal: 30.4s\tremaining: 4m 4s\n",
            "443:\tlearn: 0.9912603\ttotal: 30.5s\tremaining: 4m 4s\n",
            "444:\tlearn: 0.9912603\ttotal: 30.5s\tremaining: 4m 4s\n",
            "445:\tlearn: 0.9913326\ttotal: 30.6s\tremaining: 4m 3s\n",
            "446:\tlearn: 0.9913461\ttotal: 30.7s\tremaining: 4m 3s\n",
            "447:\tlearn: 0.9915196\ttotal: 30.8s\tremaining: 4m 3s\n",
            "448:\tlearn: 0.9916189\ttotal: 30.8s\tremaining: 4m 3s\n",
            "449:\tlearn: 0.9915938\ttotal: 30.9s\tremaining: 4m 3s\n",
            "450:\tlearn: 0.9917065\ttotal: 31s\tremaining: 4m 3s\n",
            "451:\tlearn: 0.9916998\ttotal: 31s\tremaining: 4m 3s\n",
            "452:\tlearn: 0.9916863\ttotal: 31.1s\tremaining: 4m 3s\n",
            "453:\tlearn: 0.9917673\ttotal: 31.2s\tremaining: 4m 3s\n",
            "454:\tlearn: 0.9918261\ttotal: 31.3s\tremaining: 4m 3s\n",
            "455:\tlearn: 0.9918396\ttotal: 31.4s\tremaining: 4m 3s\n",
            "456:\tlearn: 0.9918801\ttotal: 31.5s\tremaining: 4m 3s\n",
            "457:\tlearn: 0.9918733\ttotal: 31.5s\tremaining: 4m 3s\n",
            "458:\tlearn: 0.9920044\ttotal: 31.6s\tremaining: 4m 3s\n",
            "459:\tlearn: 0.9918733\ttotal: 31.7s\tremaining: 4m 3s\n",
            "460:\tlearn: 0.9918935\ttotal: 31.7s\tremaining: 4m 3s\n",
            "461:\tlearn: 0.9919523\ttotal: 31.8s\tremaining: 4m 3s\n",
            "462:\tlearn: 0.9919861\ttotal: 31.9s\tremaining: 4m 3s\n",
            "463:\tlearn: 0.9920131\ttotal: 31.9s\tremaining: 4m 3s\n",
            "464:\tlearn: 0.9920401\ttotal: 32s\tremaining: 4m 3s\n",
            "465:\tlearn: 0.9921258\ttotal: 32.1s\tremaining: 4m 3s\n",
            "466:\tlearn: 0.9922068\ttotal: 32.2s\tremaining: 4m 3s\n",
            "467:\tlearn: 0.9923716\ttotal: 32.2s\tremaining: 4m 3s\n",
            "468:\tlearn: 0.9923649\ttotal: 32.3s\tremaining: 4m 2s\n",
            "469:\tlearn: 0.9924439\ttotal: 32.3s\tremaining: 4m 2s\n",
            "470:\tlearn: 0.9924506\ttotal: 32.4s\tremaining: 4m 2s\n",
            "471:\tlearn: 0.9925114\ttotal: 32.5s\tremaining: 4m 2s\n",
            "472:\tlearn: 0.9924864\ttotal: 32.5s\tremaining: 4m 2s\n",
            "473:\tlearn: 0.9925269\ttotal: 32.6s\tremaining: 4m 2s\n",
            "474:\tlearn: 0.9925134\ttotal: 32.6s\tremaining: 4m 2s\n",
            "475:\tlearn: 0.9925944\ttotal: 32.7s\tremaining: 4m 2s\n",
            "476:\tlearn: 0.9926869\ttotal: 32.8s\tremaining: 4m 2s\n",
            "477:\tlearn: 0.9926937\ttotal: 32.8s\tremaining: 4m 1s\n",
            "478:\tlearn: 0.9926012\ttotal: 32.9s\tremaining: 4m 2s\n",
            "479:\tlearn: 0.9927004\ttotal: 33s\tremaining: 4m 1s\n",
            "480:\tlearn: 0.9927275\ttotal: 33s\tremaining: 4m 1s\n",
            "481:\tlearn: 0.9927997\ttotal: 33.1s\tremaining: 4m 1s\n",
            "482:\tlearn: 0.9927025\ttotal: 33.2s\tremaining: 4m 1s\n",
            "483:\tlearn: 0.9927430\ttotal: 33.2s\tremaining: 4m 1s\n",
            "484:\tlearn: 0.9928153\ttotal: 33.3s\tremaining: 4m 1s\n",
            "485:\tlearn: 0.9929281\ttotal: 33.4s\tremaining: 4m 1s\n",
            "486:\tlearn: 0.9929618\ttotal: 33.5s\tremaining: 4m 1s\n",
            "487:\tlearn: 0.9929416\ttotal: 33.5s\tremaining: 4m 1s\n",
            "488:\tlearn: 0.9928355\ttotal: 33.6s\tremaining: 4m 1s\n",
            "489:\tlearn: 0.9928491\ttotal: 33.6s\tremaining: 4m 1s\n",
            "490:\tlearn: 0.9928828\ttotal: 33.7s\tremaining: 4m\n",
            "491:\tlearn: 0.9929234\ttotal: 33.8s\tremaining: 4m\n",
            "492:\tlearn: 0.9930024\ttotal: 33.8s\tremaining: 4m\n",
            "493:\tlearn: 0.9930159\ttotal: 33.9s\tremaining: 4m\n",
            "494:\tlearn: 0.9930024\ttotal: 34s\tremaining: 4m\n",
            "495:\tlearn: 0.9930024\ttotal: 34s\tremaining: 4m\n",
            "496:\tlearn: 0.9930294\ttotal: 34.1s\tremaining: 4m\n",
            "497:\tlearn: 0.9931287\ttotal: 34.2s\tremaining: 4m\n",
            "498:\tlearn: 0.9931422\ttotal: 34.3s\tremaining: 4m\n",
            "499:\tlearn: 0.9931692\ttotal: 34.3s\tremaining: 4m\n",
            "500:\tlearn: 0.9931827\ttotal: 34.4s\tremaining: 4m\n",
            "501:\tlearn: 0.9931692\ttotal: 34.5s\tremaining: 4m\n",
            "502:\tlearn: 0.9931962\ttotal: 34.5s\tremaining: 4m\n",
            "503:\tlearn: 0.9931443\ttotal: 34.6s\tremaining: 4m\n",
            "504:\tlearn: 0.9931308\ttotal: 34.7s\tremaining: 4m\n",
            "505:\tlearn: 0.9931511\ttotal: 34.8s\tremaining: 4m\n",
            "506:\tlearn: 0.9931781\ttotal: 34.8s\tremaining: 4m\n",
            "507:\tlearn: 0.9931781\ttotal: 34.9s\tremaining: 3m 59s\n",
            "508:\tlearn: 0.9931781\ttotal: 35s\tremaining: 3m 59s\n",
            "509:\tlearn: 0.9932119\ttotal: 35.1s\tremaining: 3m 59s\n",
            "510:\tlearn: 0.9932727\ttotal: 35.1s\tremaining: 3m 59s\n",
            "511:\tlearn: 0.9932863\ttotal: 35.2s\tremaining: 3m 59s\n",
            "512:\tlearn: 0.9933450\ttotal: 35.3s\tremaining: 3m 59s\n",
            "513:\tlearn: 0.9933855\ttotal: 35.4s\tremaining: 3m 59s\n",
            "514:\tlearn: 0.9933991\ttotal: 35.4s\tremaining: 3m 59s\n",
            "515:\tlearn: 0.9933788\ttotal: 35.5s\tremaining: 3m 59s\n",
            "516:\tlearn: 0.9933855\ttotal: 35.6s\tremaining: 3m 59s\n",
            "517:\tlearn: 0.9934193\ttotal: 35.6s\tremaining: 3m 59s\n",
            "518:\tlearn: 0.9934329\ttotal: 35.7s\tremaining: 3m 59s\n",
            "519:\tlearn: 0.9934464\ttotal: 35.8s\tremaining: 3m 59s\n",
            "520:\tlearn: 0.9934667\ttotal: 35.8s\tremaining: 3m 59s\n",
            "521:\tlearn: 0.9934937\ttotal: 35.9s\tremaining: 3m 59s\n",
            "522:\tlearn: 0.9934870\ttotal: 36s\tremaining: 3m 59s\n",
            "523:\tlearn: 0.9934870\ttotal: 36.1s\tremaining: 3m 59s\n",
            "524:\tlearn: 0.9935411\ttotal: 36.2s\tremaining: 3m 59s\n",
            "525:\tlearn: 0.9935884\ttotal: 36.3s\tremaining: 3m 59s\n",
            "526:\tlearn: 0.9935749\ttotal: 36.3s\tremaining: 3m 59s\n",
            "527:\tlearn: 0.9935546\ttotal: 36.4s\tremaining: 3m 59s\n",
            "528:\tlearn: 0.9935411\ttotal: 36.5s\tremaining: 3m 59s\n",
            "529:\tlearn: 0.9935952\ttotal: 36.5s\tremaining: 3m 59s\n",
            "530:\tlearn: 0.9935884\ttotal: 36.6s\tremaining: 3m 59s\n",
            "531:\tlearn: 0.9936290\ttotal: 36.7s\tremaining: 3m 59s\n",
            "532:\tlearn: 0.9936696\ttotal: 36.8s\tremaining: 3m 59s\n",
            "533:\tlearn: 0.9936764\ttotal: 36.8s\tremaining: 3m 59s\n",
            "534:\tlearn: 0.9936831\ttotal: 36.9s\tremaining: 3m 58s\n",
            "535:\tlearn: 0.9937373\ttotal: 37s\tremaining: 3m 58s\n",
            "536:\tlearn: 0.9936561\ttotal: 37s\tremaining: 3m 58s\n",
            "537:\tlearn: 0.9937486\ttotal: 37.1s\tremaining: 3m 58s\n",
            "538:\tlearn: 0.9938749\ttotal: 37.2s\tremaining: 3m 58s\n",
            "539:\tlearn: 0.9939155\ttotal: 37.2s\tremaining: 3m 58s\n",
            "540:\tlearn: 0.9940102\ttotal: 37.3s\tremaining: 3m 58s\n",
            "541:\tlearn: 0.9940306\ttotal: 37.4s\tremaining: 3m 58s\n",
            "542:\tlearn: 0.9940373\ttotal: 37.5s\tremaining: 3m 58s\n",
            "543:\tlearn: 0.9940441\ttotal: 37.5s\tremaining: 3m 58s\n",
            "544:\tlearn: 0.9940373\ttotal: 37.6s\tremaining: 3m 58s\n",
            "545:\tlearn: 0.9940509\ttotal: 37.6s\tremaining: 3m 58s\n",
            "546:\tlearn: 0.9940982\ttotal: 37.7s\tremaining: 3m 58s\n",
            "547:\tlearn: 0.9940847\ttotal: 37.8s\tremaining: 3m 58s\n",
            "548:\tlearn: 0.9940712\ttotal: 37.9s\tremaining: 3m 57s\n",
            "549:\tlearn: 0.9940644\ttotal: 37.9s\tremaining: 3m 57s\n",
            "550:\tlearn: 0.9941050\ttotal: 38s\tremaining: 3m 57s\n",
            "551:\tlearn: 0.9941050\ttotal: 38s\tremaining: 3m 57s\n",
            "552:\tlearn: 0.9941050\ttotal: 38.1s\tremaining: 3m 57s\n",
            "553:\tlearn: 0.9941253\ttotal: 38.2s\tremaining: 3m 57s\n",
            "554:\tlearn: 0.9941253\ttotal: 38.2s\tremaining: 3m 57s\n",
            "555:\tlearn: 0.9940779\ttotal: 38.3s\tremaining: 3m 57s\n",
            "556:\tlearn: 0.9940779\ttotal: 38.4s\tremaining: 3m 57s\n",
            "557:\tlearn: 0.9941253\ttotal: 38.4s\tremaining: 3m 57s\n",
            "558:\tlearn: 0.9941321\ttotal: 38.5s\tremaining: 3m 57s\n",
            "559:\tlearn: 0.9941659\ttotal: 38.6s\tremaining: 3m 56s\n",
            "560:\tlearn: 0.9941524\ttotal: 38.6s\tremaining: 3m 56s\n",
            "561:\tlearn: 0.9941795\ttotal: 38.7s\tremaining: 3m 56s\n",
            "562:\tlearn: 0.9941930\ttotal: 38.8s\tremaining: 3m 56s\n",
            "563:\tlearn: 0.9942133\ttotal: 38.8s\tremaining: 3m 56s\n",
            "564:\tlearn: 0.9942201\ttotal: 38.9s\tremaining: 3m 56s\n",
            "565:\tlearn: 0.9941795\ttotal: 39s\tremaining: 3m 56s\n",
            "566:\tlearn: 0.9942066\ttotal: 39.1s\tremaining: 3m 56s\n",
            "567:\tlearn: 0.9942991\ttotal: 39.1s\tremaining: 3m 56s\n",
            "568:\tlearn: 0.9942201\ttotal: 39.2s\tremaining: 3m 56s\n",
            "569:\tlearn: 0.9942788\ttotal: 39.2s\tremaining: 3m 56s\n",
            "570:\tlearn: 0.9942788\ttotal: 39.3s\tremaining: 3m 56s\n",
            "571:\tlearn: 0.9943058\ttotal: 39.4s\tremaining: 3m 55s\n",
            "572:\tlearn: 0.9943126\ttotal: 39.4s\tremaining: 3m 55s\n",
            "573:\tlearn: 0.9943194\ttotal: 39.5s\tremaining: 3m 55s\n",
            "574:\tlearn: 0.9942607\ttotal: 39.6s\tremaining: 3m 55s\n",
            "575:\tlearn: 0.9942675\ttotal: 39.7s\tremaining: 3m 55s\n",
            "576:\tlearn: 0.9942743\ttotal: 39.7s\tremaining: 3m 55s\n",
            "577:\tlearn: 0.9942607\ttotal: 39.8s\tremaining: 3m 55s\n",
            "578:\tlearn: 0.9944006\ttotal: 39.9s\tremaining: 3m 55s\n",
            "579:\tlearn: 0.9944006\ttotal: 39.9s\tremaining: 3m 55s\n",
            "580:\tlearn: 0.9943736\ttotal: 40s\tremaining: 3m 55s\n",
            "581:\tlearn: 0.9943668\ttotal: 40.1s\tremaining: 3m 55s\n",
            "582:\tlearn: 0.9944006\ttotal: 40.2s\tremaining: 3m 55s\n",
            "583:\tlearn: 0.9943284\ttotal: 40.2s\tremaining: 3m 55s\n",
            "584:\tlearn: 0.9943420\ttotal: 40.3s\tremaining: 3m 55s\n",
            "585:\tlearn: 0.9942878\ttotal: 40.4s\tremaining: 3m 55s\n",
            "586:\tlearn: 0.9943149\ttotal: 40.5s\tremaining: 3m 55s\n",
            "587:\tlearn: 0.9944751\ttotal: 40.5s\tremaining: 3m 55s\n",
            "588:\tlearn: 0.9944887\ttotal: 40.6s\tremaining: 3m 55s\n",
            "589:\tlearn: 0.9944887\ttotal: 40.7s\tremaining: 3m 55s\n",
            "590:\tlearn: 0.9945158\ttotal: 40.8s\tremaining: 3m 55s\n",
            "591:\tlearn: 0.9945090\ttotal: 40.8s\tremaining: 3m 54s\n",
            "592:\tlearn: 0.9945090\ttotal: 40.9s\tremaining: 3m 54s\n",
            "593:\tlearn: 0.9945700\ttotal: 40.9s\tremaining: 3m 54s\n",
            "594:\tlearn: 0.9945700\ttotal: 41s\tremaining: 3m 54s\n",
            "595:\tlearn: 0.9945971\ttotal: 41.1s\tremaining: 3m 54s\n",
            "596:\tlearn: 0.9945835\ttotal: 41.2s\tremaining: 3m 54s\n",
            "597:\tlearn: 0.9946174\ttotal: 41.2s\tremaining: 3m 54s\n",
            "598:\tlearn: 0.9946310\ttotal: 41.3s\tremaining: 3m 54s\n",
            "599:\tlearn: 0.9946377\ttotal: 41.4s\tremaining: 3m 54s\n",
            "600:\tlearn: 0.9946445\ttotal: 41.5s\tremaining: 3m 54s\n",
            "601:\tlearn: 0.9946513\ttotal: 41.5s\tremaining: 3m 54s\n",
            "602:\tlearn: 0.9946581\ttotal: 41.6s\tremaining: 3m 54s\n",
            "603:\tlearn: 0.9946716\ttotal: 41.7s\tremaining: 3m 54s\n",
            "604:\tlearn: 0.9947055\ttotal: 41.7s\tremaining: 3m 54s\n",
            "605:\tlearn: 0.9946852\ttotal: 41.8s\tremaining: 3m 54s\n",
            "606:\tlearn: 0.9947055\ttotal: 41.9s\tremaining: 3m 54s\n",
            "607:\tlearn: 0.9947190\ttotal: 42s\tremaining: 3m 54s\n",
            "608:\tlearn: 0.9947733\ttotal: 42s\tremaining: 3m 53s\n",
            "609:\tlearn: 0.9947800\ttotal: 42.1s\tremaining: 3m 53s\n",
            "610:\tlearn: 0.9948072\ttotal: 42.2s\tremaining: 3m 53s\n",
            "611:\tlearn: 0.9948004\ttotal: 42.3s\tremaining: 3m 53s\n",
            "612:\tlearn: 0.9948072\ttotal: 42.3s\tremaining: 3m 53s\n",
            "613:\tlearn: 0.9948275\ttotal: 42.4s\tremaining: 3m 53s\n",
            "614:\tlearn: 0.9948139\ttotal: 42.5s\tremaining: 3m 53s\n",
            "615:\tlearn: 0.9948275\ttotal: 42.5s\tremaining: 3m 53s\n",
            "616:\tlearn: 0.9948275\ttotal: 42.6s\tremaining: 3m 53s\n",
            "617:\tlearn: 0.9948478\ttotal: 42.7s\tremaining: 3m 53s\n",
            "618:\tlearn: 0.9948682\ttotal: 42.8s\tremaining: 3m 53s\n",
            "619:\tlearn: 0.9948953\ttotal: 42.8s\tremaining: 3m 53s\n",
            "620:\tlearn: 0.9948953\ttotal: 42.9s\tremaining: 3m 53s\n",
            "621:\tlearn: 0.9949021\ttotal: 43s\tremaining: 3m 53s\n",
            "622:\tlearn: 0.9949156\ttotal: 43s\tremaining: 3m 53s\n",
            "623:\tlearn: 0.9950081\ttotal: 43.1s\tremaining: 3m 53s\n",
            "624:\tlearn: 0.9949878\ttotal: 43.2s\tremaining: 3m 53s\n",
            "625:\tlearn: 0.9949878\ttotal: 43.2s\tremaining: 3m 52s\n",
            "626:\tlearn: 0.9950556\ttotal: 43.3s\tremaining: 3m 52s\n",
            "627:\tlearn: 0.9949902\ttotal: 43.4s\tremaining: 3m 52s\n",
            "628:\tlearn: 0.9948977\ttotal: 43.4s\tremaining: 3m 52s\n",
            "629:\tlearn: 0.9948977\ttotal: 43.5s\tremaining: 3m 52s\n",
            "630:\tlearn: 0.9948841\ttotal: 43.6s\tremaining: 3m 52s\n",
            "631:\tlearn: 0.9948977\ttotal: 43.6s\tremaining: 3m 52s\n",
            "632:\tlearn: 0.9949112\ttotal: 43.7s\tremaining: 3m 52s\n",
            "633:\tlearn: 0.9949316\ttotal: 43.8s\tremaining: 3m 52s\n",
            "634:\tlearn: 0.9949451\ttotal: 43.8s\tremaining: 3m 52s\n",
            "635:\tlearn: 0.9949316\ttotal: 43.9s\tremaining: 3m 52s\n",
            "636:\tlearn: 0.9950173\ttotal: 43.9s\tremaining: 3m 51s\n",
            "637:\tlearn: 0.9949790\ttotal: 44s\tremaining: 3m 51s\n",
            "638:\tlearn: 0.9949994\ttotal: 44.1s\tremaining: 3m 51s\n",
            "639:\tlearn: 0.9950061\ttotal: 44.1s\tremaining: 3m 51s\n",
            "640:\tlearn: 0.9950129\ttotal: 44.2s\tremaining: 3m 51s\n",
            "641:\tlearn: 0.9950265\ttotal: 44.3s\tremaining: 3m 51s\n",
            "642:\tlearn: 0.9951777\ttotal: 44.4s\tremaining: 3m 51s\n",
            "643:\tlearn: 0.9952430\ttotal: 44.4s\tremaining: 3m 51s\n",
            "644:\tlearn: 0.9951912\ttotal: 44.5s\tremaining: 3m 51s\n",
            "645:\tlearn: 0.9952658\ttotal: 44.5s\tremaining: 3m 51s\n",
            "646:\tlearn: 0.9952590\ttotal: 44.6s\tremaining: 3m 51s\n",
            "647:\tlearn: 0.9952319\ttotal: 44.7s\tremaining: 3m 50s\n",
            "648:\tlearn: 0.9952387\ttotal: 44.7s\tremaining: 3m 50s\n",
            "649:\tlearn: 0.9953177\ttotal: 44.8s\tremaining: 3m 50s\n",
            "650:\tlearn: 0.9952930\ttotal: 44.9s\tremaining: 3m 50s\n",
            "651:\tlearn: 0.9953065\ttotal: 44.9s\tremaining: 3m 50s\n",
            "652:\tlearn: 0.9953133\ttotal: 45s\tremaining: 3m 50s\n",
            "653:\tlearn: 0.9953540\ttotal: 45s\tremaining: 3m 50s\n",
            "654:\tlearn: 0.9953676\ttotal: 45.2s\tremaining: 3m 50s\n",
            "655:\tlearn: 0.9953676\ttotal: 45.2s\tremaining: 3m 50s\n",
            "656:\tlearn: 0.9953879\ttotal: 45.3s\tremaining: 3m 50s\n",
            "657:\tlearn: 0.9954287\ttotal: 45.3s\tremaining: 3m 50s\n",
            "658:\tlearn: 0.9954287\ttotal: 45.4s\tremaining: 3m 50s\n",
            "659:\tlearn: 0.9954490\ttotal: 45.5s\tremaining: 3m 50s\n",
            "660:\tlearn: 0.9954897\ttotal: 45.6s\tremaining: 3m 50s\n",
            "661:\tlearn: 0.9954762\ttotal: 45.6s\tremaining: 3m 50s\n",
            "662:\tlearn: 0.9954422\ttotal: 45.7s\tremaining: 3m 49s\n",
            "663:\tlearn: 0.9955551\ttotal: 45.8s\tremaining: 3m 49s\n",
            "664:\tlearn: 0.9955755\ttotal: 45.8s\tremaining: 3m 49s\n",
            "665:\tlearn: 0.9955823\ttotal: 45.9s\tremaining: 3m 49s\n",
            "666:\tlearn: 0.9956094\ttotal: 46s\tremaining: 3m 49s\n",
            "667:\tlearn: 0.9956230\ttotal: 46s\tremaining: 3m 49s\n",
            "668:\tlearn: 0.9956230\ttotal: 46.1s\tremaining: 3m 49s\n",
            "669:\tlearn: 0.9956501\ttotal: 46.2s\tremaining: 3m 49s\n",
            "670:\tlearn: 0.9956366\ttotal: 46.3s\tremaining: 3m 49s\n",
            "671:\tlearn: 0.9956501\ttotal: 46.3s\tremaining: 3m 49s\n",
            "672:\tlearn: 0.9956366\ttotal: 46.4s\tremaining: 3m 49s\n",
            "673:\tlearn: 0.9956433\ttotal: 46.5s\tremaining: 3m 49s\n",
            "674:\tlearn: 0.9956501\ttotal: 46.5s\tremaining: 3m 49s\n",
            "675:\tlearn: 0.9956637\ttotal: 46.6s\tremaining: 3m 49s\n",
            "676:\tlearn: 0.9956773\ttotal: 46.7s\tremaining: 3m 49s\n",
            "677:\tlearn: 0.9957316\ttotal: 46.7s\tremaining: 3m 49s\n",
            "678:\tlearn: 0.9957180\ttotal: 46.8s\tremaining: 3m 48s\n",
            "679:\tlearn: 0.9957248\ttotal: 46.9s\tremaining: 3m 48s\n",
            "680:\tlearn: 0.9957248\ttotal: 46.9s\tremaining: 3m 48s\n",
            "681:\tlearn: 0.9957248\ttotal: 47s\tremaining: 3m 48s\n",
            "682:\tlearn: 0.9957452\ttotal: 47.1s\tremaining: 3m 48s\n",
            "683:\tlearn: 0.9957587\ttotal: 47.1s\tremaining: 3m 48s\n",
            "684:\tlearn: 0.9957520\ttotal: 47.2s\tremaining: 3m 48s\n",
            "685:\tlearn: 0.9957316\ttotal: 47.3s\tremaining: 3m 48s\n",
            "686:\tlearn: 0.9957452\ttotal: 47.3s\tremaining: 3m 48s\n",
            "687:\tlearn: 0.9957587\ttotal: 47.4s\tremaining: 3m 48s\n",
            "688:\tlearn: 0.9957927\ttotal: 47.5s\tremaining: 3m 48s\n",
            "689:\tlearn: 0.9957995\ttotal: 47.5s\tremaining: 3m 47s\n",
            "690:\tlearn: 0.9958063\ttotal: 47.6s\tremaining: 3m 47s\n",
            "691:\tlearn: 0.9958674\ttotal: 47.7s\tremaining: 3m 47s\n",
            "692:\tlearn: 0.9958674\ttotal: 47.7s\tremaining: 3m 47s\n",
            "693:\tlearn: 0.9958538\ttotal: 47.8s\tremaining: 3m 47s\n",
            "694:\tlearn: 0.9958674\ttotal: 47.9s\tremaining: 3m 47s\n",
            "695:\tlearn: 0.9958810\ttotal: 47.9s\tremaining: 3m 47s\n",
            "696:\tlearn: 0.9958742\ttotal: 48s\tremaining: 3m 47s\n",
            "697:\tlearn: 0.9958606\ttotal: 48s\tremaining: 3m 47s\n",
            "698:\tlearn: 0.9958877\ttotal: 48.1s\tremaining: 3m 47s\n",
            "699:\tlearn: 0.9958810\ttotal: 48.2s\tremaining: 3m 47s\n",
            "700:\tlearn: 0.9958877\ttotal: 48.3s\tremaining: 3m 47s\n",
            "701:\tlearn: 0.9959013\ttotal: 48.3s\tremaining: 3m 46s\n",
            "702:\tlearn: 0.9959081\ttotal: 48.4s\tremaining: 3m 46s\n",
            "703:\tlearn: 0.9959013\ttotal: 48.4s\tremaining: 3m 46s\n",
            "704:\tlearn: 0.9959081\ttotal: 48.5s\tremaining: 3m 46s\n",
            "705:\tlearn: 0.9959149\ttotal: 48.6s\tremaining: 3m 46s\n",
            "706:\tlearn: 0.9959489\ttotal: 48.6s\tremaining: 3m 46s\n",
            "707:\tlearn: 0.9959625\ttotal: 48.7s\tremaining: 3m 46s\n",
            "708:\tlearn: 0.9959489\ttotal: 48.7s\tremaining: 3m 46s\n",
            "709:\tlearn: 0.9959489\ttotal: 48.8s\tremaining: 3m 46s\n",
            "710:\tlearn: 0.9959692\ttotal: 48.9s\tremaining: 3m 46s\n",
            "711:\tlearn: 0.9959625\ttotal: 48.9s\tremaining: 3m 46s\n",
            "712:\tlearn: 0.9959964\ttotal: 49s\tremaining: 3m 45s\n",
            "713:\tlearn: 0.9960032\ttotal: 49.1s\tremaining: 3m 45s\n",
            "714:\tlearn: 0.9960168\ttotal: 49.2s\tremaining: 3m 45s\n",
            "715:\tlearn: 0.9960440\ttotal: 49.2s\tremaining: 3m 45s\n",
            "716:\tlearn: 0.9960440\ttotal: 49.3s\tremaining: 3m 45s\n",
            "717:\tlearn: 0.9960440\ttotal: 49.4s\tremaining: 3m 45s\n",
            "718:\tlearn: 0.9960508\ttotal: 49.4s\tremaining: 3m 45s\n",
            "719:\tlearn: 0.9960643\ttotal: 49.5s\tremaining: 3m 45s\n",
            "720:\tlearn: 0.9960575\ttotal: 49.6s\tremaining: 3m 45s\n",
            "721:\tlearn: 0.9960643\ttotal: 49.6s\tremaining: 3m 45s\n",
            "722:\tlearn: 0.9960508\ttotal: 49.7s\tremaining: 3m 45s\n",
            "723:\tlearn: 0.9960508\ttotal: 49.8s\tremaining: 3m 45s\n",
            "724:\tlearn: 0.9960643\ttotal: 49.8s\tremaining: 3m 45s\n",
            "725:\tlearn: 0.9960711\ttotal: 49.9s\tremaining: 3m 44s\n",
            "726:\tlearn: 0.9960847\ttotal: 49.9s\tremaining: 3m 44s\n",
            "727:\tlearn: 0.9961187\ttotal: 50s\tremaining: 3m 44s\n",
            "728:\tlearn: 0.9961976\ttotal: 50.1s\tremaining: 3m 44s\n",
            "729:\tlearn: 0.9962044\ttotal: 50.1s\tremaining: 3m 44s\n",
            "730:\tlearn: 0.9961976\ttotal: 50.2s\tremaining: 3m 44s\n",
            "731:\tlearn: 0.9962044\ttotal: 50.3s\tremaining: 3m 44s\n",
            "732:\tlearn: 0.9962112\ttotal: 50.3s\tremaining: 3m 44s\n",
            "733:\tlearn: 0.9962112\ttotal: 50.4s\tremaining: 3m 44s\n",
            "734:\tlearn: 0.9962248\ttotal: 50.5s\tremaining: 3m 44s\n",
            "735:\tlearn: 0.9961976\ttotal: 50.5s\tremaining: 3m 44s\n",
            "736:\tlearn: 0.9962180\ttotal: 50.6s\tremaining: 3m 43s\n",
            "737:\tlearn: 0.9962248\ttotal: 50.7s\tremaining: 3m 43s\n",
            "738:\tlearn: 0.9962180\ttotal: 50.7s\tremaining: 3m 43s\n",
            "739:\tlearn: 0.9962384\ttotal: 50.8s\tremaining: 3m 43s\n",
            "740:\tlearn: 0.9962248\ttotal: 50.9s\tremaining: 3m 43s\n",
            "741:\tlearn: 0.9962384\ttotal: 50.9s\tremaining: 3m 43s\n",
            "742:\tlearn: 0.9962384\ttotal: 51s\tremaining: 3m 43s\n",
            "743:\tlearn: 0.9962452\ttotal: 51.1s\tremaining: 3m 43s\n",
            "744:\tlearn: 0.9962316\ttotal: 51.1s\tremaining: 3m 43s\n",
            "745:\tlearn: 0.9962452\ttotal: 51.2s\tremaining: 3m 43s\n",
            "746:\tlearn: 0.9962452\ttotal: 51.3s\tremaining: 3m 43s\n",
            "747:\tlearn: 0.9962860\ttotal: 51.3s\tremaining: 3m 43s\n",
            "748:\tlearn: 0.9962792\ttotal: 51.4s\tremaining: 3m 43s\n",
            "749:\tlearn: 0.9962792\ttotal: 51.4s\tremaining: 3m 42s\n",
            "750:\tlearn: 0.9963581\ttotal: 51.5s\tremaining: 3m 42s\n",
            "751:\tlearn: 0.9963785\ttotal: 51.6s\tremaining: 3m 42s\n",
            "752:\tlearn: 0.9963717\ttotal: 51.6s\tremaining: 3m 42s\n",
            "753:\tlearn: 0.9963921\ttotal: 51.7s\tremaining: 3m 42s\n",
            "754:\tlearn: 0.9963785\ttotal: 51.8s\tremaining: 3m 42s\n",
            "755:\tlearn: 0.9963989\ttotal: 51.8s\tremaining: 3m 42s\n",
            "756:\tlearn: 0.9963853\ttotal: 51.9s\tremaining: 3m 42s\n",
            "757:\tlearn: 0.9964057\ttotal: 52s\tremaining: 3m 42s\n",
            "758:\tlearn: 0.9964329\ttotal: 52s\tremaining: 3m 42s\n",
            "759:\tlearn: 0.9964533\ttotal: 52.1s\tremaining: 3m 42s\n",
            "760:\tlearn: 0.9964669\ttotal: 52.2s\tremaining: 3m 42s\n",
            "761:\tlearn: 0.9964669\ttotal: 52.2s\tremaining: 3m 41s\n",
            "762:\tlearn: 0.9964669\ttotal: 52.3s\tremaining: 3m 41s\n",
            "763:\tlearn: 0.9965144\ttotal: 52.4s\tremaining: 3m 41s\n",
            "764:\tlearn: 0.9965144\ttotal: 52.4s\tremaining: 3m 41s\n",
            "765:\tlearn: 0.9965008\ttotal: 52.5s\tremaining: 3m 41s\n",
            "766:\tlearn: 0.9965144\ttotal: 52.6s\tremaining: 3m 41s\n",
            "767:\tlearn: 0.9965076\ttotal: 52.7s\tremaining: 3m 41s\n",
            "768:\tlearn: 0.9965144\ttotal: 52.7s\tremaining: 3m 41s\n",
            "769:\tlearn: 0.9965348\ttotal: 52.8s\tremaining: 3m 41s\n",
            "770:\tlearn: 0.9965688\ttotal: 52.9s\tremaining: 3m 41s\n",
            "771:\tlearn: 0.9965620\ttotal: 52.9s\tremaining: 3m 41s\n",
            "772:\tlearn: 0.9965552\ttotal: 53s\tremaining: 3m 41s\n",
            "773:\tlearn: 0.9965960\ttotal: 53.1s\tremaining: 3m 41s\n",
            "774:\tlearn: 0.9965960\ttotal: 53.2s\tremaining: 3m 41s\n",
            "775:\tlearn: 0.9966028\ttotal: 53.2s\tremaining: 3m 41s\n",
            "776:\tlearn: 0.9966096\ttotal: 53.3s\tremaining: 3m 41s\n",
            "777:\tlearn: 0.9966232\ttotal: 53.4s\tremaining: 3m 41s\n",
            "778:\tlearn: 0.9966232\ttotal: 53.4s\tremaining: 3m 40s\n",
            "779:\tlearn: 0.9966300\ttotal: 53.5s\tremaining: 3m 40s\n",
            "780:\tlearn: 0.9966164\ttotal: 53.6s\tremaining: 3m 40s\n",
            "781:\tlearn: 0.9966164\ttotal: 53.7s\tremaining: 3m 40s\n",
            "782:\tlearn: 0.9966164\ttotal: 53.7s\tremaining: 3m 40s\n",
            "783:\tlearn: 0.9966368\ttotal: 53.8s\tremaining: 3m 40s\n",
            "784:\tlearn: 0.9966504\ttotal: 53.9s\tremaining: 3m 40s\n",
            "785:\tlearn: 0.9966708\ttotal: 53.9s\tremaining: 3m 40s\n",
            "786:\tlearn: 0.9966708\ttotal: 54s\tremaining: 3m 40s\n",
            "787:\tlearn: 0.9966776\ttotal: 54s\tremaining: 3m 40s\n",
            "788:\tlearn: 0.9966912\ttotal: 54.1s\tremaining: 3m 40s\n",
            "789:\tlearn: 0.9967116\ttotal: 54.2s\tremaining: 3m 40s\n",
            "790:\tlearn: 0.9967048\ttotal: 54.2s\tremaining: 3m 40s\n",
            "791:\tlearn: 0.9967048\ttotal: 54.3s\tremaining: 3m 39s\n",
            "792:\tlearn: 0.9967388\ttotal: 54.4s\tremaining: 3m 39s\n",
            "793:\tlearn: 0.9967524\ttotal: 54.4s\tremaining: 3m 39s\n",
            "794:\tlearn: 0.9967660\ttotal: 54.5s\tremaining: 3m 39s\n",
            "795:\tlearn: 0.9967728\ttotal: 54.6s\tremaining: 3m 39s\n",
            "796:\tlearn: 0.9967796\ttotal: 54.6s\tremaining: 3m 39s\n",
            "797:\tlearn: 0.9967932\ttotal: 54.7s\tremaining: 3m 39s\n",
            "798:\tlearn: 0.9967932\ttotal: 54.7s\tremaining: 3m 39s\n",
            "799:\tlearn: 0.9967932\ttotal: 54.8s\tremaining: 3m 39s\n",
            "800:\tlearn: 0.9968068\ttotal: 54.9s\tremaining: 3m 39s\n",
            "801:\tlearn: 0.9968000\ttotal: 54.9s\tremaining: 3m 39s\n",
            "802:\tlearn: 0.9968068\ttotal: 55s\tremaining: 3m 38s\n",
            "803:\tlearn: 0.9968136\ttotal: 55s\tremaining: 3m 38s\n",
            "804:\tlearn: 0.9968068\ttotal: 55.1s\tremaining: 3m 38s\n",
            "805:\tlearn: 0.9968612\ttotal: 55.2s\tremaining: 3m 38s\n",
            "806:\tlearn: 0.9968340\ttotal: 55.3s\tremaining: 3m 38s\n",
            "807:\tlearn: 0.9968136\ttotal: 55.4s\tremaining: 3m 38s\n",
            "808:\tlearn: 0.9968544\ttotal: 55.4s\tremaining: 3m 38s\n",
            "809:\tlearn: 0.9968680\ttotal: 55.5s\tremaining: 3m 38s\n",
            "810:\tlearn: 0.9968680\ttotal: 55.6s\tremaining: 3m 38s\n",
            "811:\tlearn: 0.9968884\ttotal: 55.6s\tremaining: 3m 38s\n",
            "812:\tlearn: 0.9969497\ttotal: 55.7s\tremaining: 3m 38s\n",
            "813:\tlearn: 0.9969224\ttotal: 55.7s\tremaining: 3m 38s\n",
            "814:\tlearn: 0.9969565\ttotal: 55.8s\tremaining: 3m 38s\n",
            "815:\tlearn: 0.9969701\ttotal: 55.9s\tremaining: 3m 38s\n",
            "816:\tlearn: 0.9969565\ttotal: 56s\tremaining: 3m 37s\n",
            "817:\tlearn: 0.9969565\ttotal: 56s\tremaining: 3m 37s\n",
            "818:\tlearn: 0.9969701\ttotal: 56.1s\tremaining: 3m 37s\n",
            "819:\tlearn: 0.9970041\ttotal: 56.2s\tremaining: 3m 37s\n",
            "820:\tlearn: 0.9969633\ttotal: 56.2s\tremaining: 3m 37s\n",
            "821:\tlearn: 0.9969701\ttotal: 56.3s\tremaining: 3m 37s\n",
            "822:\tlearn: 0.9970109\ttotal: 56.4s\tremaining: 3m 37s\n",
            "823:\tlearn: 0.9970313\ttotal: 56.4s\tremaining: 3m 37s\n",
            "824:\tlearn: 0.9970245\ttotal: 56.5s\tremaining: 3m 37s\n",
            "825:\tlearn: 0.9970381\ttotal: 56.6s\tremaining: 3m 37s\n",
            "826:\tlearn: 0.9970381\ttotal: 56.6s\tremaining: 3m 37s\n",
            "827:\tlearn: 0.9970313\ttotal: 56.7s\tremaining: 3m 37s\n",
            "828:\tlearn: 0.9970449\ttotal: 56.8s\tremaining: 3m 37s\n",
            "829:\tlearn: 0.9970313\ttotal: 57s\tremaining: 3m 37s\n",
            "830:\tlearn: 0.9970653\ttotal: 57.3s\tremaining: 3m 38s\n",
            "831:\tlearn: 0.9970653\ttotal: 57.4s\tremaining: 3m 38s\n",
            "832:\tlearn: 0.9970925\ttotal: 57.5s\tremaining: 3m 38s\n",
            "833:\tlearn: 0.9970653\ttotal: 57.6s\tremaining: 3m 38s\n",
            "834:\tlearn: 0.9970789\ttotal: 57.7s\tremaining: 3m 38s\n",
            "835:\tlearn: 0.9970857\ttotal: 57.7s\tremaining: 3m 38s\n",
            "836:\tlearn: 0.9971198\ttotal: 57.8s\tremaining: 3m 38s\n",
            "837:\tlearn: 0.9971334\ttotal: 57.9s\tremaining: 3m 38s\n",
            "838:\tlearn: 0.9971402\ttotal: 58s\tremaining: 3m 38s\n",
            "839:\tlearn: 0.9971402\ttotal: 58.1s\tremaining: 3m 38s\n",
            "840:\tlearn: 0.9971538\ttotal: 58.3s\tremaining: 3m 38s\n",
            "841:\tlearn: 0.9971674\ttotal: 58.4s\tremaining: 3m 39s\n",
            "842:\tlearn: 0.9971810\ttotal: 58.5s\tremaining: 3m 39s\n",
            "843:\tlearn: 0.9971742\ttotal: 58.6s\tremaining: 3m 39s\n",
            "844:\tlearn: 0.9971810\ttotal: 58.7s\tremaining: 3m 39s\n",
            "845:\tlearn: 0.9971810\ttotal: 58.7s\tremaining: 3m 39s\n",
            "846:\tlearn: 0.9971878\ttotal: 58.8s\tremaining: 3m 38s\n",
            "847:\tlearn: 0.9971946\ttotal: 58.9s\tremaining: 3m 38s\n",
            "848:\tlearn: 0.9972014\ttotal: 58.9s\tremaining: 3m 38s\n",
            "849:\tlearn: 0.9972014\ttotal: 59s\tremaining: 3m 38s\n",
            "850:\tlearn: 0.9972082\ttotal: 59.1s\tremaining: 3m 38s\n",
            "851:\tlearn: 0.9972151\ttotal: 59.1s\tremaining: 3m 38s\n",
            "852:\tlearn: 0.9972082\ttotal: 59.2s\tremaining: 3m 38s\n",
            "853:\tlearn: 0.9971946\ttotal: 59.3s\tremaining: 3m 38s\n",
            "854:\tlearn: 0.9972151\ttotal: 59.3s\tremaining: 3m 38s\n",
            "855:\tlearn: 0.9972082\ttotal: 59.4s\tremaining: 3m 38s\n",
            "856:\tlearn: 0.9972082\ttotal: 59.4s\tremaining: 3m 38s\n",
            "857:\tlearn: 0.9972219\ttotal: 59.5s\tremaining: 3m 37s\n",
            "858:\tlearn: 0.9972287\ttotal: 59.6s\tremaining: 3m 37s\n",
            "859:\tlearn: 0.9972219\ttotal: 59.6s\tremaining: 3m 37s\n",
            "860:\tlearn: 0.9972287\ttotal: 59.7s\tremaining: 3m 37s\n",
            "861:\tlearn: 0.9972287\ttotal: 59.8s\tremaining: 3m 37s\n",
            "862:\tlearn: 0.9972355\ttotal: 59.8s\tremaining: 3m 37s\n",
            "863:\tlearn: 0.9972287\ttotal: 59.9s\tremaining: 3m 37s\n",
            "864:\tlearn: 0.9972355\ttotal: 1m\tremaining: 3m 37s\n",
            "865:\tlearn: 0.9972355\ttotal: 1m\tremaining: 3m 37s\n",
            "866:\tlearn: 0.9972355\ttotal: 1m\tremaining: 3m 37s\n",
            "867:\tlearn: 0.9972559\ttotal: 1m\tremaining: 3m 37s\n",
            "868:\tlearn: 0.9972627\ttotal: 1m\tremaining: 3m 37s\n",
            "869:\tlearn: 0.9972763\ttotal: 1m\tremaining: 3m 37s\n",
            "870:\tlearn: 0.9972899\ttotal: 1m\tremaining: 3m 37s\n",
            "871:\tlearn: 0.9973035\ttotal: 1m\tremaining: 3m 37s\n",
            "872:\tlearn: 0.9973172\ttotal: 1m\tremaining: 3m 36s\n",
            "873:\tlearn: 0.9973172\ttotal: 1m\tremaining: 3m 36s\n",
            "874:\tlearn: 0.9973308\ttotal: 1m\tremaining: 3m 36s\n",
            "875:\tlearn: 0.9972967\ttotal: 1m\tremaining: 3m 36s\n",
            "876:\tlearn: 0.9972967\ttotal: 1m\tremaining: 3m 36s\n",
            "877:\tlearn: 0.9972967\ttotal: 1m\tremaining: 3m 36s\n",
            "878:\tlearn: 0.9973240\ttotal: 1m\tremaining: 3m 36s\n",
            "879:\tlearn: 0.9973376\ttotal: 1m 1s\tremaining: 3m 36s\n",
            "880:\tlearn: 0.9973444\ttotal: 1m 1s\tremaining: 3m 36s\n",
            "881:\tlearn: 0.9973308\ttotal: 1m 1s\tremaining: 3m 36s\n",
            "882:\tlearn: 0.9973512\ttotal: 1m 1s\tremaining: 3m 36s\n",
            "883:\tlearn: 0.9973580\ttotal: 1m 1s\tremaining: 3m 35s\n",
            "884:\tlearn: 0.9973512\ttotal: 1m 1s\tremaining: 3m 35s\n",
            "885:\tlearn: 0.9973648\ttotal: 1m 1s\tremaining: 3m 35s\n",
            "886:\tlearn: 0.9973784\ttotal: 1m 1s\tremaining: 3m 35s\n",
            "887:\tlearn: 0.9973784\ttotal: 1m 1s\tremaining: 3m 35s\n",
            "888:\tlearn: 0.9973784\ttotal: 1m 1s\tremaining: 3m 35s\n",
            "889:\tlearn: 0.9973784\ttotal: 1m 1s\tremaining: 3m 35s\n",
            "890:\tlearn: 0.9973853\ttotal: 1m 1s\tremaining: 3m 35s\n",
            "891:\tlearn: 0.9973853\ttotal: 1m 1s\tremaining: 3m 35s\n",
            "892:\tlearn: 0.9974125\ttotal: 1m 1s\tremaining: 3m 35s\n",
            "893:\tlearn: 0.9974193\ttotal: 1m 1s\tremaining: 3m 35s\n",
            "894:\tlearn: 0.9974057\ttotal: 1m 2s\tremaining: 3m 35s\n",
            "895:\tlearn: 0.9974057\ttotal: 1m 2s\tremaining: 3m 35s\n",
            "896:\tlearn: 0.9974533\ttotal: 1m 2s\tremaining: 3m 35s\n",
            "897:\tlearn: 0.9974942\ttotal: 1m 2s\tremaining: 3m 34s\n",
            "898:\tlearn: 0.9975010\ttotal: 1m 2s\tremaining: 3m 34s\n",
            "899:\tlearn: 0.9974533\ttotal: 1m 2s\tremaining: 3m 34s\n",
            "900:\tlearn: 0.9974874\ttotal: 1m 2s\tremaining: 3m 34s\n",
            "901:\tlearn: 0.9974874\ttotal: 1m 2s\tremaining: 3m 34s\n",
            "902:\tlearn: 0.9974942\ttotal: 1m 2s\tremaining: 3m 34s\n",
            "903:\tlearn: 0.9974738\ttotal: 1m 2s\tremaining: 3m 34s\n",
            "904:\tlearn: 0.9975078\ttotal: 1m 2s\tremaining: 3m 34s\n",
            "905:\tlearn: 0.9975010\ttotal: 1m 2s\tremaining: 3m 34s\n",
            "906:\tlearn: 0.9975215\ttotal: 1m 2s\tremaining: 3m 34s\n",
            "907:\tlearn: 0.9975215\ttotal: 1m 2s\tremaining: 3m 34s\n",
            "908:\tlearn: 0.9975146\ttotal: 1m 3s\tremaining: 3m 34s\n",
            "909:\tlearn: 0.9975283\ttotal: 1m 3s\tremaining: 3m 34s\n",
            "910:\tlearn: 0.9975215\ttotal: 1m 3s\tremaining: 3m 34s\n",
            "911:\tlearn: 0.9975078\ttotal: 1m 3s\tremaining: 3m 33s\n",
            "912:\tlearn: 0.9975146\ttotal: 1m 3s\tremaining: 3m 33s\n",
            "913:\tlearn: 0.9975351\ttotal: 1m 3s\tremaining: 3m 33s\n",
            "914:\tlearn: 0.9975623\ttotal: 1m 3s\tremaining: 3m 33s\n",
            "915:\tlearn: 0.9975419\ttotal: 1m 3s\tremaining: 3m 33s\n",
            "916:\tlearn: 0.9975351\ttotal: 1m 3s\tremaining: 3m 33s\n",
            "917:\tlearn: 0.9975487\ttotal: 1m 3s\tremaining: 3m 33s\n",
            "918:\tlearn: 0.9975555\ttotal: 1m 3s\tremaining: 3m 33s\n",
            "919:\tlearn: 0.9975896\ttotal: 1m 3s\tremaining: 3m 33s\n",
            "920:\tlearn: 0.9975896\ttotal: 1m 3s\tremaining: 3m 33s\n",
            "921:\tlearn: 0.9975964\ttotal: 1m 3s\tremaining: 3m 33s\n",
            "922:\tlearn: 0.9975691\ttotal: 1m 3s\tremaining: 3m 33s\n",
            "923:\tlearn: 0.9975759\ttotal: 1m 4s\tremaining: 3m 33s\n",
            "924:\tlearn: 0.9975828\ttotal: 1m 4s\tremaining: 3m 33s\n",
            "925:\tlearn: 0.9975964\ttotal: 1m 4s\tremaining: 3m 33s\n",
            "926:\tlearn: 0.9975964\ttotal: 1m 4s\tremaining: 3m 32s\n",
            "927:\tlearn: 0.9975964\ttotal: 1m 4s\tremaining: 3m 32s\n",
            "928:\tlearn: 0.9976032\ttotal: 1m 4s\tremaining: 3m 32s\n",
            "929:\tlearn: 0.9976304\ttotal: 1m 4s\tremaining: 3m 32s\n",
            "930:\tlearn: 0.9976441\ttotal: 1m 4s\tremaining: 3m 32s\n",
            "931:\tlearn: 0.9976713\ttotal: 1m 4s\tremaining: 3m 32s\n",
            "932:\tlearn: 0.9976781\ttotal: 1m 4s\tremaining: 3m 32s\n",
            "933:\tlearn: 0.9977122\ttotal: 1m 4s\tremaining: 3m 32s\n",
            "934:\tlearn: 0.9976918\ttotal: 1m 4s\tremaining: 3m 32s\n",
            "935:\tlearn: 0.9976918\ttotal: 1m 4s\tremaining: 3m 32s\n",
            "936:\tlearn: 0.9976918\ttotal: 1m 4s\tremaining: 3m 32s\n",
            "937:\tlearn: 0.9976986\ttotal: 1m 4s\tremaining: 3m 32s\n",
            "938:\tlearn: 0.9976986\ttotal: 1m 5s\tremaining: 3m 32s\n",
            "939:\tlearn: 0.9977122\ttotal: 1m 5s\tremaining: 3m 32s\n",
            "940:\tlearn: 0.9977395\ttotal: 1m 5s\tremaining: 3m 31s\n",
            "941:\tlearn: 0.9977463\ttotal: 1m 5s\tremaining: 3m 31s\n",
            "942:\tlearn: 0.9977599\ttotal: 1m 5s\tremaining: 3m 31s\n",
            "943:\tlearn: 0.9977531\ttotal: 1m 5s\tremaining: 3m 31s\n",
            "944:\tlearn: 0.9977803\ttotal: 1m 5s\tremaining: 3m 31s\n",
            "945:\tlearn: 0.9977667\ttotal: 1m 5s\tremaining: 3m 31s\n",
            "946:\tlearn: 0.9977667\ttotal: 1m 5s\tremaining: 3m 31s\n",
            "947:\tlearn: 0.9977667\ttotal: 1m 5s\tremaining: 3m 31s\n",
            "948:\tlearn: 0.9977735\ttotal: 1m 5s\tremaining: 3m 31s\n",
            "949:\tlearn: 0.9978008\ttotal: 1m 5s\tremaining: 3m 31s\n",
            "950:\tlearn: 0.9978076\ttotal: 1m 5s\tremaining: 3m 30s\n",
            "951:\tlearn: 0.9978212\ttotal: 1m 5s\tremaining: 3m 30s\n",
            "952:\tlearn: 0.9978280\ttotal: 1m 5s\tremaining: 3m 30s\n",
            "953:\tlearn: 0.9978280\ttotal: 1m 6s\tremaining: 3m 30s\n",
            "954:\tlearn: 0.9978553\ttotal: 1m 6s\tremaining: 3m 30s\n",
            "955:\tlearn: 0.9978621\ttotal: 1m 6s\tremaining: 3m 30s\n",
            "956:\tlearn: 0.9978758\ttotal: 1m 6s\tremaining: 3m 30s\n",
            "957:\tlearn: 0.9978621\ttotal: 1m 6s\tremaining: 3m 30s\n",
            "958:\tlearn: 0.9978621\ttotal: 1m 6s\tremaining: 3m 30s\n",
            "959:\tlearn: 0.9978689\ttotal: 1m 6s\tremaining: 3m 30s\n",
            "960:\tlearn: 0.9978758\ttotal: 1m 6s\tremaining: 3m 30s\n",
            "961:\tlearn: 0.9978826\ttotal: 1m 6s\tremaining: 3m 29s\n",
            "962:\tlearn: 0.9978894\ttotal: 1m 6s\tremaining: 3m 29s\n",
            "963:\tlearn: 0.9978758\ttotal: 1m 6s\tremaining: 3m 29s\n",
            "964:\tlearn: 0.9978894\ttotal: 1m 6s\tremaining: 3m 29s\n",
            "965:\tlearn: 0.9978962\ttotal: 1m 6s\tremaining: 3m 29s\n",
            "966:\tlearn: 0.9978962\ttotal: 1m 6s\tremaining: 3m 29s\n",
            "967:\tlearn: 0.9979439\ttotal: 1m 7s\tremaining: 3m 30s\n",
            "968:\tlearn: 0.9979371\ttotal: 1m 7s\tremaining: 3m 30s\n",
            "969:\tlearn: 0.9979507\ttotal: 1m 7s\tremaining: 3m 30s\n",
            "970:\tlearn: 0.9979507\ttotal: 1m 7s\tremaining: 3m 30s\n",
            "971:\tlearn: 0.9979644\ttotal: 1m 7s\tremaining: 3m 30s\n",
            "972:\tlearn: 0.9979644\ttotal: 1m 7s\tremaining: 3m 30s\n",
            "973:\tlearn: 0.9979848\ttotal: 1m 7s\tremaining: 3m 30s\n",
            "974:\tlearn: 0.9979848\ttotal: 1m 7s\tremaining: 3m 30s\n",
            "975:\tlearn: 0.9979916\ttotal: 1m 7s\tremaining: 3m 30s\n",
            "976:\tlearn: 0.9979985\ttotal: 1m 8s\tremaining: 3m 30s\n",
            "977:\tlearn: 0.9979916\ttotal: 1m 8s\tremaining: 3m 30s\n",
            "978:\tlearn: 0.9979985\ttotal: 1m 8s\tremaining: 3m 30s\n",
            "979:\tlearn: 0.9979985\ttotal: 1m 8s\tremaining: 3m 30s\n",
            "980:\tlearn: 0.9980121\ttotal: 1m 8s\tremaining: 3m 31s\n",
            "981:\tlearn: 0.9979985\ttotal: 1m 8s\tremaining: 3m 31s\n",
            "982:\tlearn: 0.9980121\ttotal: 1m 8s\tremaining: 3m 31s\n",
            "983:\tlearn: 0.9980121\ttotal: 1m 9s\tremaining: 3m 31s\n",
            "984:\tlearn: 0.9980325\ttotal: 1m 9s\tremaining: 3m 31s\n",
            "985:\tlearn: 0.9980325\ttotal: 1m 9s\tremaining: 3m 31s\n",
            "986:\tlearn: 0.9980462\ttotal: 1m 9s\tremaining: 3m 31s\n",
            "987:\tlearn: 0.9980530\ttotal: 1m 9s\tremaining: 3m 31s\n",
            "988:\tlearn: 0.9980598\ttotal: 1m 9s\tremaining: 3m 31s\n",
            "989:\tlearn: 0.9980598\ttotal: 1m 9s\tremaining: 3m 31s\n",
            "990:\tlearn: 0.9980462\ttotal: 1m 9s\tremaining: 3m 31s\n",
            "991:\tlearn: 0.9980462\ttotal: 1m 9s\tremaining: 3m 31s\n",
            "992:\tlearn: 0.9980462\ttotal: 1m 9s\tremaining: 3m 30s\n",
            "993:\tlearn: 0.9980666\ttotal: 1m 9s\tremaining: 3m 30s\n",
            "994:\tlearn: 0.9980530\ttotal: 1m 9s\tremaining: 3m 30s\n",
            "995:\tlearn: 0.9980598\ttotal: 1m 9s\tremaining: 3m 30s\n",
            "996:\tlearn: 0.9980530\ttotal: 1m 9s\tremaining: 3m 30s\n",
            "997:\tlearn: 0.9980666\ttotal: 1m 9s\tremaining: 3m 30s\n",
            "998:\tlearn: 0.9980735\ttotal: 1m 10s\tremaining: 3m 30s\n",
            "999:\tlearn: 0.9980666\ttotal: 1m 10s\tremaining: 3m 30s\n",
            "1000:\tlearn: 0.9980939\ttotal: 1m 10s\tremaining: 3m 30s\n",
            "1001:\tlearn: 0.9980939\ttotal: 1m 10s\tremaining: 3m 30s\n",
            "1002:\tlearn: 0.9980939\ttotal: 1m 10s\tremaining: 3m 30s\n",
            "1003:\tlearn: 0.9981075\ttotal: 1m 10s\tremaining: 3m 30s\n",
            "1004:\tlearn: 0.9981144\ttotal: 1m 10s\tremaining: 3m 29s\n",
            "1005:\tlearn: 0.9981212\ttotal: 1m 10s\tremaining: 3m 29s\n",
            "1006:\tlearn: 0.9981280\ttotal: 1m 10s\tremaining: 3m 29s\n",
            "1007:\tlearn: 0.9981348\ttotal: 1m 10s\tremaining: 3m 29s\n",
            "1008:\tlearn: 0.9981212\ttotal: 1m 10s\tremaining: 3m 29s\n",
            "1009:\tlearn: 0.9981212\ttotal: 1m 10s\tremaining: 3m 29s\n",
            "1010:\tlearn: 0.9981280\ttotal: 1m 10s\tremaining: 3m 29s\n",
            "1011:\tlearn: 0.9981348\ttotal: 1m 10s\tremaining: 3m 29s\n",
            "1012:\tlearn: 0.9981348\ttotal: 1m 11s\tremaining: 3m 29s\n",
            "1013:\tlearn: 0.9981485\ttotal: 1m 11s\tremaining: 3m 29s\n",
            "1014:\tlearn: 0.9981689\ttotal: 1m 11s\tremaining: 3m 29s\n",
            "1015:\tlearn: 0.9981757\ttotal: 1m 11s\tremaining: 3m 29s\n",
            "1016:\tlearn: 0.9981757\ttotal: 1m 11s\tremaining: 3m 29s\n",
            "1017:\tlearn: 0.9981826\ttotal: 1m 11s\tremaining: 3m 29s\n",
            "1018:\tlearn: 0.9981757\ttotal: 1m 11s\tremaining: 3m 30s\n",
            "1019:\tlearn: 0.9981826\ttotal: 1m 11s\tremaining: 3m 30s\n",
            "1020:\tlearn: 0.9981826\ttotal: 1m 12s\tremaining: 3m 30s\n",
            "1021:\tlearn: 0.9981894\ttotal: 1m 12s\tremaining: 3m 30s\n",
            "1022:\tlearn: 0.9981621\ttotal: 1m 12s\tremaining: 3m 30s\n",
            "1023:\tlearn: 0.9981485\ttotal: 1m 12s\tremaining: 3m 30s\n",
            "1024:\tlearn: 0.9981416\ttotal: 1m 12s\tremaining: 3m 30s\n",
            "1025:\tlearn: 0.9981485\ttotal: 1m 12s\tremaining: 3m 30s\n",
            "1026:\tlearn: 0.9981348\ttotal: 1m 12s\tremaining: 3m 30s\n",
            "1027:\tlearn: 0.9981553\ttotal: 1m 12s\tremaining: 3m 30s\n",
            "1028:\tlearn: 0.9981826\ttotal: 1m 12s\tremaining: 3m 30s\n",
            "1029:\tlearn: 0.9981689\ttotal: 1m 12s\tremaining: 3m 30s\n",
            "1030:\tlearn: 0.9981689\ttotal: 1m 12s\tremaining: 3m 29s\n",
            "1031:\tlearn: 0.9981826\ttotal: 1m 12s\tremaining: 3m 29s\n",
            "1032:\tlearn: 0.9981757\ttotal: 1m 13s\tremaining: 3m 29s\n",
            "1033:\tlearn: 0.9982098\ttotal: 1m 13s\tremaining: 3m 29s\n",
            "1034:\tlearn: 0.9981894\ttotal: 1m 13s\tremaining: 3m 29s\n",
            "1035:\tlearn: 0.9982098\ttotal: 1m 13s\tremaining: 3m 29s\n",
            "1036:\tlearn: 0.9981962\ttotal: 1m 13s\tremaining: 3m 29s\n",
            "1037:\tlearn: 0.9982030\ttotal: 1m 13s\tremaining: 3m 29s\n",
            "1038:\tlearn: 0.9982167\ttotal: 1m 13s\tremaining: 3m 29s\n",
            "1039:\tlearn: 0.9982167\ttotal: 1m 13s\tremaining: 3m 29s\n",
            "1040:\tlearn: 0.9982098\ttotal: 1m 13s\tremaining: 3m 29s\n",
            "1041:\tlearn: 0.9982098\ttotal: 1m 13s\tremaining: 3m 28s\n",
            "1042:\tlearn: 0.9982235\ttotal: 1m 13s\tremaining: 3m 28s\n",
            "1043:\tlearn: 0.9982303\ttotal: 1m 13s\tremaining: 3m 28s\n",
            "1044:\tlearn: 0.9982371\ttotal: 1m 13s\tremaining: 3m 28s\n",
            "1045:\tlearn: 0.9982371\ttotal: 1m 13s\tremaining: 3m 28s\n",
            "1046:\tlearn: 0.9982439\ttotal: 1m 13s\tremaining: 3m 28s\n",
            "1047:\tlearn: 0.9982439\ttotal: 1m 13s\tremaining: 3m 28s\n",
            "1048:\tlearn: 0.9982439\ttotal: 1m 14s\tremaining: 3m 28s\n",
            "1049:\tlearn: 0.9982371\ttotal: 1m 14s\tremaining: 3m 28s\n",
            "1050:\tlearn: 0.9982439\ttotal: 1m 14s\tremaining: 3m 28s\n",
            "1051:\tlearn: 0.9982303\ttotal: 1m 14s\tremaining: 3m 28s\n",
            "1052:\tlearn: 0.9982439\ttotal: 1m 14s\tremaining: 3m 28s\n",
            "1053:\tlearn: 0.9982508\ttotal: 1m 14s\tremaining: 3m 27s\n",
            "1054:\tlearn: 0.9982644\ttotal: 1m 14s\tremaining: 3m 27s\n",
            "1055:\tlearn: 0.9982644\ttotal: 1m 14s\tremaining: 3m 27s\n",
            "1056:\tlearn: 0.9982712\ttotal: 1m 14s\tremaining: 3m 27s\n",
            "1057:\tlearn: 0.9982849\ttotal: 1m 14s\tremaining: 3m 27s\n",
            "1058:\tlearn: 0.9983122\ttotal: 1m 14s\tremaining: 3m 27s\n",
            "1059:\tlearn: 0.9982917\ttotal: 1m 14s\tremaining: 3m 27s\n",
            "1060:\tlearn: 0.9982917\ttotal: 1m 14s\tremaining: 3m 27s\n",
            "1061:\tlearn: 0.9983122\ttotal: 1m 14s\tremaining: 3m 27s\n",
            "1062:\tlearn: 0.9983190\ttotal: 1m 15s\tremaining: 3m 27s\n",
            "1063:\tlearn: 0.9983258\ttotal: 1m 15s\tremaining: 3m 27s\n",
            "1064:\tlearn: 0.9983394\ttotal: 1m 15s\tremaining: 3m 27s\n",
            "1065:\tlearn: 0.9983463\ttotal: 1m 15s\tremaining: 3m 27s\n",
            "1066:\tlearn: 0.9983463\ttotal: 1m 15s\tremaining: 3m 27s\n",
            "1067:\tlearn: 0.9983326\ttotal: 1m 15s\tremaining: 3m 26s\n",
            "1068:\tlearn: 0.9983531\ttotal: 1m 15s\tremaining: 3m 26s\n",
            "1069:\tlearn: 0.9983531\ttotal: 1m 15s\tremaining: 3m 26s\n",
            "1070:\tlearn: 0.9983326\ttotal: 1m 15s\tremaining: 3m 26s\n",
            "1071:\tlearn: 0.9983463\ttotal: 1m 15s\tremaining: 3m 26s\n",
            "1072:\tlearn: 0.9983531\ttotal: 1m 15s\tremaining: 3m 26s\n",
            "1073:\tlearn: 0.9983599\ttotal: 1m 15s\tremaining: 3m 26s\n",
            "1074:\tlearn: 0.9983531\ttotal: 1m 15s\tremaining: 3m 26s\n",
            "1075:\tlearn: 0.9983531\ttotal: 1m 15s\tremaining: 3m 26s\n",
            "1076:\tlearn: 0.9983463\ttotal: 1m 16s\tremaining: 3m 26s\n",
            "1077:\tlearn: 0.9983531\ttotal: 1m 16s\tremaining: 3m 26s\n",
            "1078:\tlearn: 0.9983804\ttotal: 1m 16s\tremaining: 3m 26s\n",
            "1079:\tlearn: 0.9983736\ttotal: 1m 16s\tremaining: 3m 26s\n",
            "1080:\tlearn: 0.9983804\ttotal: 1m 16s\tremaining: 3m 25s\n",
            "1081:\tlearn: 0.9983940\ttotal: 1m 16s\tremaining: 3m 25s\n",
            "1082:\tlearn: 0.9984009\ttotal: 1m 16s\tremaining: 3m 25s\n",
            "1083:\tlearn: 0.9984009\ttotal: 1m 16s\tremaining: 3m 25s\n",
            "1084:\tlearn: 0.9983940\ttotal: 1m 16s\tremaining: 3m 25s\n",
            "1085:\tlearn: 0.9984009\ttotal: 1m 16s\tremaining: 3m 25s\n",
            "1086:\tlearn: 0.9984077\ttotal: 1m 16s\tremaining: 3m 25s\n",
            "1087:\tlearn: 0.9984281\ttotal: 1m 16s\tremaining: 3m 25s\n",
            "1088:\tlearn: 0.9984418\ttotal: 1m 16s\tremaining: 3m 25s\n",
            "1089:\tlearn: 0.9984486\ttotal: 1m 16s\tremaining: 3m 25s\n",
            "1090:\tlearn: 0.9984486\ttotal: 1m 16s\tremaining: 3m 25s\n",
            "1091:\tlearn: 0.9984486\ttotal: 1m 16s\tremaining: 3m 24s\n",
            "1092:\tlearn: 0.9984418\ttotal: 1m 17s\tremaining: 3m 24s\n",
            "1093:\tlearn: 0.9984554\ttotal: 1m 17s\tremaining: 3m 24s\n",
            "1094:\tlearn: 0.9984554\ttotal: 1m 17s\tremaining: 3m 24s\n",
            "1095:\tlearn: 0.9984759\ttotal: 1m 17s\tremaining: 3m 24s\n",
            "1096:\tlearn: 0.9984964\ttotal: 1m 17s\tremaining: 3m 24s\n",
            "1097:\tlearn: 0.9984896\ttotal: 1m 17s\tremaining: 3m 24s\n",
            "1098:\tlearn: 0.9985032\ttotal: 1m 17s\tremaining: 3m 24s\n",
            "1099:\tlearn: 0.9985032\ttotal: 1m 17s\tremaining: 3m 24s\n",
            "1100:\tlearn: 0.9985169\ttotal: 1m 17s\tremaining: 3m 24s\n",
            "1101:\tlearn: 0.9985169\ttotal: 1m 17s\tremaining: 3m 24s\n",
            "1102:\tlearn: 0.9985169\ttotal: 1m 17s\tremaining: 3m 24s\n",
            "1103:\tlearn: 0.9985169\ttotal: 1m 17s\tremaining: 3m 24s\n",
            "1104:\tlearn: 0.9985442\ttotal: 1m 17s\tremaining: 3m 24s\n",
            "1105:\tlearn: 0.9985442\ttotal: 1m 17s\tremaining: 3m 23s\n",
            "1106:\tlearn: 0.9985442\ttotal: 1m 18s\tremaining: 3m 23s\n",
            "1107:\tlearn: 0.9985442\ttotal: 1m 18s\tremaining: 3m 23s\n",
            "1108:\tlearn: 0.9985373\ttotal: 1m 18s\tremaining: 3m 23s\n",
            "1109:\tlearn: 0.9985373\ttotal: 1m 18s\tremaining: 3m 23s\n",
            "1110:\tlearn: 0.9985442\ttotal: 1m 18s\tremaining: 3m 23s\n",
            "1111:\tlearn: 0.9985510\ttotal: 1m 18s\tremaining: 3m 23s\n",
            "1112:\tlearn: 0.9985510\ttotal: 1m 18s\tremaining: 3m 23s\n",
            "1113:\tlearn: 0.9985510\ttotal: 1m 18s\tremaining: 3m 23s\n",
            "1114:\tlearn: 0.9985578\ttotal: 1m 18s\tremaining: 3m 23s\n",
            "1115:\tlearn: 0.9985510\ttotal: 1m 18s\tremaining: 3m 23s\n",
            "1116:\tlearn: 0.9985578\ttotal: 1m 18s\tremaining: 3m 23s\n",
            "1117:\tlearn: 0.9985578\ttotal: 1m 18s\tremaining: 3m 22s\n",
            "1118:\tlearn: 0.9985646\ttotal: 1m 18s\tremaining: 3m 22s\n",
            "1119:\tlearn: 0.9985715\ttotal: 1m 18s\tremaining: 3m 22s\n",
            "1120:\tlearn: 0.9985783\ttotal: 1m 18s\tremaining: 3m 22s\n",
            "1121:\tlearn: 0.9985988\ttotal: 1m 18s\tremaining: 3m 22s\n",
            "1122:\tlearn: 0.9985988\ttotal: 1m 19s\tremaining: 3m 22s\n",
            "1123:\tlearn: 0.9986124\ttotal: 1m 19s\tremaining: 3m 22s\n",
            "1124:\tlearn: 0.9986056\ttotal: 1m 19s\tremaining: 3m 22s\n",
            "1125:\tlearn: 0.9986056\ttotal: 1m 19s\tremaining: 3m 22s\n",
            "1126:\tlearn: 0.9986192\ttotal: 1m 19s\tremaining: 3m 22s\n",
            "1127:\tlearn: 0.9986192\ttotal: 1m 19s\tremaining: 3m 22s\n",
            "1128:\tlearn: 0.9985988\ttotal: 1m 19s\tremaining: 3m 22s\n",
            "1129:\tlearn: 0.9986261\ttotal: 1m 19s\tremaining: 3m 22s\n",
            "1130:\tlearn: 0.9986261\ttotal: 1m 19s\tremaining: 3m 22s\n",
            "1131:\tlearn: 0.9986329\ttotal: 1m 19s\tremaining: 3m 21s\n",
            "1132:\tlearn: 0.9986261\ttotal: 1m 19s\tremaining: 3m 21s\n",
            "1133:\tlearn: 0.9986397\ttotal: 1m 19s\tremaining: 3m 21s\n",
            "1134:\tlearn: 0.9986397\ttotal: 1m 19s\tremaining: 3m 21s\n",
            "1135:\tlearn: 0.9986465\ttotal: 1m 20s\tremaining: 3m 21s\n",
            "1136:\tlearn: 0.9986534\ttotal: 1m 20s\tremaining: 3m 21s\n",
            "1137:\tlearn: 0.9986534\ttotal: 1m 20s\tremaining: 3m 21s\n",
            "1138:\tlearn: 0.9986465\ttotal: 1m 20s\tremaining: 3m 21s\n",
            "1139:\tlearn: 0.9986534\ttotal: 1m 20s\tremaining: 3m 21s\n",
            "1140:\tlearn: 0.9986534\ttotal: 1m 20s\tremaining: 3m 21s\n",
            "1141:\tlearn: 0.9986465\ttotal: 1m 20s\tremaining: 3m 21s\n",
            "1142:\tlearn: 0.9986465\ttotal: 1m 20s\tremaining: 3m 21s\n",
            "1143:\tlearn: 0.9986534\ttotal: 1m 20s\tremaining: 3m 21s\n",
            "1144:\tlearn: 0.9986534\ttotal: 1m 20s\tremaining: 3m 20s\n",
            "1145:\tlearn: 0.9986602\ttotal: 1m 20s\tremaining: 3m 20s\n",
            "1146:\tlearn: 0.9986602\ttotal: 1m 20s\tremaining: 3m 20s\n",
            "1147:\tlearn: 0.9986738\ttotal: 1m 20s\tremaining: 3m 20s\n",
            "1148:\tlearn: 0.9986670\ttotal: 1m 20s\tremaining: 3m 20s\n",
            "1149:\tlearn: 0.9986738\ttotal: 1m 20s\tremaining: 3m 20s\n",
            "1150:\tlearn: 0.9986807\ttotal: 1m 20s\tremaining: 3m 20s\n",
            "1151:\tlearn: 0.9986943\ttotal: 1m 21s\tremaining: 3m 20s\n",
            "1152:\tlearn: 0.9986943\ttotal: 1m 21s\tremaining: 3m 20s\n",
            "1153:\tlearn: 0.9986943\ttotal: 1m 21s\tremaining: 3m 20s\n",
            "1154:\tlearn: 0.9986875\ttotal: 1m 21s\tremaining: 3m 20s\n",
            "1155:\tlearn: 0.9986943\ttotal: 1m 21s\tremaining: 3m 20s\n",
            "1156:\tlearn: 0.9986875\ttotal: 1m 21s\tremaining: 3m 20s\n",
            "1157:\tlearn: 0.9986875\ttotal: 1m 21s\tremaining: 3m 19s\n",
            "1158:\tlearn: 0.9986875\ttotal: 1m 21s\tremaining: 3m 19s\n",
            "1159:\tlearn: 0.9986807\ttotal: 1m 21s\tremaining: 3m 19s\n",
            "1160:\tlearn: 0.9987012\ttotal: 1m 21s\tremaining: 3m 19s\n",
            "1161:\tlearn: 0.9987148\ttotal: 1m 21s\tremaining: 3m 19s\n",
            "1162:\tlearn: 0.9987080\ttotal: 1m 21s\tremaining: 3m 19s\n",
            "1163:\tlearn: 0.9987148\ttotal: 1m 21s\tremaining: 3m 19s\n",
            "1164:\tlearn: 0.9987148\ttotal: 1m 21s\tremaining: 3m 19s\n",
            "1165:\tlearn: 0.9987012\ttotal: 1m 22s\tremaining: 3m 19s\n",
            "1166:\tlearn: 0.9986943\ttotal: 1m 22s\tremaining: 3m 19s\n",
            "1167:\tlearn: 0.9986943\ttotal: 1m 22s\tremaining: 3m 19s\n",
            "1168:\tlearn: 0.9986943\ttotal: 1m 22s\tremaining: 3m 19s\n",
            "1169:\tlearn: 0.9986943\ttotal: 1m 22s\tremaining: 3m 19s\n",
            "1170:\tlearn: 0.9986875\ttotal: 1m 22s\tremaining: 3m 18s\n",
            "1171:\tlearn: 0.9987012\ttotal: 1m 22s\tremaining: 3m 18s\n",
            "1172:\tlearn: 0.9987012\ttotal: 1m 22s\tremaining: 3m 18s\n",
            "1173:\tlearn: 0.9986943\ttotal: 1m 22s\tremaining: 3m 18s\n",
            "1174:\tlearn: 0.9986943\ttotal: 1m 22s\tremaining: 3m 18s\n",
            "1175:\tlearn: 0.9987012\ttotal: 1m 22s\tremaining: 3m 18s\n",
            "1176:\tlearn: 0.9987012\ttotal: 1m 22s\tremaining: 3m 18s\n",
            "1177:\tlearn: 0.9987080\ttotal: 1m 22s\tremaining: 3m 18s\n",
            "1178:\tlearn: 0.9986943\ttotal: 1m 22s\tremaining: 3m 18s\n",
            "1179:\tlearn: 0.9986943\ttotal: 1m 22s\tremaining: 3m 18s\n",
            "1180:\tlearn: 0.9987012\ttotal: 1m 22s\tremaining: 3m 18s\n",
            "1181:\tlearn: 0.9987080\ttotal: 1m 23s\tremaining: 3m 18s\n",
            "1182:\tlearn: 0.9987285\ttotal: 1m 23s\tremaining: 3m 17s\n",
            "1183:\tlearn: 0.9987285\ttotal: 1m 23s\tremaining: 3m 17s\n",
            "1184:\tlearn: 0.9987216\ttotal: 1m 23s\tremaining: 3m 17s\n",
            "1185:\tlearn: 0.9987421\ttotal: 1m 23s\tremaining: 3m 17s\n",
            "1186:\tlearn: 0.9987489\ttotal: 1m 23s\tremaining: 3m 17s\n",
            "1187:\tlearn: 0.9987489\ttotal: 1m 23s\tremaining: 3m 17s\n",
            "1188:\tlearn: 0.9987489\ttotal: 1m 23s\tremaining: 3m 17s\n",
            "1189:\tlearn: 0.9987421\ttotal: 1m 23s\tremaining: 3m 17s\n",
            "1190:\tlearn: 0.9987489\ttotal: 1m 23s\tremaining: 3m 17s\n",
            "1191:\tlearn: 0.9987489\ttotal: 1m 23s\tremaining: 3m 17s\n",
            "1192:\tlearn: 0.9987489\ttotal: 1m 23s\tremaining: 3m 17s\n",
            "1193:\tlearn: 0.9987489\ttotal: 1m 23s\tremaining: 3m 17s\n",
            "1194:\tlearn: 0.9987558\ttotal: 1m 23s\tremaining: 3m 17s\n",
            "1195:\tlearn: 0.9987626\ttotal: 1m 24s\tremaining: 3m 17s\n",
            "1196:\tlearn: 0.9987626\ttotal: 1m 24s\tremaining: 3m 16s\n",
            "1197:\tlearn: 0.9987763\ttotal: 1m 24s\tremaining: 3m 16s\n",
            "1198:\tlearn: 0.9987899\ttotal: 1m 24s\tremaining: 3m 16s\n",
            "1199:\tlearn: 0.9987899\ttotal: 1m 24s\tremaining: 3m 16s\n",
            "1200:\tlearn: 0.9987899\ttotal: 1m 24s\tremaining: 3m 16s\n",
            "1201:\tlearn: 0.9987763\ttotal: 1m 24s\tremaining: 3m 16s\n",
            "1202:\tlearn: 0.9987763\ttotal: 1m 24s\tremaining: 3m 16s\n",
            "1203:\tlearn: 0.9987763\ttotal: 1m 24s\tremaining: 3m 16s\n",
            "1204:\tlearn: 0.9987899\ttotal: 1m 24s\tremaining: 3m 16s\n",
            "1205:\tlearn: 0.9987763\ttotal: 1m 24s\tremaining: 3m 16s\n",
            "1206:\tlearn: 0.9987763\ttotal: 1m 24s\tremaining: 3m 16s\n",
            "1207:\tlearn: 0.9987763\ttotal: 1m 24s\tremaining: 3m 16s\n",
            "1208:\tlearn: 0.9987831\ttotal: 1m 24s\tremaining: 3m 16s\n",
            "1209:\tlearn: 0.9987831\ttotal: 1m 24s\tremaining: 3m 15s\n",
            "1210:\tlearn: 0.9988036\ttotal: 1m 25s\tremaining: 3m 15s\n",
            "1211:\tlearn: 0.9988036\ttotal: 1m 25s\tremaining: 3m 15s\n",
            "1212:\tlearn: 0.9988104\ttotal: 1m 25s\tremaining: 3m 15s\n",
            "1213:\tlearn: 0.9987967\ttotal: 1m 25s\tremaining: 3m 15s\n",
            "1214:\tlearn: 0.9987967\ttotal: 1m 25s\tremaining: 3m 15s\n",
            "1215:\tlearn: 0.9988172\ttotal: 1m 25s\tremaining: 3m 15s\n",
            "1216:\tlearn: 0.9988172\ttotal: 1m 25s\tremaining: 3m 15s\n",
            "1217:\tlearn: 0.9988172\ttotal: 1m 25s\tremaining: 3m 15s\n",
            "1218:\tlearn: 0.9988172\ttotal: 1m 25s\tremaining: 3m 15s\n",
            "1219:\tlearn: 0.9988241\ttotal: 1m 25s\tremaining: 3m 15s\n",
            "1220:\tlearn: 0.9988172\ttotal: 1m 25s\tremaining: 3m 15s\n",
            "1221:\tlearn: 0.9988445\ttotal: 1m 25s\tremaining: 3m 15s\n",
            "1222:\tlearn: 0.9988377\ttotal: 1m 25s\tremaining: 3m 14s\n",
            "1223:\tlearn: 0.9988582\ttotal: 1m 25s\tremaining: 3m 14s\n",
            "1224:\tlearn: 0.9988924\ttotal: 1m 26s\tremaining: 3m 14s\n",
            "1225:\tlearn: 0.9988924\ttotal: 1m 26s\tremaining: 3m 14s\n",
            "1226:\tlearn: 0.9988924\ttotal: 1m 26s\tremaining: 3m 14s\n",
            "1227:\tlearn: 0.9988924\ttotal: 1m 26s\tremaining: 3m 14s\n",
            "1228:\tlearn: 0.9988787\ttotal: 1m 26s\tremaining: 3m 14s\n",
            "1229:\tlearn: 0.9988924\ttotal: 1m 26s\tremaining: 3m 14s\n",
            "1230:\tlearn: 0.9988924\ttotal: 1m 26s\tremaining: 3m 14s\n",
            "1231:\tlearn: 0.9988924\ttotal: 1m 26s\tremaining: 3m 14s\n",
            "1232:\tlearn: 0.9988924\ttotal: 1m 26s\tremaining: 3m 14s\n",
            "1233:\tlearn: 0.9989060\ttotal: 1m 26s\tremaining: 3m 14s\n",
            "1234:\tlearn: 0.9988992\ttotal: 1m 26s\tremaining: 3m 13s\n",
            "1235:\tlearn: 0.9988992\ttotal: 1m 26s\tremaining: 3m 13s\n",
            "1236:\tlearn: 0.9988992\ttotal: 1m 26s\tremaining: 3m 13s\n",
            "1237:\tlearn: 0.9989060\ttotal: 1m 26s\tremaining: 3m 13s\n",
            "1238:\tlearn: 0.9988992\ttotal: 1m 26s\tremaining: 3m 13s\n",
            "1239:\tlearn: 0.9988924\ttotal: 1m 26s\tremaining: 3m 13s\n",
            "1240:\tlearn: 0.9988924\ttotal: 1m 27s\tremaining: 3m 13s\n",
            "1241:\tlearn: 0.9988924\ttotal: 1m 27s\tremaining: 3m 13s\n",
            "1242:\tlearn: 0.9988992\ttotal: 1m 27s\tremaining: 3m 13s\n",
            "1243:\tlearn: 0.9989060\ttotal: 1m 27s\tremaining: 3m 13s\n",
            "1244:\tlearn: 0.9989060\ttotal: 1m 27s\tremaining: 3m 13s\n",
            "1245:\tlearn: 0.9989128\ttotal: 1m 27s\tremaining: 3m 13s\n",
            "1246:\tlearn: 0.9988855\ttotal: 1m 27s\tremaining: 3m 12s\n",
            "1247:\tlearn: 0.9988924\ttotal: 1m 27s\tremaining: 3m 12s\n",
            "1248:\tlearn: 0.9989060\ttotal: 1m 27s\tremaining: 3m 12s\n",
            "1249:\tlearn: 0.9989333\ttotal: 1m 27s\tremaining: 3m 12s\n",
            "1250:\tlearn: 0.9989333\ttotal: 1m 27s\tremaining: 3m 12s\n",
            "1251:\tlearn: 0.9989402\ttotal: 1m 27s\tremaining: 3m 12s\n",
            "1252:\tlearn: 0.9989402\ttotal: 1m 27s\tremaining: 3m 12s\n",
            "1253:\tlearn: 0.9989333\ttotal: 1m 27s\tremaining: 3m 12s\n",
            "1254:\tlearn: 0.9989538\ttotal: 1m 27s\tremaining: 3m 12s\n",
            "1255:\tlearn: 0.9989538\ttotal: 1m 28s\tremaining: 3m 12s\n",
            "1256:\tlearn: 0.9989607\ttotal: 1m 28s\tremaining: 3m 12s\n",
            "1257:\tlearn: 0.9989743\ttotal: 1m 28s\tremaining: 3m 12s\n",
            "1258:\tlearn: 0.9989811\ttotal: 1m 28s\tremaining: 3m 12s\n",
            "1259:\tlearn: 0.9989811\ttotal: 1m 28s\tremaining: 3m 11s\n",
            "1260:\tlearn: 0.9989811\ttotal: 1m 28s\tremaining: 3m 11s\n",
            "1261:\tlearn: 0.9989811\ttotal: 1m 28s\tremaining: 3m 11s\n",
            "1262:\tlearn: 0.9989811\ttotal: 1m 28s\tremaining: 3m 11s\n",
            "1263:\tlearn: 0.9989743\ttotal: 1m 28s\tremaining: 3m 11s\n",
            "1264:\tlearn: 0.9989675\ttotal: 1m 28s\tremaining: 3m 11s\n",
            "1265:\tlearn: 0.9989743\ttotal: 1m 28s\tremaining: 3m 11s\n",
            "1266:\tlearn: 0.9989811\ttotal: 1m 28s\tremaining: 3m 11s\n",
            "1267:\tlearn: 0.9989880\ttotal: 1m 28s\tremaining: 3m 11s\n",
            "1268:\tlearn: 0.9989811\ttotal: 1m 28s\tremaining: 3m 11s\n",
            "1269:\tlearn: 0.9989811\ttotal: 1m 28s\tremaining: 3m 11s\n",
            "1270:\tlearn: 0.9989811\ttotal: 1m 28s\tremaining: 3m 11s\n",
            "1271:\tlearn: 0.9989811\ttotal: 1m 29s\tremaining: 3m 10s\n",
            "1272:\tlearn: 0.9989811\ttotal: 1m 29s\tremaining: 3m 10s\n",
            "1273:\tlearn: 0.9989811\ttotal: 1m 29s\tremaining: 3m 10s\n",
            "1274:\tlearn: 0.9989811\ttotal: 1m 29s\tremaining: 3m 10s\n",
            "1275:\tlearn: 0.9989811\ttotal: 1m 29s\tremaining: 3m 10s\n",
            "1276:\tlearn: 0.9989880\ttotal: 1m 29s\tremaining: 3m 10s\n",
            "1277:\tlearn: 0.9989880\ttotal: 1m 29s\tremaining: 3m 10s\n",
            "1278:\tlearn: 0.9989948\ttotal: 1m 29s\tremaining: 3m 10s\n",
            "1279:\tlearn: 0.9989880\ttotal: 1m 29s\tremaining: 3m 10s\n",
            "1280:\tlearn: 0.9989880\ttotal: 1m 29s\tremaining: 3m 10s\n",
            "1281:\tlearn: 0.9989880\ttotal: 1m 29s\tremaining: 3m 10s\n",
            "1282:\tlearn: 0.9989811\ttotal: 1m 29s\tremaining: 3m 10s\n",
            "1283:\tlearn: 0.9989811\ttotal: 1m 29s\tremaining: 3m 10s\n",
            "1284:\tlearn: 0.9989811\ttotal: 1m 30s\tremaining: 3m 10s\n",
            "1285:\tlearn: 0.9989811\ttotal: 1m 30s\tremaining: 3m 10s\n",
            "1286:\tlearn: 0.9989880\ttotal: 1m 30s\tremaining: 3m 10s\n",
            "1287:\tlearn: 0.9989880\ttotal: 1m 30s\tremaining: 3m 9s\n",
            "1288:\tlearn: 0.9989948\ttotal: 1m 30s\tremaining: 3m 9s\n",
            "1289:\tlearn: 0.9989880\ttotal: 1m 30s\tremaining: 3m 9s\n",
            "1290:\tlearn: 0.9989880\ttotal: 1m 30s\tremaining: 3m 9s\n",
            "1291:\tlearn: 0.9989880\ttotal: 1m 30s\tremaining: 3m 9s\n",
            "1292:\tlearn: 0.9989880\ttotal: 1m 30s\tremaining: 3m 9s\n",
            "1293:\tlearn: 0.9989880\ttotal: 1m 30s\tremaining: 3m 9s\n",
            "1294:\tlearn: 0.9989880\ttotal: 1m 30s\tremaining: 3m 9s\n",
            "1295:\tlearn: 0.9989811\ttotal: 1m 30s\tremaining: 3m 9s\n",
            "1296:\tlearn: 0.9989948\ttotal: 1m 30s\tremaining: 3m 9s\n",
            "1297:\tlearn: 0.9990085\ttotal: 1m 30s\tremaining: 3m 9s\n",
            "1298:\tlearn: 0.9990016\ttotal: 1m 30s\tremaining: 3m 9s\n",
            "1299:\tlearn: 0.9990221\ttotal: 1m 31s\tremaining: 3m 9s\n",
            "1300:\tlearn: 0.9990221\ttotal: 1m 31s\tremaining: 3m 9s\n",
            "1301:\tlearn: 0.9990221\ttotal: 1m 31s\tremaining: 3m 8s\n",
            "1302:\tlearn: 0.9990221\ttotal: 1m 31s\tremaining: 3m 8s\n",
            "1303:\tlearn: 0.9990221\ttotal: 1m 31s\tremaining: 3m 8s\n",
            "1304:\tlearn: 0.9990085\ttotal: 1m 31s\tremaining: 3m 8s\n",
            "1305:\tlearn: 0.9990153\ttotal: 1m 31s\tremaining: 3m 8s\n",
            "1306:\tlearn: 0.9990153\ttotal: 1m 31s\tremaining: 3m 8s\n",
            "1307:\tlearn: 0.9990153\ttotal: 1m 31s\tremaining: 3m 8s\n",
            "1308:\tlearn: 0.9990153\ttotal: 1m 31s\tremaining: 3m 8s\n",
            "1309:\tlearn: 0.9990085\ttotal: 1m 31s\tremaining: 3m 8s\n",
            "1310:\tlearn: 0.9990221\ttotal: 1m 31s\tremaining: 3m 8s\n",
            "1311:\tlearn: 0.9990221\ttotal: 1m 31s\tremaining: 3m 8s\n",
            "1312:\tlearn: 0.9990221\ttotal: 1m 31s\tremaining: 3m 8s\n",
            "1313:\tlearn: 0.9990221\ttotal: 1m 31s\tremaining: 3m 8s\n",
            "1314:\tlearn: 0.9990221\ttotal: 1m 32s\tremaining: 3m 7s\n",
            "1315:\tlearn: 0.9990290\ttotal: 1m 32s\tremaining: 3m 7s\n",
            "1316:\tlearn: 0.9990290\ttotal: 1m 32s\tremaining: 3m 7s\n",
            "1317:\tlearn: 0.9990290\ttotal: 1m 32s\tremaining: 3m 7s\n",
            "1318:\tlearn: 0.9990358\ttotal: 1m 32s\tremaining: 3m 7s\n",
            "1319:\tlearn: 0.9990358\ttotal: 1m 32s\tremaining: 3m 7s\n",
            "1320:\tlearn: 0.9990358\ttotal: 1m 32s\tremaining: 3m 7s\n",
            "1321:\tlearn: 0.9990426\ttotal: 1m 32s\tremaining: 3m 7s\n",
            "1322:\tlearn: 0.9990495\ttotal: 1m 32s\tremaining: 3m 7s\n",
            "1323:\tlearn: 0.9990426\ttotal: 1m 32s\tremaining: 3m 7s\n",
            "1324:\tlearn: 0.9990426\ttotal: 1m 32s\tremaining: 3m 7s\n",
            "1325:\tlearn: 0.9990426\ttotal: 1m 32s\tremaining: 3m 7s\n",
            "1326:\tlearn: 0.9990426\ttotal: 1m 32s\tremaining: 3m 7s\n",
            "1327:\tlearn: 0.9990495\ttotal: 1m 32s\tremaining: 3m 6s\n",
            "1328:\tlearn: 0.9990495\ttotal: 1m 32s\tremaining: 3m 6s\n",
            "1329:\tlearn: 0.9990426\ttotal: 1m 33s\tremaining: 3m 6s\n",
            "1330:\tlearn: 0.9990426\ttotal: 1m 33s\tremaining: 3m 6s\n",
            "1331:\tlearn: 0.9990426\ttotal: 1m 33s\tremaining: 3m 6s\n",
            "1332:\tlearn: 0.9990426\ttotal: 1m 33s\tremaining: 3m 6s\n",
            "1333:\tlearn: 0.9990426\ttotal: 1m 33s\tremaining: 3m 6s\n",
            "1334:\tlearn: 0.9990426\ttotal: 1m 33s\tremaining: 3m 6s\n",
            "1335:\tlearn: 0.9990358\ttotal: 1m 33s\tremaining: 3m 6s\n",
            "1336:\tlearn: 0.9990358\ttotal: 1m 33s\tremaining: 3m 6s\n",
            "1337:\tlearn: 0.9990358\ttotal: 1m 33s\tremaining: 3m 6s\n",
            "1338:\tlearn: 0.9990495\ttotal: 1m 33s\tremaining: 3m 6s\n",
            "1339:\tlearn: 0.9990358\ttotal: 1m 33s\tremaining: 3m 6s\n",
            "1340:\tlearn: 0.9990358\ttotal: 1m 33s\tremaining: 3m 5s\n",
            "1341:\tlearn: 0.9990290\ttotal: 1m 33s\tremaining: 3m 5s\n",
            "1342:\tlearn: 0.9990358\ttotal: 1m 33s\tremaining: 3m 5s\n",
            "1343:\tlearn: 0.9990495\ttotal: 1m 34s\tremaining: 3m 5s\n",
            "1344:\tlearn: 0.9990426\ttotal: 1m 34s\tremaining: 3m 5s\n",
            "1345:\tlearn: 0.9990358\ttotal: 1m 34s\tremaining: 3m 5s\n",
            "1346:\tlearn: 0.9990358\ttotal: 1m 34s\tremaining: 3m 5s\n",
            "1347:\tlearn: 0.9990358\ttotal: 1m 34s\tremaining: 3m 5s\n",
            "1348:\tlearn: 0.9990358\ttotal: 1m 34s\tremaining: 3m 5s\n",
            "1349:\tlearn: 0.9990358\ttotal: 1m 34s\tremaining: 3m 5s\n",
            "1350:\tlearn: 0.9990358\ttotal: 1m 34s\tremaining: 3m 5s\n",
            "1351:\tlearn: 0.9990426\ttotal: 1m 34s\tremaining: 3m 5s\n",
            "1352:\tlearn: 0.9990426\ttotal: 1m 34s\tremaining: 3m 5s\n",
            "1353:\tlearn: 0.9990426\ttotal: 1m 34s\tremaining: 3m 5s\n",
            "1354:\tlearn: 0.9990495\ttotal: 1m 34s\tremaining: 3m 5s\n",
            "1355:\tlearn: 0.9990495\ttotal: 1m 34s\tremaining: 3m 4s\n",
            "1356:\tlearn: 0.9990495\ttotal: 1m 34s\tremaining: 3m 4s\n",
            "1357:\tlearn: 0.9990495\ttotal: 1m 34s\tremaining: 3m 4s\n",
            "1358:\tlearn: 0.9990563\ttotal: 1m 35s\tremaining: 3m 4s\n",
            "1359:\tlearn: 0.9990631\ttotal: 1m 35s\tremaining: 3m 4s\n",
            "1360:\tlearn: 0.9990631\ttotal: 1m 35s\tremaining: 3m 4s\n",
            "1361:\tlearn: 0.9990631\ttotal: 1m 35s\tremaining: 3m 4s\n",
            "1362:\tlearn: 0.9990631\ttotal: 1m 35s\tremaining: 3m 4s\n",
            "1363:\tlearn: 0.9990631\ttotal: 1m 35s\tremaining: 3m 4s\n",
            "1364:\tlearn: 0.9990631\ttotal: 1m 35s\tremaining: 3m 4s\n",
            "1365:\tlearn: 0.9990631\ttotal: 1m 35s\tremaining: 3m 4s\n",
            "1366:\tlearn: 0.9990631\ttotal: 1m 35s\tremaining: 3m 4s\n",
            "1367:\tlearn: 0.9990631\ttotal: 1m 35s\tremaining: 3m 3s\n",
            "1368:\tlearn: 0.9990631\ttotal: 1m 35s\tremaining: 3m 3s\n",
            "1369:\tlearn: 0.9990631\ttotal: 1m 35s\tremaining: 3m 3s\n",
            "1370:\tlearn: 0.9990700\ttotal: 1m 35s\tremaining: 3m 3s\n",
            "1371:\tlearn: 0.9990836\ttotal: 1m 35s\tremaining: 3m 3s\n",
            "1372:\tlearn: 0.9990836\ttotal: 1m 35s\tremaining: 3m 3s\n",
            "1373:\tlearn: 0.9990836\ttotal: 1m 36s\tremaining: 3m 3s\n",
            "1374:\tlearn: 0.9990836\ttotal: 1m 36s\tremaining: 3m 3s\n",
            "1375:\tlearn: 0.9990836\ttotal: 1m 36s\tremaining: 3m 3s\n",
            "1376:\tlearn: 0.9990836\ttotal: 1m 36s\tremaining: 3m 3s\n",
            "1377:\tlearn: 0.9990836\ttotal: 1m 36s\tremaining: 3m 3s\n",
            "1378:\tlearn: 0.9990836\ttotal: 1m 36s\tremaining: 3m 3s\n",
            "1379:\tlearn: 0.9990836\ttotal: 1m 36s\tremaining: 3m 3s\n",
            "1380:\tlearn: 0.9990836\ttotal: 1m 36s\tremaining: 3m 2s\n",
            "1381:\tlearn: 0.9990836\ttotal: 1m 36s\tremaining: 3m 2s\n",
            "1382:\tlearn: 0.9990836\ttotal: 1m 36s\tremaining: 3m 2s\n",
            "1383:\tlearn: 0.9990768\ttotal: 1m 36s\tremaining: 3m 2s\n",
            "1384:\tlearn: 0.9990836\ttotal: 1m 36s\tremaining: 3m 2s\n",
            "1385:\tlearn: 0.9990973\ttotal: 1m 36s\tremaining: 3m 2s\n",
            "1386:\tlearn: 0.9990973\ttotal: 1m 36s\tremaining: 3m 2s\n",
            "1387:\tlearn: 0.9990973\ttotal: 1m 36s\tremaining: 3m 2s\n",
            "1388:\tlearn: 0.9990973\ttotal: 1m 37s\tremaining: 3m 2s\n",
            "1389:\tlearn: 0.9991041\ttotal: 1m 37s\tremaining: 3m 2s\n",
            "1390:\tlearn: 0.9991041\ttotal: 1m 37s\tremaining: 3m 2s\n",
            "1391:\tlearn: 0.9991110\ttotal: 1m 37s\tremaining: 3m 2s\n",
            "1392:\tlearn: 0.9991110\ttotal: 1m 37s\tremaining: 3m 2s\n",
            "1393:\tlearn: 0.9991110\ttotal: 1m 37s\tremaining: 3m 2s\n",
            "1394:\tlearn: 0.9990973\ttotal: 1m 37s\tremaining: 3m 1s\n",
            "1395:\tlearn: 0.9991041\ttotal: 1m 37s\tremaining: 3m 1s\n",
            "1396:\tlearn: 0.9991110\ttotal: 1m 37s\tremaining: 3m 1s\n",
            "1397:\tlearn: 0.9991178\ttotal: 1m 37s\tremaining: 3m 1s\n",
            "1398:\tlearn: 0.9991178\ttotal: 1m 37s\tremaining: 3m 1s\n",
            "1399:\tlearn: 0.9991178\ttotal: 1m 37s\tremaining: 3m 1s\n",
            "1400:\tlearn: 0.9991246\ttotal: 1m 37s\tremaining: 3m 1s\n",
            "1401:\tlearn: 0.9991246\ttotal: 1m 37s\tremaining: 3m 1s\n",
            "1402:\tlearn: 0.9991246\ttotal: 1m 37s\tremaining: 3m 1s\n",
            "1403:\tlearn: 0.9991246\ttotal: 1m 38s\tremaining: 3m 1s\n",
            "1404:\tlearn: 0.9991246\ttotal: 1m 38s\tremaining: 3m 1s\n",
            "1405:\tlearn: 0.9991383\ttotal: 1m 38s\tremaining: 3m 1s\n",
            "1406:\tlearn: 0.9991520\ttotal: 1m 38s\tremaining: 3m 1s\n",
            "1407:\tlearn: 0.9991383\ttotal: 1m 38s\tremaining: 3m\n",
            "1408:\tlearn: 0.9991451\ttotal: 1m 38s\tremaining: 3m\n",
            "1409:\tlearn: 0.9991451\ttotal: 1m 38s\tremaining: 3m\n",
            "1410:\tlearn: 0.9991451\ttotal: 1m 38s\tremaining: 3m\n",
            "1411:\tlearn: 0.9991451\ttotal: 1m 38s\tremaining: 3m\n",
            "1412:\tlearn: 0.9991451\ttotal: 1m 38s\tremaining: 3m\n",
            "1413:\tlearn: 0.9991451\ttotal: 1m 38s\tremaining: 3m\n",
            "1414:\tlearn: 0.9991451\ttotal: 1m 38s\tremaining: 3m\n",
            "1415:\tlearn: 0.9991451\ttotal: 1m 38s\tremaining: 3m\n",
            "1416:\tlearn: 0.9991451\ttotal: 1m 38s\tremaining: 3m\n",
            "1417:\tlearn: 0.9991451\ttotal: 1m 38s\tremaining: 3m\n",
            "1418:\tlearn: 0.9991520\ttotal: 1m 38s\tremaining: 3m\n",
            "1419:\tlearn: 0.9991520\ttotal: 1m 39s\tremaining: 2m 59s\n",
            "1420:\tlearn: 0.9991656\ttotal: 1m 39s\tremaining: 2m 59s\n",
            "1421:\tlearn: 0.9991656\ttotal: 1m 39s\tremaining: 2m 59s\n",
            "1422:\tlearn: 0.9991656\ttotal: 1m 39s\tremaining: 2m 59s\n",
            "1423:\tlearn: 0.9991725\ttotal: 1m 39s\tremaining: 2m 59s\n",
            "1424:\tlearn: 0.9991793\ttotal: 1m 39s\tremaining: 2m 59s\n",
            "1425:\tlearn: 0.9991793\ttotal: 1m 39s\tremaining: 2m 59s\n",
            "1426:\tlearn: 0.9991861\ttotal: 1m 39s\tremaining: 2m 59s\n",
            "1427:\tlearn: 0.9991861\ttotal: 1m 39s\tremaining: 2m 59s\n",
            "1428:\tlearn: 0.9991861\ttotal: 1m 39s\tremaining: 2m 59s\n",
            "1429:\tlearn: 0.9991793\ttotal: 1m 39s\tremaining: 2m 59s\n",
            "1430:\tlearn: 0.9991793\ttotal: 1m 39s\tremaining: 2m 59s\n",
            "1431:\tlearn: 0.9991793\ttotal: 1m 39s\tremaining: 2m 59s\n",
            "1432:\tlearn: 0.9991861\ttotal: 1m 39s\tremaining: 2m 58s\n",
            "1433:\tlearn: 0.9991861\ttotal: 1m 39s\tremaining: 2m 58s\n",
            "1434:\tlearn: 0.9991861\ttotal: 1m 40s\tremaining: 2m 58s\n",
            "1435:\tlearn: 0.9991861\ttotal: 1m 40s\tremaining: 2m 58s\n",
            "1436:\tlearn: 0.9991930\ttotal: 1m 40s\tremaining: 2m 58s\n",
            "1437:\tlearn: 0.9991930\ttotal: 1m 40s\tremaining: 2m 58s\n",
            "1438:\tlearn: 0.9991930\ttotal: 1m 40s\tremaining: 2m 58s\n",
            "1439:\tlearn: 0.9991930\ttotal: 1m 40s\tremaining: 2m 58s\n",
            "1440:\tlearn: 0.9991930\ttotal: 1m 40s\tremaining: 2m 58s\n",
            "1441:\tlearn: 0.9992066\ttotal: 1m 40s\tremaining: 2m 58s\n",
            "1442:\tlearn: 0.9992066\ttotal: 1m 40s\tremaining: 2m 58s\n",
            "1443:\tlearn: 0.9992203\ttotal: 1m 40s\tremaining: 2m 58s\n",
            "1444:\tlearn: 0.9992203\ttotal: 1m 40s\tremaining: 2m 57s\n",
            "1445:\tlearn: 0.9992203\ttotal: 1m 40s\tremaining: 2m 57s\n",
            "1446:\tlearn: 0.9992135\ttotal: 1m 40s\tremaining: 2m 57s\n",
            "1447:\tlearn: 0.9992135\ttotal: 1m 40s\tremaining: 2m 57s\n",
            "1448:\tlearn: 0.9992135\ttotal: 1m 40s\tremaining: 2m 57s\n",
            "1449:\tlearn: 0.9992135\ttotal: 1m 40s\tremaining: 2m 57s\n",
            "1450:\tlearn: 0.9992271\ttotal: 1m 41s\tremaining: 2m 57s\n",
            "1451:\tlearn: 0.9992340\ttotal: 1m 41s\tremaining: 2m 57s\n",
            "1452:\tlearn: 0.9992340\ttotal: 1m 41s\tremaining: 2m 57s\n",
            "1453:\tlearn: 0.9992340\ttotal: 1m 41s\tremaining: 2m 57s\n",
            "1454:\tlearn: 0.9992271\ttotal: 1m 41s\tremaining: 2m 57s\n",
            "1455:\tlearn: 0.9992271\ttotal: 1m 41s\tremaining: 2m 57s\n",
            "1456:\tlearn: 0.9992271\ttotal: 1m 41s\tremaining: 2m 57s\n",
            "1457:\tlearn: 0.9992340\ttotal: 1m 41s\tremaining: 2m 56s\n",
            "1458:\tlearn: 0.9992340\ttotal: 1m 41s\tremaining: 2m 56s\n",
            "1459:\tlearn: 0.9992613\ttotal: 1m 41s\tremaining: 2m 56s\n",
            "1460:\tlearn: 0.9992613\ttotal: 1m 41s\tremaining: 2m 56s\n",
            "1461:\tlearn: 0.9992613\ttotal: 1m 41s\tremaining: 2m 56s\n",
            "1462:\tlearn: 0.9992613\ttotal: 1m 41s\tremaining: 2m 56s\n",
            "1463:\tlearn: 0.9992613\ttotal: 1m 41s\tremaining: 2m 56s\n",
            "1464:\tlearn: 0.9992613\ttotal: 1m 41s\tremaining: 2m 56s\n",
            "1465:\tlearn: 0.9992613\ttotal: 1m 42s\tremaining: 2m 56s\n",
            "1466:\tlearn: 0.9992613\ttotal: 1m 42s\tremaining: 2m 56s\n",
            "1467:\tlearn: 0.9992681\ttotal: 1m 42s\tremaining: 2m 56s\n",
            "1468:\tlearn: 0.9992750\ttotal: 1m 42s\tremaining: 2m 56s\n",
            "1469:\tlearn: 0.9992681\ttotal: 1m 42s\tremaining: 2m 56s\n",
            "1470:\tlearn: 0.9992681\ttotal: 1m 42s\tremaining: 2m 55s\n",
            "1471:\tlearn: 0.9992681\ttotal: 1m 42s\tremaining: 2m 55s\n",
            "1472:\tlearn: 0.9992750\ttotal: 1m 42s\tremaining: 2m 55s\n",
            "1473:\tlearn: 0.9992750\ttotal: 1m 42s\tremaining: 2m 55s\n",
            "1474:\tlearn: 0.9992750\ttotal: 1m 42s\tremaining: 2m 55s\n",
            "1475:\tlearn: 0.9992750\ttotal: 1m 42s\tremaining: 2m 55s\n",
            "1476:\tlearn: 0.9992750\ttotal: 1m 42s\tremaining: 2m 55s\n",
            "1477:\tlearn: 0.9992818\ttotal: 1m 42s\tremaining: 2m 55s\n",
            "1478:\tlearn: 0.9992886\ttotal: 1m 42s\tremaining: 2m 55s\n",
            "1479:\tlearn: 0.9992886\ttotal: 1m 42s\tremaining: 2m 55s\n",
            "1480:\tlearn: 0.9992886\ttotal: 1m 42s\tremaining: 2m 55s\n",
            "1481:\tlearn: 0.9992886\ttotal: 1m 43s\tremaining: 2m 55s\n",
            "1482:\tlearn: 0.9992818\ttotal: 1m 43s\tremaining: 2m 55s\n",
            "1483:\tlearn: 0.9992886\ttotal: 1m 43s\tremaining: 2m 54s\n",
            "1484:\tlearn: 0.9992886\ttotal: 1m 43s\tremaining: 2m 54s\n",
            "1485:\tlearn: 0.9992886\ttotal: 1m 43s\tremaining: 2m 54s\n",
            "1486:\tlearn: 0.9992955\ttotal: 1m 43s\tremaining: 2m 54s\n",
            "1487:\tlearn: 0.9992955\ttotal: 1m 43s\tremaining: 2m 54s\n",
            "1488:\tlearn: 0.9992955\ttotal: 1m 43s\tremaining: 2m 54s\n",
            "1489:\tlearn: 0.9993023\ttotal: 1m 43s\tremaining: 2m 54s\n",
            "1490:\tlearn: 0.9993023\ttotal: 1m 43s\tremaining: 2m 54s\n",
            "1491:\tlearn: 0.9993023\ttotal: 1m 43s\tremaining: 2m 54s\n",
            "1492:\tlearn: 0.9993023\ttotal: 1m 43s\tremaining: 2m 54s\n",
            "1493:\tlearn: 0.9992955\ttotal: 1m 43s\tremaining: 2m 54s\n",
            "1494:\tlearn: 0.9992955\ttotal: 1m 43s\tremaining: 2m 54s\n",
            "1495:\tlearn: 0.9993023\ttotal: 1m 43s\tremaining: 2m 53s\n",
            "1496:\tlearn: 0.9993023\ttotal: 1m 44s\tremaining: 2m 53s\n",
            "1497:\tlearn: 0.9993023\ttotal: 1m 44s\tremaining: 2m 53s\n",
            "1498:\tlearn: 0.9993023\ttotal: 1m 44s\tremaining: 2m 53s\n",
            "1499:\tlearn: 0.9993091\ttotal: 1m 44s\tremaining: 2m 53s\n",
            "1500:\tlearn: 0.9993091\ttotal: 1m 44s\tremaining: 2m 53s\n",
            "1501:\tlearn: 0.9993160\ttotal: 1m 44s\tremaining: 2m 53s\n",
            "1502:\tlearn: 0.9993228\ttotal: 1m 44s\tremaining: 2m 53s\n",
            "1503:\tlearn: 0.9993228\ttotal: 1m 44s\tremaining: 2m 53s\n",
            "1504:\tlearn: 0.9993228\ttotal: 1m 44s\tremaining: 2m 53s\n",
            "1505:\tlearn: 0.9993296\ttotal: 1m 44s\tremaining: 2m 53s\n",
            "1506:\tlearn: 0.9993296\ttotal: 1m 44s\tremaining: 2m 53s\n",
            "1507:\tlearn: 0.9993296\ttotal: 1m 44s\tremaining: 2m 53s\n",
            "1508:\tlearn: 0.9993433\ttotal: 1m 44s\tremaining: 2m 53s\n",
            "1509:\tlearn: 0.9993433\ttotal: 1m 44s\tremaining: 2m 52s\n",
            "1510:\tlearn: 0.9993433\ttotal: 1m 44s\tremaining: 2m 52s\n",
            "1511:\tlearn: 0.9993502\ttotal: 1m 44s\tremaining: 2m 52s\n",
            "1512:\tlearn: 0.9993502\ttotal: 1m 45s\tremaining: 2m 52s\n",
            "1513:\tlearn: 0.9993502\ttotal: 1m 45s\tremaining: 2m 52s\n",
            "1514:\tlearn: 0.9993502\ttotal: 1m 45s\tremaining: 2m 52s\n",
            "1515:\tlearn: 0.9993433\ttotal: 1m 45s\tremaining: 2m 52s\n",
            "1516:\tlearn: 0.9993433\ttotal: 1m 45s\tremaining: 2m 52s\n",
            "1517:\tlearn: 0.9993433\ttotal: 1m 45s\tremaining: 2m 52s\n",
            "1518:\tlearn: 0.9993433\ttotal: 1m 45s\tremaining: 2m 52s\n",
            "1519:\tlearn: 0.9993433\ttotal: 1m 45s\tremaining: 2m 52s\n",
            "1520:\tlearn: 0.9993433\ttotal: 1m 45s\tremaining: 2m 52s\n",
            "1521:\tlearn: 0.9993433\ttotal: 1m 45s\tremaining: 2m 51s\n",
            "1522:\tlearn: 0.9993502\ttotal: 1m 45s\tremaining: 2m 51s\n",
            "1523:\tlearn: 0.9993502\ttotal: 1m 45s\tremaining: 2m 51s\n",
            "1524:\tlearn: 0.9993502\ttotal: 1m 45s\tremaining: 2m 51s\n",
            "1525:\tlearn: 0.9993502\ttotal: 1m 45s\tremaining: 2m 51s\n",
            "1526:\tlearn: 0.9993502\ttotal: 1m 45s\tremaining: 2m 51s\n",
            "1527:\tlearn: 0.9993502\ttotal: 1m 45s\tremaining: 2m 51s\n",
            "1528:\tlearn: 0.9993502\ttotal: 1m 46s\tremaining: 2m 51s\n",
            "1529:\tlearn: 0.9993570\ttotal: 1m 46s\tremaining: 2m 51s\n",
            "1530:\tlearn: 0.9993570\ttotal: 1m 46s\tremaining: 2m 51s\n",
            "1531:\tlearn: 0.9993570\ttotal: 1m 46s\tremaining: 2m 51s\n",
            "1532:\tlearn: 0.9993502\ttotal: 1m 46s\tremaining: 2m 51s\n",
            "1533:\tlearn: 0.9993502\ttotal: 1m 46s\tremaining: 2m 51s\n",
            "1534:\tlearn: 0.9993502\ttotal: 1m 46s\tremaining: 2m 50s\n",
            "1535:\tlearn: 0.9993638\ttotal: 1m 46s\tremaining: 2m 50s\n",
            "1536:\tlearn: 0.9993707\ttotal: 1m 46s\tremaining: 2m 50s\n",
            "1537:\tlearn: 0.9993707\ttotal: 1m 46s\tremaining: 2m 50s\n",
            "1538:\tlearn: 0.9993707\ttotal: 1m 46s\tremaining: 2m 50s\n",
            "1539:\tlearn: 0.9993775\ttotal: 1m 46s\tremaining: 2m 50s\n",
            "1540:\tlearn: 0.9993707\ttotal: 1m 46s\tremaining: 2m 50s\n",
            "1541:\tlearn: 0.9993843\ttotal: 1m 46s\tremaining: 2m 50s\n",
            "1542:\tlearn: 0.9993843\ttotal: 1m 46s\tremaining: 2m 50s\n",
            "1543:\tlearn: 0.9993843\ttotal: 1m 47s\tremaining: 2m 50s\n",
            "1544:\tlearn: 0.9993843\ttotal: 1m 47s\tremaining: 2m 50s\n",
            "1545:\tlearn: 0.9993843\ttotal: 1m 47s\tremaining: 2m 50s\n",
            "1546:\tlearn: 0.9993775\ttotal: 1m 47s\tremaining: 2m 49s\n",
            "1547:\tlearn: 0.9993843\ttotal: 1m 47s\tremaining: 2m 49s\n",
            "1548:\tlearn: 0.9993843\ttotal: 1m 47s\tremaining: 2m 49s\n",
            "1549:\tlearn: 0.9993843\ttotal: 1m 47s\tremaining: 2m 49s\n",
            "1550:\tlearn: 0.9993843\ttotal: 1m 47s\tremaining: 2m 49s\n",
            "1551:\tlearn: 0.9993843\ttotal: 1m 47s\tremaining: 2m 49s\n",
            "1552:\tlearn: 0.9993707\ttotal: 1m 47s\tremaining: 2m 49s\n",
            "1553:\tlearn: 0.9993707\ttotal: 1m 47s\tremaining: 2m 49s\n",
            "1554:\tlearn: 0.9993843\ttotal: 1m 47s\tremaining: 2m 49s\n",
            "1555:\tlearn: 0.9993843\ttotal: 1m 47s\tremaining: 2m 49s\n",
            "1556:\tlearn: 0.9993912\ttotal: 1m 47s\tremaining: 2m 49s\n",
            "1557:\tlearn: 0.9993843\ttotal: 1m 47s\tremaining: 2m 49s\n",
            "1558:\tlearn: 0.9993843\ttotal: 1m 48s\tremaining: 2m 49s\n",
            "1559:\tlearn: 0.9993843\ttotal: 1m 48s\tremaining: 2m 49s\n",
            "1560:\tlearn: 0.9993980\ttotal: 1m 48s\tremaining: 2m 49s\n",
            "1561:\tlearn: 0.9994048\ttotal: 1m 48s\tremaining: 2m 48s\n",
            "1562:\tlearn: 0.9994048\ttotal: 1m 48s\tremaining: 2m 48s\n",
            "1563:\tlearn: 0.9994048\ttotal: 1m 48s\tremaining: 2m 48s\n",
            "1564:\tlearn: 0.9994048\ttotal: 1m 48s\tremaining: 2m 48s\n",
            "1565:\tlearn: 0.9993980\ttotal: 1m 48s\tremaining: 2m 48s\n",
            "1566:\tlearn: 0.9993980\ttotal: 1m 48s\tremaining: 2m 48s\n",
            "1567:\tlearn: 0.9993980\ttotal: 1m 48s\tremaining: 2m 48s\n",
            "1568:\tlearn: 0.9993980\ttotal: 1m 48s\tremaining: 2m 48s\n",
            "1569:\tlearn: 0.9993980\ttotal: 1m 48s\tremaining: 2m 48s\n",
            "1570:\tlearn: 0.9994117\ttotal: 1m 48s\tremaining: 2m 48s\n",
            "1571:\tlearn: 0.9994185\ttotal: 1m 48s\tremaining: 2m 48s\n",
            "1572:\tlearn: 0.9994185\ttotal: 1m 49s\tremaining: 2m 48s\n",
            "1573:\tlearn: 0.9994185\ttotal: 1m 49s\tremaining: 2m 48s\n",
            "1574:\tlearn: 0.9994185\ttotal: 1m 49s\tremaining: 2m 48s\n",
            "1575:\tlearn: 0.9994117\ttotal: 1m 49s\tremaining: 2m 48s\n",
            "1576:\tlearn: 0.9994117\ttotal: 1m 49s\tremaining: 2m 47s\n",
            "1577:\tlearn: 0.9994117\ttotal: 1m 49s\tremaining: 2m 47s\n",
            "1578:\tlearn: 0.9994117\ttotal: 1m 49s\tremaining: 2m 47s\n",
            "1579:\tlearn: 0.9994048\ttotal: 1m 49s\tremaining: 2m 47s\n",
            "1580:\tlearn: 0.9993980\ttotal: 1m 49s\tremaining: 2m 47s\n",
            "1581:\tlearn: 0.9994117\ttotal: 1m 49s\tremaining: 2m 47s\n",
            "1582:\tlearn: 0.9994254\ttotal: 1m 49s\tremaining: 2m 47s\n",
            "1583:\tlearn: 0.9994254\ttotal: 1m 49s\tremaining: 2m 47s\n",
            "1584:\tlearn: 0.9994254\ttotal: 1m 49s\tremaining: 2m 47s\n",
            "1585:\tlearn: 0.9994254\ttotal: 1m 49s\tremaining: 2m 47s\n",
            "1586:\tlearn: 0.9994254\ttotal: 1m 50s\tremaining: 2m 47s\n",
            "1587:\tlearn: 0.9994117\ttotal: 1m 50s\tremaining: 2m 47s\n",
            "1588:\tlearn: 0.9994117\ttotal: 1m 50s\tremaining: 2m 47s\n",
            "1589:\tlearn: 0.9994117\ttotal: 1m 50s\tremaining: 2m 47s\n",
            "1590:\tlearn: 0.9994117\ttotal: 1m 50s\tremaining: 2m 46s\n",
            "1591:\tlearn: 0.9994117\ttotal: 1m 50s\tremaining: 2m 46s\n",
            "1592:\tlearn: 0.9994254\ttotal: 1m 50s\tremaining: 2m 46s\n",
            "1593:\tlearn: 0.9994254\ttotal: 1m 50s\tremaining: 2m 46s\n",
            "1594:\tlearn: 0.9994322\ttotal: 1m 50s\tremaining: 2m 46s\n",
            "1595:\tlearn: 0.9994322\ttotal: 1m 50s\tremaining: 2m 46s\n",
            "1596:\tlearn: 0.9994322\ttotal: 1m 50s\tremaining: 2m 46s\n",
            "1597:\tlearn: 0.9994390\ttotal: 1m 50s\tremaining: 2m 46s\n",
            "1598:\tlearn: 0.9994322\ttotal: 1m 50s\tremaining: 2m 46s\n",
            "1599:\tlearn: 0.9994322\ttotal: 1m 50s\tremaining: 2m 46s\n",
            "1600:\tlearn: 0.9994254\ttotal: 1m 50s\tremaining: 2m 46s\n",
            "1601:\tlearn: 0.9994185\ttotal: 1m 51s\tremaining: 2m 46s\n",
            "1602:\tlearn: 0.9994254\ttotal: 1m 51s\tremaining: 2m 46s\n",
            "1603:\tlearn: 0.9994185\ttotal: 1m 51s\tremaining: 2m 46s\n",
            "1604:\tlearn: 0.9994185\ttotal: 1m 51s\tremaining: 2m 46s\n",
            "1605:\tlearn: 0.9994185\ttotal: 1m 51s\tremaining: 2m 45s\n",
            "1606:\tlearn: 0.9994254\ttotal: 1m 51s\tremaining: 2m 45s\n",
            "1607:\tlearn: 0.9994254\ttotal: 1m 51s\tremaining: 2m 45s\n",
            "1608:\tlearn: 0.9994254\ttotal: 1m 51s\tremaining: 2m 45s\n",
            "1609:\tlearn: 0.9994254\ttotal: 1m 51s\tremaining: 2m 45s\n",
            "1610:\tlearn: 0.9994254\ttotal: 1m 51s\tremaining: 2m 45s\n",
            "1611:\tlearn: 0.9994322\ttotal: 1m 51s\tremaining: 2m 45s\n",
            "1612:\tlearn: 0.9994459\ttotal: 1m 51s\tremaining: 2m 45s\n",
            "1613:\tlearn: 0.9994390\ttotal: 1m 51s\tremaining: 2m 45s\n",
            "1614:\tlearn: 0.9994459\ttotal: 1m 51s\tremaining: 2m 45s\n",
            "1615:\tlearn: 0.9994459\ttotal: 1m 52s\tremaining: 2m 45s\n",
            "1616:\tlearn: 0.9994459\ttotal: 1m 52s\tremaining: 2m 45s\n",
            "1617:\tlearn: 0.9994459\ttotal: 1m 52s\tremaining: 2m 45s\n",
            "1618:\tlearn: 0.9994390\ttotal: 1m 52s\tremaining: 2m 45s\n",
            "1619:\tlearn: 0.9994390\ttotal: 1m 52s\tremaining: 2m 45s\n",
            "1620:\tlearn: 0.9994390\ttotal: 1m 52s\tremaining: 2m 44s\n",
            "1621:\tlearn: 0.9994527\ttotal: 1m 52s\tremaining: 2m 44s\n",
            "1622:\tlearn: 0.9994459\ttotal: 1m 52s\tremaining: 2m 44s\n",
            "1623:\tlearn: 0.9994459\ttotal: 1m 52s\tremaining: 2m 44s\n",
            "1624:\tlearn: 0.9994595\ttotal: 1m 52s\tremaining: 2m 44s\n",
            "1625:\tlearn: 0.9994595\ttotal: 1m 52s\tremaining: 2m 44s\n",
            "1626:\tlearn: 0.9994664\ttotal: 1m 52s\tremaining: 2m 44s\n",
            "1627:\tlearn: 0.9994732\ttotal: 1m 52s\tremaining: 2m 44s\n",
            "1628:\tlearn: 0.9994732\ttotal: 1m 52s\tremaining: 2m 44s\n",
            "1629:\tlearn: 0.9994732\ttotal: 1m 53s\tremaining: 2m 44s\n",
            "1630:\tlearn: 0.9994664\ttotal: 1m 53s\tremaining: 2m 44s\n",
            "1631:\tlearn: 0.9994664\ttotal: 1m 53s\tremaining: 2m 44s\n",
            "1632:\tlearn: 0.9994732\ttotal: 1m 53s\tremaining: 2m 44s\n",
            "1633:\tlearn: 0.9994732\ttotal: 1m 53s\tremaining: 2m 44s\n",
            "1634:\tlearn: 0.9994801\ttotal: 1m 53s\tremaining: 2m 44s\n",
            "1635:\tlearn: 0.9994801\ttotal: 1m 53s\tremaining: 2m 43s\n",
            "1636:\tlearn: 0.9994801\ttotal: 1m 53s\tremaining: 2m 43s\n",
            "1637:\tlearn: 0.9994801\ttotal: 1m 53s\tremaining: 2m 43s\n",
            "1638:\tlearn: 0.9994869\ttotal: 1m 53s\tremaining: 2m 43s\n",
            "1639:\tlearn: 0.9994869\ttotal: 1m 53s\tremaining: 2m 43s\n",
            "1640:\tlearn: 0.9994869\ttotal: 1m 53s\tremaining: 2m 43s\n",
            "1641:\tlearn: 0.9994869\ttotal: 1m 53s\tremaining: 2m 43s\n",
            "1642:\tlearn: 0.9994732\ttotal: 1m 53s\tremaining: 2m 43s\n",
            "1643:\tlearn: 0.9994732\ttotal: 1m 54s\tremaining: 2m 43s\n",
            "1644:\tlearn: 0.9994732\ttotal: 1m 54s\tremaining: 2m 43s\n",
            "1645:\tlearn: 0.9994732\ttotal: 1m 54s\tremaining: 2m 43s\n",
            "1646:\tlearn: 0.9994732\ttotal: 1m 54s\tremaining: 2m 43s\n",
            "1647:\tlearn: 0.9994664\ttotal: 1m 54s\tremaining: 2m 43s\n",
            "1648:\tlearn: 0.9994664\ttotal: 1m 54s\tremaining: 2m 42s\n",
            "1649:\tlearn: 0.9994732\ttotal: 1m 54s\tremaining: 2m 42s\n",
            "1650:\tlearn: 0.9994732\ttotal: 1m 54s\tremaining: 2m 42s\n",
            "1651:\tlearn: 0.9994732\ttotal: 1m 54s\tremaining: 2m 42s\n",
            "1652:\tlearn: 0.9994801\ttotal: 1m 54s\tremaining: 2m 42s\n",
            "1653:\tlearn: 0.9994801\ttotal: 1m 54s\tremaining: 2m 42s\n",
            "1654:\tlearn: 0.9994732\ttotal: 1m 54s\tremaining: 2m 42s\n",
            "1655:\tlearn: 0.9994732\ttotal: 1m 54s\tremaining: 2m 42s\n",
            "1656:\tlearn: 0.9994801\ttotal: 1m 54s\tremaining: 2m 42s\n",
            "1657:\tlearn: 0.9994732\ttotal: 1m 54s\tremaining: 2m 42s\n",
            "1658:\tlearn: 0.9994801\ttotal: 1m 55s\tremaining: 2m 42s\n",
            "1659:\tlearn: 0.9994801\ttotal: 1m 55s\tremaining: 2m 42s\n",
            "1660:\tlearn: 0.9994869\ttotal: 1m 55s\tremaining: 2m 42s\n",
            "1661:\tlearn: 0.9994801\ttotal: 1m 55s\tremaining: 2m 42s\n",
            "1662:\tlearn: 0.9994801\ttotal: 1m 55s\tremaining: 2m 42s\n",
            "1663:\tlearn: 0.9994732\ttotal: 1m 55s\tremaining: 2m 41s\n",
            "1664:\tlearn: 0.9994732\ttotal: 1m 55s\tremaining: 2m 41s\n",
            "1665:\tlearn: 0.9994801\ttotal: 1m 55s\tremaining: 2m 41s\n",
            "1666:\tlearn: 0.9995006\ttotal: 1m 55s\tremaining: 2m 41s\n",
            "1667:\tlearn: 0.9995006\ttotal: 1m 55s\tremaining: 2m 41s\n",
            "1668:\tlearn: 0.9995006\ttotal: 1m 55s\tremaining: 2m 41s\n",
            "1669:\tlearn: 0.9995006\ttotal: 1m 55s\tremaining: 2m 41s\n",
            "1670:\tlearn: 0.9995006\ttotal: 1m 55s\tremaining: 2m 41s\n",
            "1671:\tlearn: 0.9995074\ttotal: 1m 55s\tremaining: 2m 41s\n",
            "1672:\tlearn: 0.9995074\ttotal: 1m 55s\tremaining: 2m 41s\n",
            "1673:\tlearn: 0.9995142\ttotal: 1m 56s\tremaining: 2m 41s\n",
            "1674:\tlearn: 0.9995142\ttotal: 1m 56s\tremaining: 2m 41s\n",
            "1675:\tlearn: 0.9995142\ttotal: 1m 56s\tremaining: 2m 41s\n",
            "1676:\tlearn: 0.9995142\ttotal: 1m 56s\tremaining: 2m 41s\n",
            "1677:\tlearn: 0.9995211\ttotal: 1m 56s\tremaining: 2m 40s\n",
            "1678:\tlearn: 0.9995211\ttotal: 1m 56s\tremaining: 2m 40s\n",
            "1679:\tlearn: 0.9995279\ttotal: 1m 56s\tremaining: 2m 40s\n",
            "1680:\tlearn: 0.9995279\ttotal: 1m 56s\tremaining: 2m 40s\n",
            "1681:\tlearn: 0.9995348\ttotal: 1m 56s\tremaining: 2m 40s\n",
            "1682:\tlearn: 0.9995348\ttotal: 1m 56s\tremaining: 2m 40s\n",
            "1683:\tlearn: 0.9995348\ttotal: 1m 56s\tremaining: 2m 40s\n",
            "1684:\tlearn: 0.9995279\ttotal: 1m 56s\tremaining: 2m 40s\n",
            "1685:\tlearn: 0.9995348\ttotal: 1m 56s\tremaining: 2m 40s\n",
            "1686:\tlearn: 0.9995348\ttotal: 1m 56s\tremaining: 2m 40s\n",
            "1687:\tlearn: 0.9995416\ttotal: 1m 57s\tremaining: 2m 40s\n",
            "1688:\tlearn: 0.9995416\ttotal: 1m 57s\tremaining: 2m 40s\n",
            "1689:\tlearn: 0.9995416\ttotal: 1m 57s\tremaining: 2m 40s\n",
            "1690:\tlearn: 0.9995416\ttotal: 1m 57s\tremaining: 2m 40s\n",
            "1691:\tlearn: 0.9995416\ttotal: 1m 57s\tremaining: 2m 39s\n",
            "1692:\tlearn: 0.9995416\ttotal: 1m 57s\tremaining: 2m 39s\n",
            "1693:\tlearn: 0.9995348\ttotal: 1m 57s\tremaining: 2m 39s\n",
            "1694:\tlearn: 0.9995484\ttotal: 1m 57s\tremaining: 2m 39s\n",
            "1695:\tlearn: 0.9995484\ttotal: 1m 57s\tremaining: 2m 39s\n",
            "1696:\tlearn: 0.9995484\ttotal: 1m 57s\tremaining: 2m 39s\n",
            "1697:\tlearn: 0.9995348\ttotal: 1m 57s\tremaining: 2m 39s\n",
            "1698:\tlearn: 0.9995348\ttotal: 1m 57s\tremaining: 2m 39s\n",
            "1699:\tlearn: 0.9995348\ttotal: 1m 57s\tremaining: 2m 39s\n",
            "1700:\tlearn: 0.9995348\ttotal: 1m 57s\tremaining: 2m 39s\n",
            "1701:\tlearn: 0.9995348\ttotal: 1m 57s\tremaining: 2m 39s\n",
            "1702:\tlearn: 0.9995348\ttotal: 1m 57s\tremaining: 2m 39s\n",
            "1703:\tlearn: 0.9995348\ttotal: 1m 57s\tremaining: 2m 38s\n",
            "1704:\tlearn: 0.9995348\ttotal: 1m 58s\tremaining: 2m 38s\n",
            "1705:\tlearn: 0.9995348\ttotal: 1m 58s\tremaining: 2m 38s\n",
            "1706:\tlearn: 0.9995348\ttotal: 1m 58s\tremaining: 2m 38s\n",
            "1707:\tlearn: 0.9995348\ttotal: 1m 58s\tremaining: 2m 38s\n",
            "1708:\tlearn: 0.9995416\ttotal: 1m 58s\tremaining: 2m 38s\n",
            "1709:\tlearn: 0.9995416\ttotal: 1m 58s\tremaining: 2m 38s\n",
            "1710:\tlearn: 0.9995416\ttotal: 1m 58s\tremaining: 2m 38s\n",
            "1711:\tlearn: 0.9995484\ttotal: 1m 58s\tremaining: 2m 38s\n",
            "1712:\tlearn: 0.9995484\ttotal: 1m 58s\tremaining: 2m 38s\n",
            "1713:\tlearn: 0.9995553\ttotal: 1m 58s\tremaining: 2m 38s\n",
            "1714:\tlearn: 0.9995553\ttotal: 1m 58s\tremaining: 2m 38s\n",
            "1715:\tlearn: 0.9995484\ttotal: 1m 58s\tremaining: 2m 38s\n",
            "1716:\tlearn: 0.9995621\ttotal: 1m 58s\tremaining: 2m 37s\n",
            "1717:\tlearn: 0.9995553\ttotal: 1m 58s\tremaining: 2m 37s\n",
            "1718:\tlearn: 0.9995621\ttotal: 1m 58s\tremaining: 2m 37s\n",
            "1719:\tlearn: 0.9995621\ttotal: 1m 58s\tremaining: 2m 37s\n",
            "1720:\tlearn: 0.9995621\ttotal: 1m 59s\tremaining: 2m 37s\n",
            "1721:\tlearn: 0.9995621\ttotal: 1m 59s\tremaining: 2m 37s\n",
            "1722:\tlearn: 0.9995621\ttotal: 1m 59s\tremaining: 2m 37s\n",
            "1723:\tlearn: 0.9995621\ttotal: 1m 59s\tremaining: 2m 37s\n",
            "1724:\tlearn: 0.9995690\ttotal: 1m 59s\tremaining: 2m 37s\n",
            "1725:\tlearn: 0.9995690\ttotal: 1m 59s\tremaining: 2m 37s\n",
            "1726:\tlearn: 0.9995758\ttotal: 1m 59s\tremaining: 2m 37s\n",
            "1727:\tlearn: 0.9995758\ttotal: 1m 59s\tremaining: 2m 37s\n",
            "1728:\tlearn: 0.9995758\ttotal: 1m 59s\tremaining: 2m 37s\n",
            "1729:\tlearn: 0.9995758\ttotal: 1m 59s\tremaining: 2m 36s\n",
            "1730:\tlearn: 0.9995758\ttotal: 1m 59s\tremaining: 2m 36s\n",
            "1731:\tlearn: 0.9995758\ttotal: 1m 59s\tremaining: 2m 36s\n",
            "1732:\tlearn: 0.9995826\ttotal: 1m 59s\tremaining: 2m 36s\n",
            "1733:\tlearn: 0.9995826\ttotal: 1m 59s\tremaining: 2m 36s\n",
            "1734:\tlearn: 0.9995826\ttotal: 1m 59s\tremaining: 2m 36s\n",
            "1735:\tlearn: 0.9995826\ttotal: 2m\tremaining: 2m 36s\n",
            "1736:\tlearn: 0.9995826\ttotal: 2m\tremaining: 2m 36s\n",
            "1737:\tlearn: 0.9995826\ttotal: 2m\tremaining: 2m 36s\n",
            "1738:\tlearn: 0.9995826\ttotal: 2m\tremaining: 2m 36s\n",
            "1739:\tlearn: 0.9995826\ttotal: 2m\tremaining: 2m 36s\n",
            "1740:\tlearn: 0.9995826\ttotal: 2m\tremaining: 2m 36s\n",
            "1741:\tlearn: 0.9995826\ttotal: 2m\tremaining: 2m 36s\n",
            "1742:\tlearn: 0.9995895\ttotal: 2m\tremaining: 2m 35s\n",
            "1743:\tlearn: 0.9995895\ttotal: 2m\tremaining: 2m 35s\n",
            "1744:\tlearn: 0.9995963\ttotal: 2m\tremaining: 2m 35s\n",
            "1745:\tlearn: 0.9995963\ttotal: 2m\tremaining: 2m 35s\n",
            "1746:\tlearn: 0.9995963\ttotal: 2m\tremaining: 2m 35s\n",
            "1747:\tlearn: 0.9995963\ttotal: 2m\tremaining: 2m 35s\n",
            "1748:\tlearn: 0.9995963\ttotal: 2m\tremaining: 2m 35s\n",
            "1749:\tlearn: 0.9995895\ttotal: 2m\tremaining: 2m 35s\n",
            "1750:\tlearn: 0.9995963\ttotal: 2m\tremaining: 2m 35s\n",
            "1751:\tlearn: 0.9995963\ttotal: 2m 1s\tremaining: 2m 35s\n",
            "1752:\tlearn: 0.9995963\ttotal: 2m 1s\tremaining: 2m 35s\n",
            "1753:\tlearn: 0.9995963\ttotal: 2m 1s\tremaining: 2m 35s\n",
            "1754:\tlearn: 0.9995963\ttotal: 2m 1s\tremaining: 2m 35s\n",
            "1755:\tlearn: 0.9995963\ttotal: 2m 1s\tremaining: 2m 35s\n",
            "1756:\tlearn: 0.9995963\ttotal: 2m 1s\tremaining: 2m 34s\n",
            "1757:\tlearn: 0.9995963\ttotal: 2m 1s\tremaining: 2m 34s\n",
            "1758:\tlearn: 0.9995963\ttotal: 2m 1s\tremaining: 2m 34s\n",
            "1759:\tlearn: 0.9995963\ttotal: 2m 1s\tremaining: 2m 34s\n",
            "1760:\tlearn: 0.9995963\ttotal: 2m 1s\tremaining: 2m 34s\n",
            "1761:\tlearn: 0.9995963\ttotal: 2m 1s\tremaining: 2m 34s\n",
            "1762:\tlearn: 0.9995963\ttotal: 2m 1s\tremaining: 2m 34s\n",
            "1763:\tlearn: 0.9995963\ttotal: 2m 1s\tremaining: 2m 34s\n",
            "1764:\tlearn: 0.9995963\ttotal: 2m 1s\tremaining: 2m 34s\n",
            "1765:\tlearn: 0.9995895\ttotal: 2m 1s\tremaining: 2m 34s\n",
            "1766:\tlearn: 0.9995895\ttotal: 2m 1s\tremaining: 2m 34s\n",
            "1767:\tlearn: 0.9995895\ttotal: 2m 2s\tremaining: 2m 34s\n",
            "1768:\tlearn: 0.9995895\ttotal: 2m 2s\tremaining: 2m 33s\n",
            "1769:\tlearn: 0.9995895\ttotal: 2m 2s\tremaining: 2m 33s\n",
            "1770:\tlearn: 0.9995895\ttotal: 2m 2s\tremaining: 2m 33s\n",
            "1771:\tlearn: 0.9995895\ttotal: 2m 2s\tremaining: 2m 33s\n",
            "1772:\tlearn: 0.9995895\ttotal: 2m 2s\tremaining: 2m 33s\n",
            "1773:\tlearn: 0.9995895\ttotal: 2m 2s\tremaining: 2m 33s\n",
            "1774:\tlearn: 0.9995895\ttotal: 2m 2s\tremaining: 2m 33s\n",
            "1775:\tlearn: 0.9995895\ttotal: 2m 2s\tremaining: 2m 33s\n",
            "1776:\tlearn: 0.9995963\ttotal: 2m 2s\tremaining: 2m 33s\n",
            "1777:\tlearn: 0.9995963\ttotal: 2m 2s\tremaining: 2m 33s\n",
            "1778:\tlearn: 0.9995963\ttotal: 2m 2s\tremaining: 2m 33s\n",
            "1779:\tlearn: 0.9995963\ttotal: 2m 2s\tremaining: 2m 33s\n",
            "1780:\tlearn: 0.9995963\ttotal: 2m 2s\tremaining: 2m 32s\n",
            "1781:\tlearn: 0.9995963\ttotal: 2m 2s\tremaining: 2m 32s\n",
            "1782:\tlearn: 0.9995963\ttotal: 2m 2s\tremaining: 2m 32s\n",
            "1783:\tlearn: 0.9995963\ttotal: 2m 2s\tremaining: 2m 32s\n",
            "1784:\tlearn: 0.9995963\ttotal: 2m 3s\tremaining: 2m 32s\n",
            "1785:\tlearn: 0.9995963\ttotal: 2m 3s\tremaining: 2m 32s\n",
            "1786:\tlearn: 0.9995895\ttotal: 2m 3s\tremaining: 2m 32s\n",
            "1787:\tlearn: 0.9995895\ttotal: 2m 3s\tremaining: 2m 32s\n",
            "1788:\tlearn: 0.9995895\ttotal: 2m 3s\tremaining: 2m 32s\n",
            "1789:\tlearn: 0.9995895\ttotal: 2m 3s\tremaining: 2m 32s\n",
            "1790:\tlearn: 0.9995963\ttotal: 2m 3s\tremaining: 2m 32s\n",
            "1791:\tlearn: 0.9996032\ttotal: 2m 3s\tremaining: 2m 32s\n",
            "1792:\tlearn: 0.9996032\ttotal: 2m 3s\tremaining: 2m 32s\n",
            "1793:\tlearn: 0.9996032\ttotal: 2m 3s\tremaining: 2m 32s\n",
            "1794:\tlearn: 0.9996032\ttotal: 2m 3s\tremaining: 2m 32s\n",
            "1795:\tlearn: 0.9996100\ttotal: 2m 3s\tremaining: 2m 31s\n",
            "1796:\tlearn: 0.9996100\ttotal: 2m 3s\tremaining: 2m 31s\n",
            "1797:\tlearn: 0.9996168\ttotal: 2m 3s\tremaining: 2m 31s\n",
            "1798:\tlearn: 0.9996168\ttotal: 2m 3s\tremaining: 2m 31s\n",
            "1799:\tlearn: 0.9996168\ttotal: 2m 4s\tremaining: 2m 31s\n",
            "1800:\tlearn: 0.9996168\ttotal: 2m 4s\tremaining: 2m 31s\n",
            "1801:\tlearn: 0.9996168\ttotal: 2m 4s\tremaining: 2m 31s\n",
            "1802:\tlearn: 0.9996168\ttotal: 2m 4s\tremaining: 2m 31s\n",
            "1803:\tlearn: 0.9996168\ttotal: 2m 4s\tremaining: 2m 31s\n",
            "1804:\tlearn: 0.9996168\ttotal: 2m 4s\tremaining: 2m 31s\n",
            "1805:\tlearn: 0.9996100\ttotal: 2m 4s\tremaining: 2m 31s\n",
            "1806:\tlearn: 0.9996100\ttotal: 2m 4s\tremaining: 2m 31s\n",
            "1807:\tlearn: 0.9996100\ttotal: 2m 4s\tremaining: 2m 30s\n",
            "1808:\tlearn: 0.9996100\ttotal: 2m 4s\tremaining: 2m 30s\n",
            "1809:\tlearn: 0.9996100\ttotal: 2m 4s\tremaining: 2m 30s\n",
            "1810:\tlearn: 0.9996100\ttotal: 2m 4s\tremaining: 2m 30s\n",
            "1811:\tlearn: 0.9996100\ttotal: 2m 4s\tremaining: 2m 30s\n",
            "1812:\tlearn: 0.9996100\ttotal: 2m 4s\tremaining: 2m 30s\n",
            "1813:\tlearn: 0.9996100\ttotal: 2m 4s\tremaining: 2m 30s\n",
            "1814:\tlearn: 0.9996100\ttotal: 2m 4s\tremaining: 2m 30s\n",
            "1815:\tlearn: 0.9996168\ttotal: 2m 5s\tremaining: 2m 30s\n",
            "1816:\tlearn: 0.9996237\ttotal: 2m 5s\tremaining: 2m 30s\n",
            "1817:\tlearn: 0.9996237\ttotal: 2m 5s\tremaining: 2m 30s\n",
            "1818:\tlearn: 0.9996237\ttotal: 2m 5s\tremaining: 2m 30s\n",
            "1819:\tlearn: 0.9996237\ttotal: 2m 5s\tremaining: 2m 30s\n",
            "1820:\tlearn: 0.9996237\ttotal: 2m 5s\tremaining: 2m 29s\n",
            "1821:\tlearn: 0.9996237\ttotal: 2m 5s\tremaining: 2m 29s\n",
            "1822:\tlearn: 0.9996237\ttotal: 2m 5s\tremaining: 2m 29s\n",
            "1823:\tlearn: 0.9996237\ttotal: 2m 5s\tremaining: 2m 29s\n",
            "1824:\tlearn: 0.9996237\ttotal: 2m 5s\tremaining: 2m 29s\n",
            "1825:\tlearn: 0.9996237\ttotal: 2m 5s\tremaining: 2m 29s\n",
            "1826:\tlearn: 0.9996168\ttotal: 2m 5s\tremaining: 2m 29s\n",
            "1827:\tlearn: 0.9996168\ttotal: 2m 5s\tremaining: 2m 29s\n",
            "1828:\tlearn: 0.9996168\ttotal: 2m 5s\tremaining: 2m 29s\n",
            "1829:\tlearn: 0.9996168\ttotal: 2m 5s\tremaining: 2m 29s\n",
            "1830:\tlearn: 0.9996168\ttotal: 2m 5s\tremaining: 2m 29s\n",
            "1831:\tlearn: 0.9996237\ttotal: 2m 6s\tremaining: 2m 29s\n",
            "1832:\tlearn: 0.9996374\ttotal: 2m 6s\tremaining: 2m 29s\n",
            "1833:\tlearn: 0.9996374\ttotal: 2m 6s\tremaining: 2m 28s\n",
            "1834:\tlearn: 0.9996374\ttotal: 2m 6s\tremaining: 2m 28s\n",
            "1835:\tlearn: 0.9996374\ttotal: 2m 6s\tremaining: 2m 28s\n",
            "1836:\tlearn: 0.9996374\ttotal: 2m 6s\tremaining: 2m 28s\n",
            "1837:\tlearn: 0.9996374\ttotal: 2m 6s\tremaining: 2m 28s\n",
            "1838:\tlearn: 0.9996442\ttotal: 2m 6s\tremaining: 2m 28s\n",
            "1839:\tlearn: 0.9996442\ttotal: 2m 6s\tremaining: 2m 28s\n",
            "1840:\tlearn: 0.9996647\ttotal: 2m 6s\tremaining: 2m 28s\n",
            "1841:\tlearn: 0.9996579\ttotal: 2m 6s\tremaining: 2m 28s\n",
            "1842:\tlearn: 0.9996510\ttotal: 2m 6s\tremaining: 2m 28s\n",
            "1843:\tlearn: 0.9996510\ttotal: 2m 7s\tremaining: 2m 28s\n",
            "1844:\tlearn: 0.9996579\ttotal: 2m 7s\tremaining: 2m 28s\n",
            "1845:\tlearn: 0.9996579\ttotal: 2m 7s\tremaining: 2m 28s\n",
            "1846:\tlearn: 0.9996579\ttotal: 2m 7s\tremaining: 2m 28s\n",
            "1847:\tlearn: 0.9996579\ttotal: 2m 7s\tremaining: 2m 28s\n",
            "1848:\tlearn: 0.9996579\ttotal: 2m 7s\tremaining: 2m 28s\n",
            "1849:\tlearn: 0.9996579\ttotal: 2m 7s\tremaining: 2m 28s\n",
            "1850:\tlearn: 0.9996579\ttotal: 2m 7s\tremaining: 2m 28s\n",
            "1851:\tlearn: 0.9996647\ttotal: 2m 8s\tremaining: 2m 28s\n",
            "1852:\tlearn: 0.9996510\ttotal: 2m 8s\tremaining: 2m 28s\n",
            "1853:\tlearn: 0.9996510\ttotal: 2m 8s\tremaining: 2m 28s\n",
            "1854:\tlearn: 0.9996510\ttotal: 2m 8s\tremaining: 2m 28s\n",
            "1855:\tlearn: 0.9996579\ttotal: 2m 8s\tremaining: 2m 28s\n",
            "1856:\tlearn: 0.9996579\ttotal: 2m 8s\tremaining: 2m 28s\n",
            "1857:\tlearn: 0.9996579\ttotal: 2m 8s\tremaining: 2m 28s\n",
            "1858:\tlearn: 0.9996579\ttotal: 2m 8s\tremaining: 2m 28s\n",
            "1859:\tlearn: 0.9996579\ttotal: 2m 8s\tremaining: 2m 28s\n",
            "1860:\tlearn: 0.9996579\ttotal: 2m 9s\tremaining: 2m 28s\n",
            "1861:\tlearn: 0.9996579\ttotal: 2m 9s\tremaining: 2m 28s\n",
            "1862:\tlearn: 0.9996579\ttotal: 2m 9s\tremaining: 2m 28s\n",
            "1863:\tlearn: 0.9996579\ttotal: 2m 9s\tremaining: 2m 28s\n",
            "1864:\tlearn: 0.9996579\ttotal: 2m 9s\tremaining: 2m 28s\n",
            "1865:\tlearn: 0.9996579\ttotal: 2m 9s\tremaining: 2m 28s\n",
            "1866:\tlearn: 0.9996579\ttotal: 2m 9s\tremaining: 2m 28s\n",
            "1867:\tlearn: 0.9996579\ttotal: 2m 9s\tremaining: 2m 28s\n",
            "1868:\tlearn: 0.9996579\ttotal: 2m 9s\tremaining: 2m 28s\n",
            "1869:\tlearn: 0.9996579\ttotal: 2m 9s\tremaining: 2m 27s\n",
            "1870:\tlearn: 0.9996579\ttotal: 2m 9s\tremaining: 2m 27s\n",
            "1871:\tlearn: 0.9996579\ttotal: 2m 10s\tremaining: 2m 27s\n",
            "1872:\tlearn: 0.9996579\ttotal: 2m 10s\tremaining: 2m 27s\n",
            "1873:\tlearn: 0.9996579\ttotal: 2m 10s\tremaining: 2m 27s\n",
            "1874:\tlearn: 0.9996579\ttotal: 2m 10s\tremaining: 2m 27s\n",
            "1875:\tlearn: 0.9996579\ttotal: 2m 10s\tremaining: 2m 27s\n",
            "1876:\tlearn: 0.9996716\ttotal: 2m 10s\tremaining: 2m 27s\n",
            "1877:\tlearn: 0.9996716\ttotal: 2m 10s\tremaining: 2m 27s\n",
            "1878:\tlearn: 0.9996716\ttotal: 2m 10s\tremaining: 2m 27s\n",
            "1879:\tlearn: 0.9996852\ttotal: 2m 10s\tremaining: 2m 27s\n",
            "1880:\tlearn: 0.9996852\ttotal: 2m 10s\tremaining: 2m 27s\n",
            "1881:\tlearn: 0.9996784\ttotal: 2m 10s\tremaining: 2m 27s\n",
            "1882:\tlearn: 0.9996784\ttotal: 2m 10s\tremaining: 2m 27s\n",
            "1883:\tlearn: 0.9996852\ttotal: 2m 10s\tremaining: 2m 26s\n",
            "1884:\tlearn: 0.9996852\ttotal: 2m 10s\tremaining: 2m 26s\n",
            "1885:\tlearn: 0.9996852\ttotal: 2m 10s\tremaining: 2m 26s\n",
            "1886:\tlearn: 0.9996852\ttotal: 2m 11s\tremaining: 2m 26s\n",
            "1887:\tlearn: 0.9996852\ttotal: 2m 11s\tremaining: 2m 26s\n",
            "1888:\tlearn: 0.9996852\ttotal: 2m 11s\tremaining: 2m 26s\n",
            "1889:\tlearn: 0.9996852\ttotal: 2m 11s\tremaining: 2m 26s\n",
            "1890:\tlearn: 0.9996852\ttotal: 2m 11s\tremaining: 2m 26s\n",
            "1891:\tlearn: 0.9996852\ttotal: 2m 11s\tremaining: 2m 26s\n",
            "1892:\tlearn: 0.9996852\ttotal: 2m 11s\tremaining: 2m 26s\n",
            "1893:\tlearn: 0.9996784\ttotal: 2m 11s\tremaining: 2m 26s\n",
            "1894:\tlearn: 0.9996784\ttotal: 2m 11s\tremaining: 2m 26s\n",
            "1895:\tlearn: 0.9996784\ttotal: 2m 11s\tremaining: 2m 26s\n",
            "1896:\tlearn: 0.9996784\ttotal: 2m 11s\tremaining: 2m 26s\n",
            "1897:\tlearn: 0.9996852\ttotal: 2m 11s\tremaining: 2m 25s\n",
            "1898:\tlearn: 0.9996852\ttotal: 2m 11s\tremaining: 2m 25s\n",
            "1899:\tlearn: 0.9996852\ttotal: 2m 11s\tremaining: 2m 25s\n",
            "1900:\tlearn: 0.9996852\ttotal: 2m 12s\tremaining: 2m 25s\n",
            "1901:\tlearn: 0.9996852\ttotal: 2m 12s\tremaining: 2m 25s\n",
            "1902:\tlearn: 0.9996852\ttotal: 2m 12s\tremaining: 2m 25s\n",
            "1903:\tlearn: 0.9996852\ttotal: 2m 12s\tremaining: 2m 25s\n",
            "1904:\tlearn: 0.9996921\ttotal: 2m 12s\tremaining: 2m 25s\n",
            "1905:\tlearn: 0.9996921\ttotal: 2m 12s\tremaining: 2m 25s\n",
            "1906:\tlearn: 0.9996921\ttotal: 2m 12s\tremaining: 2m 25s\n",
            "1907:\tlearn: 0.9996921\ttotal: 2m 12s\tremaining: 2m 25s\n",
            "1908:\tlearn: 0.9996921\ttotal: 2m 12s\tremaining: 2m 25s\n",
            "1909:\tlearn: 0.9996989\ttotal: 2m 12s\tremaining: 2m 25s\n",
            "1910:\tlearn: 0.9996989\ttotal: 2m 12s\tremaining: 2m 25s\n",
            "1911:\tlearn: 0.9996989\ttotal: 2m 12s\tremaining: 2m 24s\n",
            "1912:\tlearn: 0.9996921\ttotal: 2m 12s\tremaining: 2m 24s\n",
            "1913:\tlearn: 0.9996921\ttotal: 2m 12s\tremaining: 2m 24s\n",
            "1914:\tlearn: 0.9996921\ttotal: 2m 12s\tremaining: 2m 24s\n",
            "1915:\tlearn: 0.9996921\ttotal: 2m 12s\tremaining: 2m 24s\n",
            "1916:\tlearn: 0.9996921\ttotal: 2m 13s\tremaining: 2m 24s\n",
            "1917:\tlearn: 0.9996921\ttotal: 2m 13s\tremaining: 2m 24s\n",
            "1918:\tlearn: 0.9996921\ttotal: 2m 13s\tremaining: 2m 24s\n",
            "1919:\tlearn: 0.9996921\ttotal: 2m 13s\tremaining: 2m 24s\n",
            "1920:\tlearn: 0.9996921\ttotal: 2m 13s\tremaining: 2m 24s\n",
            "1921:\tlearn: 0.9997058\ttotal: 2m 13s\tremaining: 2m 24s\n",
            "1922:\tlearn: 0.9996989\ttotal: 2m 13s\tremaining: 2m 24s\n",
            "1923:\tlearn: 0.9997058\ttotal: 2m 13s\tremaining: 2m 24s\n",
            "1924:\tlearn: 0.9997058\ttotal: 2m 13s\tremaining: 2m 24s\n",
            "1925:\tlearn: 0.9996921\ttotal: 2m 13s\tremaining: 2m 23s\n",
            "1926:\tlearn: 0.9996921\ttotal: 2m 13s\tremaining: 2m 23s\n",
            "1927:\tlearn: 0.9996921\ttotal: 2m 13s\tremaining: 2m 23s\n",
            "1928:\tlearn: 0.9996921\ttotal: 2m 13s\tremaining: 2m 23s\n",
            "1929:\tlearn: 0.9996921\ttotal: 2m 13s\tremaining: 2m 23s\n",
            "1930:\tlearn: 0.9996921\ttotal: 2m 13s\tremaining: 2m 23s\n",
            "1931:\tlearn: 0.9996989\ttotal: 2m 14s\tremaining: 2m 23s\n",
            "1932:\tlearn: 0.9996989\ttotal: 2m 14s\tremaining: 2m 23s\n",
            "1933:\tlearn: 0.9996989\ttotal: 2m 14s\tremaining: 2m 23s\n",
            "1934:\tlearn: 0.9996989\ttotal: 2m 14s\tremaining: 2m 23s\n",
            "1935:\tlearn: 0.9996989\ttotal: 2m 14s\tremaining: 2m 23s\n",
            "1936:\tlearn: 0.9997058\ttotal: 2m 14s\tremaining: 2m 23s\n",
            "1937:\tlearn: 0.9997058\ttotal: 2m 14s\tremaining: 2m 23s\n",
            "1938:\tlearn: 0.9997058\ttotal: 2m 14s\tremaining: 2m 23s\n",
            "1939:\tlearn: 0.9997058\ttotal: 2m 14s\tremaining: 2m 22s\n",
            "1940:\tlearn: 0.9997058\ttotal: 2m 14s\tremaining: 2m 22s\n",
            "1941:\tlearn: 0.9997058\ttotal: 2m 14s\tremaining: 2m 22s\n",
            "1942:\tlearn: 0.9997058\ttotal: 2m 14s\tremaining: 2m 22s\n",
            "1943:\tlearn: 0.9997058\ttotal: 2m 14s\tremaining: 2m 22s\n",
            "1944:\tlearn: 0.9997058\ttotal: 2m 14s\tremaining: 2m 22s\n",
            "1945:\tlearn: 0.9997058\ttotal: 2m 15s\tremaining: 2m 22s\n",
            "1946:\tlearn: 0.9997058\ttotal: 2m 15s\tremaining: 2m 22s\n",
            "1947:\tlearn: 0.9997058\ttotal: 2m 15s\tremaining: 2m 22s\n",
            "1948:\tlearn: 0.9997058\ttotal: 2m 15s\tremaining: 2m 22s\n",
            "1949:\tlearn: 0.9997058\ttotal: 2m 15s\tremaining: 2m 22s\n",
            "1950:\tlearn: 0.9997058\ttotal: 2m 15s\tremaining: 2m 22s\n",
            "1951:\tlearn: 0.9997058\ttotal: 2m 15s\tremaining: 2m 22s\n",
            "1952:\tlearn: 0.9997058\ttotal: 2m 15s\tremaining: 2m 22s\n",
            "1953:\tlearn: 0.9997058\ttotal: 2m 15s\tremaining: 2m 21s\n",
            "1954:\tlearn: 0.9997058\ttotal: 2m 15s\tremaining: 2m 21s\n",
            "1955:\tlearn: 0.9997058\ttotal: 2m 15s\tremaining: 2m 21s\n",
            "1956:\tlearn: 0.9997058\ttotal: 2m 15s\tremaining: 2m 21s\n",
            "1957:\tlearn: 0.9997058\ttotal: 2m 15s\tremaining: 2m 21s\n",
            "1958:\tlearn: 0.9997058\ttotal: 2m 15s\tremaining: 2m 21s\n",
            "1959:\tlearn: 0.9996989\ttotal: 2m 15s\tremaining: 2m 21s\n",
            "1960:\tlearn: 0.9996989\ttotal: 2m 16s\tremaining: 2m 21s\n",
            "1961:\tlearn: 0.9996989\ttotal: 2m 16s\tremaining: 2m 21s\n",
            "1962:\tlearn: 0.9997058\ttotal: 2m 16s\tremaining: 2m 21s\n",
            "1963:\tlearn: 0.9997058\ttotal: 2m 16s\tremaining: 2m 21s\n",
            "1964:\tlearn: 0.9997058\ttotal: 2m 16s\tremaining: 2m 21s\n",
            "1965:\tlearn: 0.9997058\ttotal: 2m 16s\tremaining: 2m 21s\n",
            "1966:\tlearn: 0.9997058\ttotal: 2m 16s\tremaining: 2m 21s\n",
            "1967:\tlearn: 0.9997058\ttotal: 2m 16s\tremaining: 2m 20s\n",
            "1968:\tlearn: 0.9997058\ttotal: 2m 16s\tremaining: 2m 20s\n",
            "1969:\tlearn: 0.9996989\ttotal: 2m 16s\tremaining: 2m 20s\n",
            "1970:\tlearn: 0.9996989\ttotal: 2m 16s\tremaining: 2m 20s\n",
            "1971:\tlearn: 0.9996989\ttotal: 2m 16s\tremaining: 2m 20s\n",
            "1972:\tlearn: 0.9996989\ttotal: 2m 16s\tremaining: 2m 20s\n",
            "1973:\tlearn: 0.9997058\ttotal: 2m 16s\tremaining: 2m 20s\n",
            "1974:\tlearn: 0.9997126\ttotal: 2m 16s\tremaining: 2m 20s\n",
            "1975:\tlearn: 0.9997126\ttotal: 2m 17s\tremaining: 2m 20s\n",
            "1976:\tlearn: 0.9997058\ttotal: 2m 17s\tremaining: 2m 20s\n",
            "1977:\tlearn: 0.9997058\ttotal: 2m 17s\tremaining: 2m 20s\n",
            "1978:\tlearn: 0.9997058\ttotal: 2m 17s\tremaining: 2m 20s\n",
            "1979:\tlearn: 0.9997058\ttotal: 2m 17s\tremaining: 2m 20s\n",
            "1980:\tlearn: 0.9997126\ttotal: 2m 17s\tremaining: 2m 20s\n",
            "1981:\tlearn: 0.9997126\ttotal: 2m 17s\tremaining: 2m 19s\n",
            "1982:\tlearn: 0.9997126\ttotal: 2m 17s\tremaining: 2m 19s\n",
            "1983:\tlearn: 0.9997126\ttotal: 2m 17s\tremaining: 2m 19s\n",
            "1984:\tlearn: 0.9997126\ttotal: 2m 17s\tremaining: 2m 19s\n",
            "1985:\tlearn: 0.9997194\ttotal: 2m 17s\tremaining: 2m 19s\n",
            "1986:\tlearn: 0.9997263\ttotal: 2m 17s\tremaining: 2m 19s\n",
            "1987:\tlearn: 0.9997263\ttotal: 2m 17s\tremaining: 2m 19s\n",
            "1988:\tlearn: 0.9997263\ttotal: 2m 17s\tremaining: 2m 19s\n",
            "1989:\tlearn: 0.9997263\ttotal: 2m 18s\tremaining: 2m 19s\n",
            "1990:\tlearn: 0.9997263\ttotal: 2m 18s\tremaining: 2m 19s\n",
            "1991:\tlearn: 0.9997263\ttotal: 2m 18s\tremaining: 2m 19s\n",
            "1992:\tlearn: 0.9997263\ttotal: 2m 18s\tremaining: 2m 19s\n",
            "1993:\tlearn: 0.9997263\ttotal: 2m 18s\tremaining: 2m 19s\n",
            "1994:\tlearn: 0.9997400\ttotal: 2m 18s\tremaining: 2m 19s\n",
            "1995:\tlearn: 0.9997400\ttotal: 2m 18s\tremaining: 2m 18s\n",
            "1996:\tlearn: 0.9997400\ttotal: 2m 18s\tremaining: 2m 18s\n",
            "1997:\tlearn: 0.9997400\ttotal: 2m 18s\tremaining: 2m 18s\n",
            "1998:\tlearn: 0.9997400\ttotal: 2m 18s\tremaining: 2m 18s\n",
            "1999:\tlearn: 0.9997400\ttotal: 2m 18s\tremaining: 2m 18s\n",
            "2000:\tlearn: 0.9997400\ttotal: 2m 18s\tremaining: 2m 18s\n",
            "2001:\tlearn: 0.9997468\ttotal: 2m 18s\tremaining: 2m 18s\n",
            "2002:\tlearn: 0.9997468\ttotal: 2m 18s\tremaining: 2m 18s\n",
            "2003:\tlearn: 0.9997468\ttotal: 2m 18s\tremaining: 2m 18s\n",
            "2004:\tlearn: 0.9997468\ttotal: 2m 18s\tremaining: 2m 18s\n",
            "2005:\tlearn: 0.9997468\ttotal: 2m 18s\tremaining: 2m 18s\n",
            "2006:\tlearn: 0.9997468\ttotal: 2m 19s\tremaining: 2m 18s\n",
            "2007:\tlearn: 0.9997468\ttotal: 2m 19s\tremaining: 2m 18s\n",
            "2008:\tlearn: 0.9997468\ttotal: 2m 19s\tremaining: 2m 17s\n",
            "2009:\tlearn: 0.9997468\ttotal: 2m 19s\tremaining: 2m 17s\n",
            "2010:\tlearn: 0.9997468\ttotal: 2m 19s\tremaining: 2m 17s\n",
            "2011:\tlearn: 0.9997468\ttotal: 2m 19s\tremaining: 2m 17s\n",
            "2012:\tlearn: 0.9997468\ttotal: 2m 19s\tremaining: 2m 17s\n",
            "2013:\tlearn: 0.9997468\ttotal: 2m 19s\tremaining: 2m 17s\n",
            "2014:\tlearn: 0.9997468\ttotal: 2m 19s\tremaining: 2m 17s\n",
            "2015:\tlearn: 0.9997468\ttotal: 2m 19s\tremaining: 2m 17s\n",
            "2016:\tlearn: 0.9997536\ttotal: 2m 19s\tremaining: 2m 17s\n",
            "2017:\tlearn: 0.9997536\ttotal: 2m 19s\tremaining: 2m 17s\n",
            "2018:\tlearn: 0.9997468\ttotal: 2m 19s\tremaining: 2m 17s\n",
            "2019:\tlearn: 0.9997468\ttotal: 2m 19s\tremaining: 2m 17s\n",
            "2020:\tlearn: 0.9997468\ttotal: 2m 19s\tremaining: 2m 17s\n",
            "2021:\tlearn: 0.9997468\ttotal: 2m 20s\tremaining: 2m 16s\n",
            "2022:\tlearn: 0.9997468\ttotal: 2m 20s\tremaining: 2m 16s\n",
            "2023:\tlearn: 0.9997468\ttotal: 2m 20s\tremaining: 2m 16s\n",
            "2024:\tlearn: 0.9997468\ttotal: 2m 20s\tremaining: 2m 16s\n",
            "2025:\tlearn: 0.9997468\ttotal: 2m 20s\tremaining: 2m 16s\n",
            "2026:\tlearn: 0.9997468\ttotal: 2m 20s\tremaining: 2m 16s\n",
            "2027:\tlearn: 0.9997400\ttotal: 2m 20s\tremaining: 2m 16s\n",
            "2028:\tlearn: 0.9997468\ttotal: 2m 20s\tremaining: 2m 16s\n",
            "2029:\tlearn: 0.9997468\ttotal: 2m 20s\tremaining: 2m 16s\n",
            "2030:\tlearn: 0.9997468\ttotal: 2m 20s\tremaining: 2m 16s\n",
            "2031:\tlearn: 0.9997400\ttotal: 2m 20s\tremaining: 2m 16s\n",
            "2032:\tlearn: 0.9997400\ttotal: 2m 20s\tremaining: 2m 16s\n",
            "2033:\tlearn: 0.9997400\ttotal: 2m 20s\tremaining: 2m 16s\n",
            "2034:\tlearn: 0.9997400\ttotal: 2m 20s\tremaining: 2m 15s\n",
            "2035:\tlearn: 0.9997400\ttotal: 2m 20s\tremaining: 2m 15s\n",
            "2036:\tlearn: 0.9997400\ttotal: 2m 20s\tremaining: 2m 15s\n",
            "2037:\tlearn: 0.9997400\ttotal: 2m 21s\tremaining: 2m 15s\n",
            "2038:\tlearn: 0.9997400\ttotal: 2m 21s\tremaining: 2m 15s\n",
            "2039:\tlearn: 0.9997400\ttotal: 2m 21s\tremaining: 2m 15s\n",
            "2040:\tlearn: 0.9997400\ttotal: 2m 21s\tremaining: 2m 15s\n",
            "2041:\tlearn: 0.9997536\ttotal: 2m 21s\tremaining: 2m 15s\n",
            "2042:\tlearn: 0.9997536\ttotal: 2m 21s\tremaining: 2m 15s\n",
            "2043:\tlearn: 0.9997536\ttotal: 2m 21s\tremaining: 2m 15s\n",
            "2044:\tlearn: 0.9997536\ttotal: 2m 21s\tremaining: 2m 15s\n",
            "2045:\tlearn: 0.9997468\ttotal: 2m 21s\tremaining: 2m 15s\n",
            "2046:\tlearn: 0.9997468\ttotal: 2m 21s\tremaining: 2m 15s\n",
            "2047:\tlearn: 0.9997468\ttotal: 2m 21s\tremaining: 2m 15s\n",
            "2048:\tlearn: 0.9997468\ttotal: 2m 21s\tremaining: 2m 14s\n",
            "2049:\tlearn: 0.9997536\ttotal: 2m 21s\tremaining: 2m 14s\n",
            "2050:\tlearn: 0.9997605\ttotal: 2m 21s\tremaining: 2m 14s\n",
            "2051:\tlearn: 0.9997605\ttotal: 2m 21s\tremaining: 2m 14s\n",
            "2052:\tlearn: 0.9997605\ttotal: 2m 22s\tremaining: 2m 14s\n",
            "2053:\tlearn: 0.9997605\ttotal: 2m 22s\tremaining: 2m 14s\n",
            "2054:\tlearn: 0.9997605\ttotal: 2m 22s\tremaining: 2m 14s\n",
            "2055:\tlearn: 0.9997536\ttotal: 2m 22s\tremaining: 2m 14s\n",
            "2056:\tlearn: 0.9997536\ttotal: 2m 22s\tremaining: 2m 14s\n",
            "2057:\tlearn: 0.9997468\ttotal: 2m 22s\tremaining: 2m 14s\n",
            "2058:\tlearn: 0.9997468\ttotal: 2m 22s\tremaining: 2m 14s\n",
            "2059:\tlearn: 0.9997468\ttotal: 2m 22s\tremaining: 2m 14s\n",
            "2060:\tlearn: 0.9997468\ttotal: 2m 22s\tremaining: 2m 14s\n",
            "2061:\tlearn: 0.9997468\ttotal: 2m 22s\tremaining: 2m 14s\n",
            "2062:\tlearn: 0.9997468\ttotal: 2m 22s\tremaining: 2m 13s\n",
            "2063:\tlearn: 0.9997468\ttotal: 2m 22s\tremaining: 2m 13s\n",
            "2064:\tlearn: 0.9997468\ttotal: 2m 22s\tremaining: 2m 13s\n",
            "2065:\tlearn: 0.9997468\ttotal: 2m 22s\tremaining: 2m 13s\n",
            "2066:\tlearn: 0.9997468\ttotal: 2m 22s\tremaining: 2m 13s\n",
            "2067:\tlearn: 0.9997468\ttotal: 2m 23s\tremaining: 2m 13s\n",
            "2068:\tlearn: 0.9997468\ttotal: 2m 23s\tremaining: 2m 13s\n",
            "2069:\tlearn: 0.9997468\ttotal: 2m 23s\tremaining: 2m 13s\n",
            "2070:\tlearn: 0.9997468\ttotal: 2m 23s\tremaining: 2m 13s\n",
            "2071:\tlearn: 0.9997468\ttotal: 2m 23s\tremaining: 2m 13s\n",
            "2072:\tlearn: 0.9997468\ttotal: 2m 23s\tremaining: 2m 13s\n",
            "2073:\tlearn: 0.9997468\ttotal: 2m 23s\tremaining: 2m 13s\n",
            "2074:\tlearn: 0.9997468\ttotal: 2m 23s\tremaining: 2m 13s\n",
            "2075:\tlearn: 0.9997468\ttotal: 2m 23s\tremaining: 2m 13s\n",
            "2076:\tlearn: 0.9997468\ttotal: 2m 23s\tremaining: 2m 13s\n",
            "2077:\tlearn: 0.9997468\ttotal: 2m 23s\tremaining: 2m 13s\n",
            "2078:\tlearn: 0.9997605\ttotal: 2m 23s\tremaining: 2m 12s\n",
            "2079:\tlearn: 0.9997536\ttotal: 2m 23s\tremaining: 2m 12s\n",
            "2080:\tlearn: 0.9997536\ttotal: 2m 24s\tremaining: 2m 12s\n",
            "2081:\tlearn: 0.9997536\ttotal: 2m 24s\tremaining: 2m 12s\n",
            "2082:\tlearn: 0.9997536\ttotal: 2m 24s\tremaining: 2m 12s\n",
            "2083:\tlearn: 0.9997536\ttotal: 2m 24s\tremaining: 2m 12s\n",
            "2084:\tlearn: 0.9997536\ttotal: 2m 24s\tremaining: 2m 12s\n",
            "2085:\tlearn: 0.9997605\ttotal: 2m 24s\tremaining: 2m 12s\n",
            "2086:\tlearn: 0.9997605\ttotal: 2m 24s\tremaining: 2m 12s\n",
            "2087:\tlearn: 0.9997536\ttotal: 2m 24s\tremaining: 2m 12s\n",
            "2088:\tlearn: 0.9997468\ttotal: 2m 24s\tremaining: 2m 12s\n",
            "2089:\tlearn: 0.9997468\ttotal: 2m 24s\tremaining: 2m 12s\n",
            "2090:\tlearn: 0.9997536\ttotal: 2m 24s\tremaining: 2m 12s\n",
            "2091:\tlearn: 0.9997536\ttotal: 2m 24s\tremaining: 2m 12s\n",
            "2092:\tlearn: 0.9997605\ttotal: 2m 24s\tremaining: 2m 12s\n",
            "2093:\tlearn: 0.9997536\ttotal: 2m 24s\tremaining: 2m 11s\n",
            "2094:\tlearn: 0.9997605\ttotal: 2m 25s\tremaining: 2m 11s\n",
            "2095:\tlearn: 0.9997742\ttotal: 2m 25s\tremaining: 2m 11s\n",
            "2096:\tlearn: 0.9997742\ttotal: 2m 25s\tremaining: 2m 11s\n",
            "2097:\tlearn: 0.9997742\ttotal: 2m 25s\tremaining: 2m 11s\n",
            "2098:\tlearn: 0.9997742\ttotal: 2m 25s\tremaining: 2m 11s\n",
            "2099:\tlearn: 0.9997742\ttotal: 2m 25s\tremaining: 2m 11s\n",
            "2100:\tlearn: 0.9997742\ttotal: 2m 25s\tremaining: 2m 11s\n",
            "2101:\tlearn: 0.9997742\ttotal: 2m 25s\tremaining: 2m 11s\n",
            "2102:\tlearn: 0.9997742\ttotal: 2m 25s\tremaining: 2m 11s\n",
            "2103:\tlearn: 0.9997742\ttotal: 2m 25s\tremaining: 2m 11s\n",
            "2104:\tlearn: 0.9997742\ttotal: 2m 25s\tremaining: 2m 11s\n",
            "2105:\tlearn: 0.9997742\ttotal: 2m 25s\tremaining: 2m 11s\n",
            "2106:\tlearn: 0.9997742\ttotal: 2m 25s\tremaining: 2m 11s\n",
            "2107:\tlearn: 0.9997742\ttotal: 2m 26s\tremaining: 2m 11s\n",
            "2108:\tlearn: 0.9997742\ttotal: 2m 26s\tremaining: 2m 11s\n",
            "2109:\tlearn: 0.9997742\ttotal: 2m 26s\tremaining: 2m 10s\n",
            "2110:\tlearn: 0.9997742\ttotal: 2m 26s\tremaining: 2m 10s\n",
            "2111:\tlearn: 0.9997742\ttotal: 2m 26s\tremaining: 2m 10s\n",
            "2112:\tlearn: 0.9997742\ttotal: 2m 26s\tremaining: 2m 10s\n",
            "2113:\tlearn: 0.9997742\ttotal: 2m 26s\tremaining: 2m 10s\n",
            "2114:\tlearn: 0.9997742\ttotal: 2m 26s\tremaining: 2m 10s\n",
            "2115:\tlearn: 0.9997742\ttotal: 2m 26s\tremaining: 2m 10s\n",
            "2116:\tlearn: 0.9997742\ttotal: 2m 26s\tremaining: 2m 10s\n",
            "2117:\tlearn: 0.9997742\ttotal: 2m 26s\tremaining: 2m 10s\n",
            "2118:\tlearn: 0.9997742\ttotal: 2m 26s\tremaining: 2m 10s\n",
            "2119:\tlearn: 0.9997742\ttotal: 2m 26s\tremaining: 2m 10s\n",
            "2120:\tlearn: 0.9997742\ttotal: 2m 27s\tremaining: 2m 10s\n",
            "2121:\tlearn: 0.9997742\ttotal: 2m 27s\tremaining: 2m 10s\n",
            "2122:\tlearn: 0.9997742\ttotal: 2m 27s\tremaining: 2m 10s\n",
            "2123:\tlearn: 0.9997742\ttotal: 2m 27s\tremaining: 2m 10s\n",
            "2124:\tlearn: 0.9997742\ttotal: 2m 27s\tremaining: 2m 9s\n",
            "2125:\tlearn: 0.9997742\ttotal: 2m 27s\tremaining: 2m 9s\n",
            "2126:\tlearn: 0.9997742\ttotal: 2m 27s\tremaining: 2m 9s\n",
            "2127:\tlearn: 0.9997742\ttotal: 2m 27s\tremaining: 2m 9s\n",
            "2128:\tlearn: 0.9997742\ttotal: 2m 27s\tremaining: 2m 9s\n",
            "2129:\tlearn: 0.9997742\ttotal: 2m 27s\tremaining: 2m 9s\n",
            "2130:\tlearn: 0.9997742\ttotal: 2m 27s\tremaining: 2m 9s\n",
            "2131:\tlearn: 0.9997742\ttotal: 2m 27s\tremaining: 2m 9s\n",
            "2132:\tlearn: 0.9997742\ttotal: 2m 27s\tremaining: 2m 9s\n",
            "2133:\tlearn: 0.9997742\ttotal: 2m 28s\tremaining: 2m 9s\n",
            "2134:\tlearn: 0.9997742\ttotal: 2m 28s\tremaining: 2m 9s\n",
            "2135:\tlearn: 0.9997742\ttotal: 2m 28s\tremaining: 2m 9s\n",
            "2136:\tlearn: 0.9997742\ttotal: 2m 28s\tremaining: 2m 9s\n",
            "2137:\tlearn: 0.9997742\ttotal: 2m 28s\tremaining: 2m 9s\n",
            "2138:\tlearn: 0.9997742\ttotal: 2m 28s\tremaining: 2m 9s\n",
            "2139:\tlearn: 0.9997742\ttotal: 2m 28s\tremaining: 2m 9s\n",
            "2140:\tlearn: 0.9997742\ttotal: 2m 28s\tremaining: 2m 9s\n",
            "2141:\tlearn: 0.9997742\ttotal: 2m 28s\tremaining: 2m 9s\n",
            "2142:\tlearn: 0.9997742\ttotal: 2m 28s\tremaining: 2m 8s\n",
            "2143:\tlearn: 0.9997742\ttotal: 2m 28s\tremaining: 2m 8s\n",
            "2144:\tlearn: 0.9997742\ttotal: 2m 29s\tremaining: 2m 8s\n",
            "2145:\tlearn: 0.9997742\ttotal: 2m 29s\tremaining: 2m 8s\n",
            "2146:\tlearn: 0.9997742\ttotal: 2m 29s\tremaining: 2m 8s\n",
            "2147:\tlearn: 0.9997742\ttotal: 2m 29s\tremaining: 2m 8s\n",
            "2148:\tlearn: 0.9997742\ttotal: 2m 29s\tremaining: 2m 8s\n",
            "2149:\tlearn: 0.9997742\ttotal: 2m 29s\tremaining: 2m 8s\n",
            "2150:\tlearn: 0.9997742\ttotal: 2m 29s\tremaining: 2m 8s\n",
            "2151:\tlearn: 0.9997742\ttotal: 2m 29s\tremaining: 2m 8s\n",
            "2152:\tlearn: 0.9997742\ttotal: 2m 29s\tremaining: 2m 8s\n",
            "2153:\tlearn: 0.9997742\ttotal: 2m 29s\tremaining: 2m 8s\n",
            "2154:\tlearn: 0.9997742\ttotal: 2m 29s\tremaining: 2m 8s\n",
            "2155:\tlearn: 0.9997742\ttotal: 2m 29s\tremaining: 2m 8s\n",
            "2156:\tlearn: 0.9997742\ttotal: 2m 29s\tremaining: 2m 8s\n",
            "2157:\tlearn: 0.9997742\ttotal: 2m 30s\tremaining: 2m 8s\n",
            "2158:\tlearn: 0.9997742\ttotal: 2m 30s\tremaining: 2m 8s\n",
            "2159:\tlearn: 0.9997742\ttotal: 2m 30s\tremaining: 2m 7s\n",
            "2160:\tlearn: 0.9997742\ttotal: 2m 30s\tremaining: 2m 7s\n",
            "2161:\tlearn: 0.9997742\ttotal: 2m 30s\tremaining: 2m 7s\n",
            "2162:\tlearn: 0.9997742\ttotal: 2m 30s\tremaining: 2m 7s\n",
            "2163:\tlearn: 0.9997742\ttotal: 2m 30s\tremaining: 2m 7s\n",
            "2164:\tlearn: 0.9997742\ttotal: 2m 30s\tremaining: 2m 7s\n",
            "2165:\tlearn: 0.9997742\ttotal: 2m 30s\tremaining: 2m 7s\n",
            "2166:\tlearn: 0.9997742\ttotal: 2m 30s\tremaining: 2m 7s\n",
            "2167:\tlearn: 0.9997742\ttotal: 2m 30s\tremaining: 2m 7s\n",
            "2168:\tlearn: 0.9997742\ttotal: 2m 30s\tremaining: 2m 7s\n",
            "2169:\tlearn: 0.9997742\ttotal: 2m 30s\tremaining: 2m 7s\n",
            "2170:\tlearn: 0.9997742\ttotal: 2m 31s\tremaining: 2m 7s\n",
            "2171:\tlearn: 0.9997742\ttotal: 2m 31s\tremaining: 2m 7s\n",
            "2172:\tlearn: 0.9997742\ttotal: 2m 31s\tremaining: 2m 7s\n",
            "2173:\tlearn: 0.9997742\ttotal: 2m 31s\tremaining: 2m 7s\n",
            "2174:\tlearn: 0.9997742\ttotal: 2m 31s\tremaining: 2m 6s\n",
            "2175:\tlearn: 0.9997742\ttotal: 2m 31s\tremaining: 2m 6s\n",
            "2176:\tlearn: 0.9997673\ttotal: 2m 31s\tremaining: 2m 6s\n",
            "2177:\tlearn: 0.9997810\ttotal: 2m 31s\tremaining: 2m 6s\n",
            "2178:\tlearn: 0.9997810\ttotal: 2m 31s\tremaining: 2m 6s\n",
            "2179:\tlearn: 0.9997810\ttotal: 2m 31s\tremaining: 2m 6s\n",
            "2180:\tlearn: 0.9997742\ttotal: 2m 31s\tremaining: 2m 6s\n",
            "2181:\tlearn: 0.9997742\ttotal: 2m 31s\tremaining: 2m 6s\n",
            "2182:\tlearn: 0.9997742\ttotal: 2m 31s\tremaining: 2m 6s\n",
            "2183:\tlearn: 0.9997742\ttotal: 2m 32s\tremaining: 2m 6s\n",
            "2184:\tlearn: 0.9997742\ttotal: 2m 32s\tremaining: 2m 6s\n",
            "2185:\tlearn: 0.9997742\ttotal: 2m 32s\tremaining: 2m 6s\n",
            "2186:\tlearn: 0.9997742\ttotal: 2m 32s\tremaining: 2m 6s\n",
            "2187:\tlearn: 0.9997742\ttotal: 2m 32s\tremaining: 2m 6s\n",
            "2188:\tlearn: 0.9997742\ttotal: 2m 32s\tremaining: 2m 6s\n",
            "2189:\tlearn: 0.9997673\ttotal: 2m 32s\tremaining: 2m 6s\n",
            "2190:\tlearn: 0.9997673\ttotal: 2m 32s\tremaining: 2m 5s\n",
            "2191:\tlearn: 0.9997673\ttotal: 2m 32s\tremaining: 2m 5s\n",
            "2192:\tlearn: 0.9997673\ttotal: 2m 32s\tremaining: 2m 5s\n",
            "2193:\tlearn: 0.9997673\ttotal: 2m 32s\tremaining: 2m 5s\n",
            "2194:\tlearn: 0.9997673\ttotal: 2m 32s\tremaining: 2m 5s\n",
            "2195:\tlearn: 0.9997742\ttotal: 2m 32s\tremaining: 2m 5s\n",
            "2196:\tlearn: 0.9997742\ttotal: 2m 32s\tremaining: 2m 5s\n",
            "2197:\tlearn: 0.9997742\ttotal: 2m 33s\tremaining: 2m 5s\n",
            "2198:\tlearn: 0.9997742\ttotal: 2m 33s\tremaining: 2m 5s\n",
            "2199:\tlearn: 0.9997742\ttotal: 2m 33s\tremaining: 2m 5s\n",
            "2200:\tlearn: 0.9997742\ttotal: 2m 33s\tremaining: 2m 5s\n",
            "2201:\tlearn: 0.9997742\ttotal: 2m 33s\tremaining: 2m 5s\n",
            "2202:\tlearn: 0.9997742\ttotal: 2m 33s\tremaining: 2m 5s\n",
            "2203:\tlearn: 0.9997879\ttotal: 2m 33s\tremaining: 2m 5s\n",
            "2204:\tlearn: 0.9997879\ttotal: 2m 33s\tremaining: 2m 4s\n",
            "2205:\tlearn: 0.9997879\ttotal: 2m 33s\tremaining: 2m 4s\n",
            "2206:\tlearn: 0.9997879\ttotal: 2m 33s\tremaining: 2m 4s\n",
            "2207:\tlearn: 0.9997879\ttotal: 2m 33s\tremaining: 2m 4s\n",
            "2208:\tlearn: 0.9997879\ttotal: 2m 33s\tremaining: 2m 4s\n",
            "2209:\tlearn: 0.9997879\ttotal: 2m 33s\tremaining: 2m 4s\n",
            "2210:\tlearn: 0.9997947\ttotal: 2m 33s\tremaining: 2m 4s\n",
            "2211:\tlearn: 0.9997947\ttotal: 2m 33s\tremaining: 2m 4s\n",
            "2212:\tlearn: 0.9997947\ttotal: 2m 33s\tremaining: 2m 4s\n",
            "2213:\tlearn: 0.9997947\ttotal: 2m 34s\tremaining: 2m 4s\n",
            "2214:\tlearn: 0.9997947\ttotal: 2m 34s\tremaining: 2m 4s\n",
            "2215:\tlearn: 0.9997947\ttotal: 2m 34s\tremaining: 2m 4s\n",
            "2216:\tlearn: 0.9997947\ttotal: 2m 34s\tremaining: 2m 4s\n",
            "2217:\tlearn: 0.9997947\ttotal: 2m 34s\tremaining: 2m 3s\n",
            "2218:\tlearn: 0.9997947\ttotal: 2m 34s\tremaining: 2m 3s\n",
            "2219:\tlearn: 0.9997947\ttotal: 2m 34s\tremaining: 2m 3s\n",
            "2220:\tlearn: 0.9997947\ttotal: 2m 34s\tremaining: 2m 3s\n",
            "2221:\tlearn: 0.9997947\ttotal: 2m 34s\tremaining: 2m 3s\n",
            "2222:\tlearn: 0.9997947\ttotal: 2m 34s\tremaining: 2m 3s\n",
            "2223:\tlearn: 0.9997947\ttotal: 2m 34s\tremaining: 2m 3s\n",
            "2224:\tlearn: 0.9997947\ttotal: 2m 34s\tremaining: 2m 3s\n",
            "2225:\tlearn: 0.9997947\ttotal: 2m 34s\tremaining: 2m 3s\n",
            "2226:\tlearn: 0.9997947\ttotal: 2m 34s\tremaining: 2m 3s\n",
            "2227:\tlearn: 0.9997947\ttotal: 2m 34s\tremaining: 2m 3s\n",
            "2228:\tlearn: 0.9997947\ttotal: 2m 34s\tremaining: 2m 3s\n",
            "2229:\tlearn: 0.9997947\ttotal: 2m 35s\tremaining: 2m 3s\n",
            "2230:\tlearn: 0.9997947\ttotal: 2m 35s\tremaining: 2m 2s\n",
            "2231:\tlearn: 0.9997947\ttotal: 2m 35s\tremaining: 2m 2s\n",
            "2232:\tlearn: 0.9997947\ttotal: 2m 35s\tremaining: 2m 2s\n",
            "2233:\tlearn: 0.9998015\ttotal: 2m 35s\tremaining: 2m 2s\n",
            "2234:\tlearn: 0.9998015\ttotal: 2m 35s\tremaining: 2m 2s\n",
            "2235:\tlearn: 0.9998015\ttotal: 2m 35s\tremaining: 2m 2s\n",
            "2236:\tlearn: 0.9998015\ttotal: 2m 35s\tremaining: 2m 2s\n",
            "2237:\tlearn: 0.9998015\ttotal: 2m 35s\tremaining: 2m 2s\n",
            "2238:\tlearn: 0.9998015\ttotal: 2m 35s\tremaining: 2m 2s\n",
            "2239:\tlearn: 0.9998015\ttotal: 2m 35s\tremaining: 2m 2s\n",
            "2240:\tlearn: 0.9998015\ttotal: 2m 35s\tremaining: 2m 2s\n",
            "2241:\tlearn: 0.9998015\ttotal: 2m 35s\tremaining: 2m 2s\n",
            "2242:\tlearn: 0.9998084\ttotal: 2m 36s\tremaining: 2m 2s\n",
            "2243:\tlearn: 0.9998084\ttotal: 2m 36s\tremaining: 2m 2s\n",
            "2244:\tlearn: 0.9998084\ttotal: 2m 36s\tremaining: 2m 2s\n",
            "2245:\tlearn: 0.9998084\ttotal: 2m 36s\tremaining: 2m 1s\n",
            "2246:\tlearn: 0.9998084\ttotal: 2m 36s\tremaining: 2m 1s\n",
            "2247:\tlearn: 0.9998084\ttotal: 2m 36s\tremaining: 2m 1s\n",
            "2248:\tlearn: 0.9998084\ttotal: 2m 36s\tremaining: 2m 1s\n",
            "2249:\tlearn: 0.9998084\ttotal: 2m 36s\tremaining: 2m 1s\n",
            "2250:\tlearn: 0.9998084\ttotal: 2m 36s\tremaining: 2m 1s\n",
            "2251:\tlearn: 0.9998084\ttotal: 2m 36s\tremaining: 2m 1s\n",
            "2252:\tlearn: 0.9998084\ttotal: 2m 36s\tremaining: 2m 1s\n",
            "2253:\tlearn: 0.9998152\ttotal: 2m 36s\tremaining: 2m 1s\n",
            "2254:\tlearn: 0.9998152\ttotal: 2m 36s\tremaining: 2m 1s\n",
            "2255:\tlearn: 0.9998152\ttotal: 2m 36s\tremaining: 2m 1s\n",
            "2256:\tlearn: 0.9998152\ttotal: 2m 37s\tremaining: 2m 1s\n",
            "2257:\tlearn: 0.9998152\ttotal: 2m 37s\tremaining: 2m 1s\n",
            "2258:\tlearn: 0.9998152\ttotal: 2m 37s\tremaining: 2m 1s\n",
            "2259:\tlearn: 0.9998152\ttotal: 2m 37s\tremaining: 2m 1s\n",
            "2260:\tlearn: 0.9998152\ttotal: 2m 37s\tremaining: 2m\n",
            "2261:\tlearn: 0.9998152\ttotal: 2m 37s\tremaining: 2m\n",
            "2262:\tlearn: 0.9998152\ttotal: 2m 37s\tremaining: 2m\n",
            "2263:\tlearn: 0.9998152\ttotal: 2m 37s\tremaining: 2m\n",
            "2264:\tlearn: 0.9998152\ttotal: 2m 37s\tremaining: 2m\n",
            "2265:\tlearn: 0.9998152\ttotal: 2m 37s\tremaining: 2m\n",
            "2266:\tlearn: 0.9998152\ttotal: 2m 37s\tremaining: 2m\n",
            "2267:\tlearn: 0.9998152\ttotal: 2m 37s\tremaining: 2m\n",
            "2268:\tlearn: 0.9998152\ttotal: 2m 37s\tremaining: 2m\n",
            "2269:\tlearn: 0.9998152\ttotal: 2m 37s\tremaining: 2m\n",
            "2270:\tlearn: 0.9998152\ttotal: 2m 37s\tremaining: 2m\n",
            "2271:\tlearn: 0.9998152\ttotal: 2m 37s\tremaining: 2m\n",
            "2272:\tlearn: 0.9998152\ttotal: 2m 37s\tremaining: 2m\n",
            "2273:\tlearn: 0.9998152\ttotal: 2m 38s\tremaining: 1m 59s\n",
            "2274:\tlearn: 0.9998152\ttotal: 2m 38s\tremaining: 1m 59s\n",
            "2275:\tlearn: 0.9998152\ttotal: 2m 38s\tremaining: 1m 59s\n",
            "2276:\tlearn: 0.9998084\ttotal: 2m 38s\tremaining: 1m 59s\n",
            "2277:\tlearn: 0.9998084\ttotal: 2m 38s\tremaining: 1m 59s\n",
            "2278:\tlearn: 0.9998084\ttotal: 2m 38s\tremaining: 1m 59s\n",
            "2279:\tlearn: 0.9998084\ttotal: 2m 38s\tremaining: 1m 59s\n",
            "2280:\tlearn: 0.9998084\ttotal: 2m 38s\tremaining: 1m 59s\n",
            "2281:\tlearn: 0.9998084\ttotal: 2m 38s\tremaining: 1m 59s\n",
            "2282:\tlearn: 0.9998084\ttotal: 2m 38s\tremaining: 1m 59s\n",
            "2283:\tlearn: 0.9998084\ttotal: 2m 38s\tremaining: 1m 59s\n",
            "2284:\tlearn: 0.9998084\ttotal: 2m 38s\tremaining: 1m 59s\n",
            "2285:\tlearn: 0.9998084\ttotal: 2m 38s\tremaining: 1m 59s\n",
            "2286:\tlearn: 0.9998084\ttotal: 2m 38s\tremaining: 1m 59s\n",
            "2287:\tlearn: 0.9998084\ttotal: 2m 38s\tremaining: 1m 58s\n",
            "2288:\tlearn: 0.9998084\ttotal: 2m 39s\tremaining: 1m 58s\n",
            "2289:\tlearn: 0.9998084\ttotal: 2m 39s\tremaining: 1m 58s\n",
            "2290:\tlearn: 0.9998084\ttotal: 2m 39s\tremaining: 1m 58s\n",
            "2291:\tlearn: 0.9998084\ttotal: 2m 39s\tremaining: 1m 58s\n",
            "2292:\tlearn: 0.9998084\ttotal: 2m 39s\tremaining: 1m 58s\n",
            "2293:\tlearn: 0.9998084\ttotal: 2m 39s\tremaining: 1m 58s\n",
            "2294:\tlearn: 0.9998084\ttotal: 2m 39s\tremaining: 1m 58s\n",
            "2295:\tlearn: 0.9998084\ttotal: 2m 39s\tremaining: 1m 58s\n",
            "2296:\tlearn: 0.9998084\ttotal: 2m 39s\tremaining: 1m 58s\n",
            "2297:\tlearn: 0.9998084\ttotal: 2m 39s\tremaining: 1m 58s\n",
            "2298:\tlearn: 0.9998084\ttotal: 2m 39s\tremaining: 1m 58s\n",
            "2299:\tlearn: 0.9998084\ttotal: 2m 39s\tremaining: 1m 58s\n",
            "2300:\tlearn: 0.9998084\ttotal: 2m 39s\tremaining: 1m 57s\n",
            "2301:\tlearn: 0.9998084\ttotal: 2m 39s\tremaining: 1m 57s\n",
            "2302:\tlearn: 0.9998084\ttotal: 2m 39s\tremaining: 1m 57s\n",
            "2303:\tlearn: 0.9998084\ttotal: 2m 39s\tremaining: 1m 57s\n",
            "2304:\tlearn: 0.9998084\ttotal: 2m 39s\tremaining: 1m 57s\n",
            "2305:\tlearn: 0.9998084\ttotal: 2m 40s\tremaining: 1m 57s\n",
            "2306:\tlearn: 0.9998084\ttotal: 2m 40s\tremaining: 1m 57s\n",
            "2307:\tlearn: 0.9998084\ttotal: 2m 40s\tremaining: 1m 57s\n",
            "2308:\tlearn: 0.9998084\ttotal: 2m 40s\tremaining: 1m 57s\n",
            "2309:\tlearn: 0.9998084\ttotal: 2m 40s\tremaining: 1m 57s\n",
            "2310:\tlearn: 0.9998084\ttotal: 2m 40s\tremaining: 1m 57s\n",
            "2311:\tlearn: 0.9998084\ttotal: 2m 40s\tremaining: 1m 57s\n",
            "2312:\tlearn: 0.9998084\ttotal: 2m 40s\tremaining: 1m 57s\n",
            "2313:\tlearn: 0.9998084\ttotal: 2m 40s\tremaining: 1m 56s\n",
            "2314:\tlearn: 0.9998084\ttotal: 2m 40s\tremaining: 1m 56s\n",
            "2315:\tlearn: 0.9998084\ttotal: 2m 40s\tremaining: 1m 56s\n",
            "2316:\tlearn: 0.9998084\ttotal: 2m 40s\tremaining: 1m 56s\n",
            "2317:\tlearn: 0.9998084\ttotal: 2m 40s\tremaining: 1m 56s\n",
            "2318:\tlearn: 0.9998084\ttotal: 2m 40s\tremaining: 1m 56s\n",
            "2319:\tlearn: 0.9998084\ttotal: 2m 40s\tremaining: 1m 56s\n",
            "2320:\tlearn: 0.9998084\ttotal: 2m 40s\tremaining: 1m 56s\n",
            "2321:\tlearn: 0.9998084\ttotal: 2m 41s\tremaining: 1m 56s\n",
            "2322:\tlearn: 0.9998152\ttotal: 2m 41s\tremaining: 1m 56s\n",
            "2323:\tlearn: 0.9998152\ttotal: 2m 41s\tremaining: 1m 56s\n",
            "2324:\tlearn: 0.9998221\ttotal: 2m 41s\tremaining: 1m 56s\n",
            "2325:\tlearn: 0.9998221\ttotal: 2m 41s\tremaining: 1m 56s\n",
            "2326:\tlearn: 0.9998221\ttotal: 2m 41s\tremaining: 1m 56s\n",
            "2327:\tlearn: 0.9998221\ttotal: 2m 41s\tremaining: 1m 55s\n",
            "2328:\tlearn: 0.9998221\ttotal: 2m 41s\tremaining: 1m 55s\n",
            "2329:\tlearn: 0.9998221\ttotal: 2m 41s\tremaining: 1m 55s\n",
            "2330:\tlearn: 0.9998221\ttotal: 2m 41s\tremaining: 1m 55s\n",
            "2331:\tlearn: 0.9998221\ttotal: 2m 41s\tremaining: 1m 55s\n",
            "2332:\tlearn: 0.9998221\ttotal: 2m 41s\tremaining: 1m 55s\n",
            "2333:\tlearn: 0.9998221\ttotal: 2m 41s\tremaining: 1m 55s\n",
            "2334:\tlearn: 0.9998221\ttotal: 2m 41s\tremaining: 1m 55s\n",
            "2335:\tlearn: 0.9998221\ttotal: 2m 41s\tremaining: 1m 55s\n",
            "2336:\tlearn: 0.9998221\ttotal: 2m 42s\tremaining: 1m 55s\n",
            "2337:\tlearn: 0.9998221\ttotal: 2m 42s\tremaining: 1m 55s\n",
            "2338:\tlearn: 0.9998221\ttotal: 2m 42s\tremaining: 1m 55s\n",
            "2339:\tlearn: 0.9998221\ttotal: 2m 42s\tremaining: 1m 55s\n",
            "2340:\tlearn: 0.9998221\ttotal: 2m 42s\tremaining: 1m 55s\n",
            "2341:\tlearn: 0.9998221\ttotal: 2m 42s\tremaining: 1m 54s\n",
            "2342:\tlearn: 0.9998221\ttotal: 2m 42s\tremaining: 1m 54s\n",
            "2343:\tlearn: 0.9998221\ttotal: 2m 42s\tremaining: 1m 54s\n",
            "2344:\tlearn: 0.9998289\ttotal: 2m 42s\tremaining: 1m 54s\n",
            "2345:\tlearn: 0.9998289\ttotal: 2m 42s\tremaining: 1m 54s\n",
            "2346:\tlearn: 0.9998289\ttotal: 2m 42s\tremaining: 1m 54s\n",
            "2347:\tlearn: 0.9998357\ttotal: 2m 42s\tremaining: 1m 54s\n",
            "2348:\tlearn: 0.9998357\ttotal: 2m 42s\tremaining: 1m 54s\n",
            "2349:\tlearn: 0.9998357\ttotal: 2m 42s\tremaining: 1m 54s\n",
            "2350:\tlearn: 0.9998357\ttotal: 2m 42s\tremaining: 1m 54s\n",
            "2351:\tlearn: 0.9998357\ttotal: 2m 43s\tremaining: 1m 54s\n",
            "2352:\tlearn: 0.9998357\ttotal: 2m 43s\tremaining: 1m 54s\n",
            "2353:\tlearn: 0.9998357\ttotal: 2m 43s\tremaining: 1m 54s\n",
            "2354:\tlearn: 0.9998357\ttotal: 2m 43s\tremaining: 1m 54s\n",
            "2355:\tlearn: 0.9998357\ttotal: 2m 43s\tremaining: 1m 53s\n",
            "2356:\tlearn: 0.9998357\ttotal: 2m 43s\tremaining: 1m 53s\n",
            "2357:\tlearn: 0.9998357\ttotal: 2m 43s\tremaining: 1m 53s\n",
            "2358:\tlearn: 0.9998357\ttotal: 2m 43s\tremaining: 1m 53s\n",
            "2359:\tlearn: 0.9998357\ttotal: 2m 43s\tremaining: 1m 53s\n",
            "2360:\tlearn: 0.9998357\ttotal: 2m 43s\tremaining: 1m 53s\n",
            "2361:\tlearn: 0.9998357\ttotal: 2m 43s\tremaining: 1m 53s\n",
            "2362:\tlearn: 0.9998357\ttotal: 2m 43s\tremaining: 1m 53s\n",
            "2363:\tlearn: 0.9998357\ttotal: 2m 43s\tremaining: 1m 53s\n",
            "2364:\tlearn: 0.9998357\ttotal: 2m 43s\tremaining: 1m 53s\n",
            "2365:\tlearn: 0.9998357\ttotal: 2m 43s\tremaining: 1m 53s\n",
            "2366:\tlearn: 0.9998357\ttotal: 2m 43s\tremaining: 1m 53s\n",
            "2367:\tlearn: 0.9998357\ttotal: 2m 44s\tremaining: 1m 53s\n",
            "2368:\tlearn: 0.9998357\ttotal: 2m 44s\tremaining: 1m 52s\n",
            "2369:\tlearn: 0.9998357\ttotal: 2m 44s\tremaining: 1m 52s\n",
            "2370:\tlearn: 0.9998357\ttotal: 2m 44s\tremaining: 1m 52s\n",
            "2371:\tlearn: 0.9998357\ttotal: 2m 44s\tremaining: 1m 52s\n",
            "2372:\tlearn: 0.9998357\ttotal: 2m 44s\tremaining: 1m 52s\n",
            "2373:\tlearn: 0.9998357\ttotal: 2m 44s\tremaining: 1m 52s\n",
            "2374:\tlearn: 0.9998357\ttotal: 2m 44s\tremaining: 1m 52s\n",
            "2375:\tlearn: 0.9998357\ttotal: 2m 44s\tremaining: 1m 52s\n",
            "2376:\tlearn: 0.9998357\ttotal: 2m 44s\tremaining: 1m 52s\n",
            "2377:\tlearn: 0.9998357\ttotal: 2m 44s\tremaining: 1m 52s\n",
            "2378:\tlearn: 0.9998357\ttotal: 2m 44s\tremaining: 1m 52s\n",
            "2379:\tlearn: 0.9998357\ttotal: 2m 44s\tremaining: 1m 52s\n",
            "2380:\tlearn: 0.9998357\ttotal: 2m 44s\tremaining: 1m 52s\n",
            "2381:\tlearn: 0.9998357\ttotal: 2m 45s\tremaining: 1m 52s\n",
            "2382:\tlearn: 0.9998357\ttotal: 2m 45s\tremaining: 1m 52s\n",
            "2383:\tlearn: 0.9998357\ttotal: 2m 45s\tremaining: 1m 51s\n",
            "2384:\tlearn: 0.9998357\ttotal: 2m 45s\tremaining: 1m 51s\n",
            "2385:\tlearn: 0.9998357\ttotal: 2m 45s\tremaining: 1m 51s\n",
            "2386:\tlearn: 0.9998357\ttotal: 2m 45s\tremaining: 1m 51s\n",
            "2387:\tlearn: 0.9998357\ttotal: 2m 45s\tremaining: 1m 51s\n",
            "2388:\tlearn: 0.9998357\ttotal: 2m 45s\tremaining: 1m 51s\n",
            "2389:\tlearn: 0.9998357\ttotal: 2m 45s\tremaining: 1m 51s\n",
            "2390:\tlearn: 0.9998357\ttotal: 2m 45s\tremaining: 1m 51s\n",
            "2391:\tlearn: 0.9998289\ttotal: 2m 45s\tremaining: 1m 51s\n",
            "2392:\tlearn: 0.9998289\ttotal: 2m 45s\tremaining: 1m 51s\n",
            "2393:\tlearn: 0.9998289\ttotal: 2m 45s\tremaining: 1m 51s\n",
            "2394:\tlearn: 0.9998289\ttotal: 2m 45s\tremaining: 1m 51s\n",
            "2395:\tlearn: 0.9998289\ttotal: 2m 46s\tremaining: 1m 51s\n",
            "2396:\tlearn: 0.9998289\ttotal: 2m 46s\tremaining: 1m 51s\n",
            "2397:\tlearn: 0.9998289\ttotal: 2m 46s\tremaining: 1m 51s\n",
            "2398:\tlearn: 0.9998289\ttotal: 2m 46s\tremaining: 1m 50s\n",
            "2399:\tlearn: 0.9998289\ttotal: 2m 46s\tremaining: 1m 50s\n",
            "2400:\tlearn: 0.9998289\ttotal: 2m 46s\tremaining: 1m 50s\n",
            "2401:\tlearn: 0.9998289\ttotal: 2m 46s\tremaining: 1m 50s\n",
            "2402:\tlearn: 0.9998289\ttotal: 2m 46s\tremaining: 1m 50s\n",
            "2403:\tlearn: 0.9998289\ttotal: 2m 46s\tremaining: 1m 50s\n",
            "2404:\tlearn: 0.9998289\ttotal: 2m 46s\tremaining: 1m 50s\n",
            "2405:\tlearn: 0.9998289\ttotal: 2m 46s\tremaining: 1m 50s\n",
            "2406:\tlearn: 0.9998289\ttotal: 2m 46s\tremaining: 1m 50s\n",
            "2407:\tlearn: 0.9998289\ttotal: 2m 46s\tremaining: 1m 50s\n",
            "2408:\tlearn: 0.9998289\ttotal: 2m 46s\tremaining: 1m 50s\n",
            "2409:\tlearn: 0.9998289\ttotal: 2m 46s\tremaining: 1m 50s\n",
            "2410:\tlearn: 0.9998221\ttotal: 2m 47s\tremaining: 1m 50s\n",
            "2411:\tlearn: 0.9998221\ttotal: 2m 47s\tremaining: 1m 50s\n",
            "2412:\tlearn: 0.9998221\ttotal: 2m 47s\tremaining: 1m 49s\n",
            "2413:\tlearn: 0.9998221\ttotal: 2m 47s\tremaining: 1m 49s\n",
            "2414:\tlearn: 0.9998221\ttotal: 2m 47s\tremaining: 1m 49s\n",
            "2415:\tlearn: 0.9998221\ttotal: 2m 47s\tremaining: 1m 49s\n",
            "2416:\tlearn: 0.9998221\ttotal: 2m 47s\tremaining: 1m 49s\n",
            "2417:\tlearn: 0.9998221\ttotal: 2m 47s\tremaining: 1m 49s\n",
            "2418:\tlearn: 0.9998221\ttotal: 2m 47s\tremaining: 1m 49s\n",
            "2419:\tlearn: 0.9998221\ttotal: 2m 47s\tremaining: 1m 49s\n",
            "2420:\tlearn: 0.9998221\ttotal: 2m 47s\tremaining: 1m 49s\n",
            "2421:\tlearn: 0.9998221\ttotal: 2m 47s\tremaining: 1m 49s\n",
            "2422:\tlearn: 0.9998221\ttotal: 2m 47s\tremaining: 1m 49s\n",
            "2423:\tlearn: 0.9998221\ttotal: 2m 47s\tremaining: 1m 49s\n",
            "2424:\tlearn: 0.9998221\ttotal: 2m 47s\tremaining: 1m 49s\n",
            "2425:\tlearn: 0.9998221\ttotal: 2m 48s\tremaining: 1m 49s\n",
            "2426:\tlearn: 0.9998289\ttotal: 2m 48s\tremaining: 1m 48s\n",
            "2427:\tlearn: 0.9998289\ttotal: 2m 48s\tremaining: 1m 48s\n",
            "2428:\tlearn: 0.9998289\ttotal: 2m 48s\tremaining: 1m 48s\n",
            "2429:\tlearn: 0.9998289\ttotal: 2m 48s\tremaining: 1m 48s\n",
            "2430:\tlearn: 0.9998289\ttotal: 2m 48s\tremaining: 1m 48s\n",
            "2431:\tlearn: 0.9998289\ttotal: 2m 48s\tremaining: 1m 48s\n",
            "2432:\tlearn: 0.9998289\ttotal: 2m 48s\tremaining: 1m 48s\n",
            "2433:\tlearn: 0.9998289\ttotal: 2m 48s\tremaining: 1m 48s\n",
            "2434:\tlearn: 0.9998289\ttotal: 2m 48s\tremaining: 1m 48s\n",
            "2435:\tlearn: 0.9998289\ttotal: 2m 48s\tremaining: 1m 48s\n",
            "2436:\tlearn: 0.9998289\ttotal: 2m 48s\tremaining: 1m 48s\n",
            "2437:\tlearn: 0.9998289\ttotal: 2m 48s\tremaining: 1m 48s\n",
            "2438:\tlearn: 0.9998289\ttotal: 2m 48s\tremaining: 1m 48s\n",
            "2439:\tlearn: 0.9998289\ttotal: 2m 48s\tremaining: 1m 47s\n",
            "2440:\tlearn: 0.9998289\ttotal: 2m 48s\tremaining: 1m 47s\n",
            "2441:\tlearn: 0.9998289\ttotal: 2m 49s\tremaining: 1m 47s\n",
            "2442:\tlearn: 0.9998289\ttotal: 2m 49s\tremaining: 1m 47s\n",
            "2443:\tlearn: 0.9998289\ttotal: 2m 49s\tremaining: 1m 47s\n",
            "2444:\tlearn: 0.9998289\ttotal: 2m 49s\tremaining: 1m 47s\n",
            "2445:\tlearn: 0.9998289\ttotal: 2m 49s\tremaining: 1m 47s\n",
            "2446:\tlearn: 0.9998289\ttotal: 2m 49s\tremaining: 1m 47s\n",
            "2447:\tlearn: 0.9998289\ttotal: 2m 49s\tremaining: 1m 47s\n",
            "2448:\tlearn: 0.9998289\ttotal: 2m 49s\tremaining: 1m 47s\n",
            "2449:\tlearn: 0.9998289\ttotal: 2m 49s\tremaining: 1m 47s\n",
            "2450:\tlearn: 0.9998289\ttotal: 2m 49s\tremaining: 1m 47s\n",
            "2451:\tlearn: 0.9998289\ttotal: 2m 49s\tremaining: 1m 47s\n",
            "2452:\tlearn: 0.9998289\ttotal: 2m 49s\tremaining: 1m 47s\n",
            "2453:\tlearn: 0.9998289\ttotal: 2m 49s\tremaining: 1m 46s\n",
            "2454:\tlearn: 0.9998289\ttotal: 2m 49s\tremaining: 1m 46s\n",
            "2455:\tlearn: 0.9998289\ttotal: 2m 49s\tremaining: 1m 46s\n",
            "2456:\tlearn: 0.9998289\ttotal: 2m 49s\tremaining: 1m 46s\n",
            "2457:\tlearn: 0.9998289\ttotal: 2m 50s\tremaining: 1m 46s\n",
            "2458:\tlearn: 0.9998289\ttotal: 2m 50s\tremaining: 1m 46s\n",
            "2459:\tlearn: 0.9998289\ttotal: 2m 50s\tremaining: 1m 46s\n",
            "2460:\tlearn: 0.9998289\ttotal: 2m 50s\tremaining: 1m 46s\n",
            "2461:\tlearn: 0.9998289\ttotal: 2m 50s\tremaining: 1m 46s\n",
            "2462:\tlearn: 0.9998289\ttotal: 2m 50s\tremaining: 1m 46s\n",
            "2463:\tlearn: 0.9998289\ttotal: 2m 50s\tremaining: 1m 46s\n",
            "2464:\tlearn: 0.9998289\ttotal: 2m 50s\tremaining: 1m 46s\n",
            "2465:\tlearn: 0.9998289\ttotal: 2m 50s\tremaining: 1m 46s\n",
            "2466:\tlearn: 0.9998289\ttotal: 2m 50s\tremaining: 1m 45s\n",
            "2467:\tlearn: 0.9998289\ttotal: 2m 50s\tremaining: 1m 45s\n",
            "2468:\tlearn: 0.9998289\ttotal: 2m 50s\tremaining: 1m 45s\n",
            "2469:\tlearn: 0.9998221\ttotal: 2m 50s\tremaining: 1m 45s\n",
            "2470:\tlearn: 0.9998221\ttotal: 2m 50s\tremaining: 1m 45s\n",
            "2471:\tlearn: 0.9998221\ttotal: 2m 50s\tremaining: 1m 45s\n",
            "2472:\tlearn: 0.9998221\ttotal: 2m 50s\tremaining: 1m 45s\n",
            "2473:\tlearn: 0.9998221\ttotal: 2m 51s\tremaining: 1m 45s\n",
            "2474:\tlearn: 0.9998357\ttotal: 2m 51s\tremaining: 1m 45s\n",
            "2475:\tlearn: 0.9998357\ttotal: 2m 51s\tremaining: 1m 45s\n",
            "2476:\tlearn: 0.9998357\ttotal: 2m 51s\tremaining: 1m 45s\n",
            "2477:\tlearn: 0.9998357\ttotal: 2m 51s\tremaining: 1m 45s\n",
            "2478:\tlearn: 0.9998357\ttotal: 2m 51s\tremaining: 1m 45s\n",
            "2479:\tlearn: 0.9998289\ttotal: 2m 51s\tremaining: 1m 45s\n",
            "2480:\tlearn: 0.9998289\ttotal: 2m 51s\tremaining: 1m 45s\n",
            "2481:\tlearn: 0.9998289\ttotal: 2m 51s\tremaining: 1m 44s\n",
            "2482:\tlearn: 0.9998221\ttotal: 2m 51s\tremaining: 1m 44s\n",
            "2483:\tlearn: 0.9998221\ttotal: 2m 51s\tremaining: 1m 44s\n",
            "2484:\tlearn: 0.9998221\ttotal: 2m 51s\tremaining: 1m 44s\n",
            "2485:\tlearn: 0.9998221\ttotal: 2m 51s\tremaining: 1m 44s\n",
            "2486:\tlearn: 0.9998221\ttotal: 2m 51s\tremaining: 1m 44s\n",
            "2487:\tlearn: 0.9998221\ttotal: 2m 51s\tremaining: 1m 44s\n",
            "2488:\tlearn: 0.9998221\ttotal: 2m 51s\tremaining: 1m 44s\n",
            "2489:\tlearn: 0.9998221\ttotal: 2m 52s\tremaining: 1m 44s\n",
            "2490:\tlearn: 0.9998221\ttotal: 2m 52s\tremaining: 1m 44s\n",
            "2491:\tlearn: 0.9998221\ttotal: 2m 52s\tremaining: 1m 44s\n",
            "2492:\tlearn: 0.9998357\ttotal: 2m 52s\tremaining: 1m 44s\n",
            "2493:\tlearn: 0.9998357\ttotal: 2m 52s\tremaining: 1m 44s\n",
            "2494:\tlearn: 0.9998357\ttotal: 2m 52s\tremaining: 1m 43s\n",
            "2495:\tlearn: 0.9998357\ttotal: 2m 52s\tremaining: 1m 43s\n",
            "2496:\tlearn: 0.9998357\ttotal: 2m 52s\tremaining: 1m 43s\n",
            "2497:\tlearn: 0.9998357\ttotal: 2m 52s\tremaining: 1m 43s\n",
            "2498:\tlearn: 0.9998357\ttotal: 2m 52s\tremaining: 1m 43s\n",
            "2499:\tlearn: 0.9998357\ttotal: 2m 52s\tremaining: 1m 43s\n",
            "2500:\tlearn: 0.9998357\ttotal: 2m 52s\tremaining: 1m 43s\n",
            "2501:\tlearn: 0.9998357\ttotal: 2m 52s\tremaining: 1m 43s\n",
            "2502:\tlearn: 0.9998357\ttotal: 2m 52s\tremaining: 1m 43s\n",
            "2503:\tlearn: 0.9998357\ttotal: 2m 52s\tremaining: 1m 43s\n",
            "2504:\tlearn: 0.9998357\ttotal: 2m 53s\tremaining: 1m 43s\n",
            "2505:\tlearn: 0.9998357\ttotal: 2m 53s\tremaining: 1m 43s\n",
            "2506:\tlearn: 0.9998357\ttotal: 2m 53s\tremaining: 1m 43s\n",
            "2507:\tlearn: 0.9998357\ttotal: 2m 53s\tremaining: 1m 43s\n",
            "2508:\tlearn: 0.9998357\ttotal: 2m 53s\tremaining: 1m 42s\n",
            "2509:\tlearn: 0.9998289\ttotal: 2m 53s\tremaining: 1m 42s\n",
            "2510:\tlearn: 0.9998221\ttotal: 2m 53s\tremaining: 1m 42s\n",
            "2511:\tlearn: 0.9998221\ttotal: 2m 53s\tremaining: 1m 42s\n",
            "2512:\tlearn: 0.9998221\ttotal: 2m 53s\tremaining: 1m 42s\n",
            "2513:\tlearn: 0.9998221\ttotal: 2m 53s\tremaining: 1m 42s\n",
            "2514:\tlearn: 0.9998221\ttotal: 2m 53s\tremaining: 1m 42s\n",
            "2515:\tlearn: 0.9998221\ttotal: 2m 53s\tremaining: 1m 42s\n",
            "2516:\tlearn: 0.9998221\ttotal: 2m 53s\tremaining: 1m 42s\n",
            "2517:\tlearn: 0.9998221\ttotal: 2m 53s\tremaining: 1m 42s\n",
            "2518:\tlearn: 0.9998221\ttotal: 2m 54s\tremaining: 1m 42s\n",
            "2519:\tlearn: 0.9998221\ttotal: 2m 54s\tremaining: 1m 42s\n",
            "2520:\tlearn: 0.9998221\ttotal: 2m 54s\tremaining: 1m 42s\n",
            "2521:\tlearn: 0.9998221\ttotal: 2m 54s\tremaining: 1m 42s\n",
            "2522:\tlearn: 0.9998221\ttotal: 2m 54s\tremaining: 1m 42s\n",
            "2523:\tlearn: 0.9998221\ttotal: 2m 54s\tremaining: 1m 41s\n",
            "2524:\tlearn: 0.9998221\ttotal: 2m 54s\tremaining: 1m 41s\n",
            "2525:\tlearn: 0.9998221\ttotal: 2m 54s\tremaining: 1m 41s\n",
            "2526:\tlearn: 0.9998221\ttotal: 2m 54s\tremaining: 1m 41s\n",
            "2527:\tlearn: 0.9998221\ttotal: 2m 54s\tremaining: 1m 41s\n",
            "2528:\tlearn: 0.9998221\ttotal: 2m 54s\tremaining: 1m 41s\n",
            "2529:\tlearn: 0.9998289\ttotal: 2m 54s\tremaining: 1m 41s\n",
            "2530:\tlearn: 0.9998289\ttotal: 2m 54s\tremaining: 1m 41s\n",
            "2531:\tlearn: 0.9998289\ttotal: 2m 54s\tremaining: 1m 41s\n",
            "2532:\tlearn: 0.9998289\ttotal: 2m 54s\tremaining: 1m 41s\n",
            "2533:\tlearn: 0.9998289\ttotal: 2m 54s\tremaining: 1m 41s\n",
            "2534:\tlearn: 0.9998289\ttotal: 2m 55s\tremaining: 1m 41s\n",
            "2535:\tlearn: 0.9998289\ttotal: 2m 55s\tremaining: 1m 41s\n",
            "2536:\tlearn: 0.9998289\ttotal: 2m 55s\tremaining: 1m 41s\n",
            "2537:\tlearn: 0.9998289\ttotal: 2m 55s\tremaining: 1m 40s\n",
            "2538:\tlearn: 0.9998289\ttotal: 2m 55s\tremaining: 1m 40s\n",
            "2539:\tlearn: 0.9998289\ttotal: 2m 55s\tremaining: 1m 40s\n",
            "2540:\tlearn: 0.9998289\ttotal: 2m 55s\tremaining: 1m 40s\n",
            "2541:\tlearn: 0.9998289\ttotal: 2m 55s\tremaining: 1m 40s\n",
            "2542:\tlearn: 0.9998289\ttotal: 2m 55s\tremaining: 1m 40s\n",
            "2543:\tlearn: 0.9998289\ttotal: 2m 55s\tremaining: 1m 40s\n",
            "2544:\tlearn: 0.9998289\ttotal: 2m 55s\tremaining: 1m 40s\n",
            "2545:\tlearn: 0.9998289\ttotal: 2m 55s\tremaining: 1m 40s\n",
            "2546:\tlearn: 0.9998289\ttotal: 2m 55s\tremaining: 1m 40s\n",
            "2547:\tlearn: 0.9998289\ttotal: 2m 55s\tremaining: 1m 40s\n",
            "2548:\tlearn: 0.9998289\ttotal: 2m 55s\tremaining: 1m 40s\n",
            "2549:\tlearn: 0.9998289\ttotal: 2m 56s\tremaining: 1m 40s\n",
            "2550:\tlearn: 0.9998289\ttotal: 2m 56s\tremaining: 1m 40s\n",
            "2551:\tlearn: 0.9998289\ttotal: 2m 56s\tremaining: 1m 39s\n",
            "2552:\tlearn: 0.9998289\ttotal: 2m 56s\tremaining: 1m 39s\n",
            "2553:\tlearn: 0.9998289\ttotal: 2m 56s\tremaining: 1m 39s\n",
            "2554:\tlearn: 0.9998289\ttotal: 2m 56s\tremaining: 1m 39s\n",
            "2555:\tlearn: 0.9998289\ttotal: 2m 56s\tremaining: 1m 39s\n",
            "2556:\tlearn: 0.9998289\ttotal: 2m 56s\tremaining: 1m 39s\n",
            "2557:\tlearn: 0.9998289\ttotal: 2m 56s\tremaining: 1m 39s\n",
            "2558:\tlearn: 0.9998289\ttotal: 2m 56s\tremaining: 1m 39s\n",
            "2559:\tlearn: 0.9998289\ttotal: 2m 56s\tremaining: 1m 39s\n",
            "2560:\tlearn: 0.9998289\ttotal: 2m 56s\tremaining: 1m 39s\n",
            "2561:\tlearn: 0.9998289\ttotal: 2m 56s\tremaining: 1m 39s\n",
            "2562:\tlearn: 0.9998289\ttotal: 2m 56s\tremaining: 1m 39s\n",
            "2563:\tlearn: 0.9998289\ttotal: 2m 56s\tremaining: 1m 39s\n",
            "2564:\tlearn: 0.9998289\ttotal: 2m 56s\tremaining: 1m 39s\n",
            "2565:\tlearn: 0.9998289\ttotal: 2m 57s\tremaining: 1m 38s\n",
            "2566:\tlearn: 0.9998289\ttotal: 2m 57s\tremaining: 1m 38s\n",
            "2567:\tlearn: 0.9998289\ttotal: 2m 57s\tremaining: 1m 38s\n",
            "2568:\tlearn: 0.9998289\ttotal: 2m 57s\tremaining: 1m 38s\n",
            "2569:\tlearn: 0.9998289\ttotal: 2m 57s\tremaining: 1m 38s\n",
            "2570:\tlearn: 0.9998289\ttotal: 2m 57s\tremaining: 1m 38s\n",
            "2571:\tlearn: 0.9998289\ttotal: 2m 57s\tremaining: 1m 38s\n",
            "2572:\tlearn: 0.9998289\ttotal: 2m 57s\tremaining: 1m 38s\n",
            "2573:\tlearn: 0.9998357\ttotal: 2m 57s\tremaining: 1m 38s\n",
            "2574:\tlearn: 0.9998357\ttotal: 2m 57s\tremaining: 1m 38s\n",
            "2575:\tlearn: 0.9998357\ttotal: 2m 57s\tremaining: 1m 38s\n",
            "2576:\tlearn: 0.9998357\ttotal: 2m 57s\tremaining: 1m 38s\n",
            "2577:\tlearn: 0.9998357\ttotal: 2m 57s\tremaining: 1m 38s\n",
            "2578:\tlearn: 0.9998357\ttotal: 2m 57s\tremaining: 1m 38s\n",
            "2579:\tlearn: 0.9998357\ttotal: 2m 58s\tremaining: 1m 37s\n",
            "2580:\tlearn: 0.9998357\ttotal: 2m 58s\tremaining: 1m 37s\n",
            "2581:\tlearn: 0.9998357\ttotal: 2m 58s\tremaining: 1m 37s\n",
            "2582:\tlearn: 0.9998357\ttotal: 2m 58s\tremaining: 1m 37s\n",
            "2583:\tlearn: 0.9998357\ttotal: 2m 58s\tremaining: 1m 37s\n",
            "2584:\tlearn: 0.9998357\ttotal: 2m 58s\tremaining: 1m 37s\n",
            "2585:\tlearn: 0.9998357\ttotal: 2m 58s\tremaining: 1m 37s\n",
            "2586:\tlearn: 0.9998357\ttotal: 2m 58s\tremaining: 1m 37s\n",
            "2587:\tlearn: 0.9998357\ttotal: 2m 58s\tremaining: 1m 37s\n",
            "2588:\tlearn: 0.9998357\ttotal: 2m 58s\tremaining: 1m 37s\n",
            "2589:\tlearn: 0.9998357\ttotal: 2m 58s\tremaining: 1m 37s\n",
            "2590:\tlearn: 0.9998357\ttotal: 2m 58s\tremaining: 1m 37s\n",
            "2591:\tlearn: 0.9998357\ttotal: 2m 58s\tremaining: 1m 37s\n",
            "2592:\tlearn: 0.9998357\ttotal: 2m 58s\tremaining: 1m 37s\n",
            "2593:\tlearn: 0.9998357\ttotal: 2m 58s\tremaining: 1m 36s\n",
            "2594:\tlearn: 0.9998357\ttotal: 2m 58s\tremaining: 1m 36s\n",
            "2595:\tlearn: 0.9998357\ttotal: 2m 58s\tremaining: 1m 36s\n",
            "2596:\tlearn: 0.9998357\ttotal: 2m 59s\tremaining: 1m 36s\n",
            "2597:\tlearn: 0.9998357\ttotal: 2m 59s\tremaining: 1m 36s\n",
            "2598:\tlearn: 0.9998357\ttotal: 2m 59s\tremaining: 1m 36s\n",
            "2599:\tlearn: 0.9998357\ttotal: 2m 59s\tremaining: 1m 36s\n",
            "2600:\tlearn: 0.9998357\ttotal: 2m 59s\tremaining: 1m 36s\n",
            "2601:\tlearn: 0.9998357\ttotal: 2m 59s\tremaining: 1m 36s\n",
            "2602:\tlearn: 0.9998357\ttotal: 2m 59s\tremaining: 1m 36s\n",
            "2603:\tlearn: 0.9998357\ttotal: 2m 59s\tremaining: 1m 36s\n",
            "2604:\tlearn: 0.9998357\ttotal: 2m 59s\tremaining: 1m 36s\n",
            "2605:\tlearn: 0.9998357\ttotal: 2m 59s\tremaining: 1m 36s\n",
            "2606:\tlearn: 0.9998357\ttotal: 2m 59s\tremaining: 1m 36s\n",
            "2607:\tlearn: 0.9998357\ttotal: 2m 59s\tremaining: 1m 35s\n",
            "2608:\tlearn: 0.9998357\ttotal: 2m 59s\tremaining: 1m 35s\n",
            "2609:\tlearn: 0.9998357\ttotal: 2m 59s\tremaining: 1m 35s\n",
            "2610:\tlearn: 0.9998357\ttotal: 3m\tremaining: 1m 35s\n",
            "2611:\tlearn: 0.9998357\ttotal: 3m\tremaining: 1m 35s\n",
            "2612:\tlearn: 0.9998357\ttotal: 3m\tremaining: 1m 35s\n",
            "2613:\tlearn: 0.9998357\ttotal: 3m\tremaining: 1m 35s\n",
            "2614:\tlearn: 0.9998289\ttotal: 3m\tremaining: 1m 35s\n",
            "2615:\tlearn: 0.9998357\ttotal: 3m\tremaining: 1m 35s\n",
            "2616:\tlearn: 0.9998357\ttotal: 3m\tremaining: 1m 35s\n",
            "2617:\tlearn: 0.9998357\ttotal: 3m\tremaining: 1m 35s\n",
            "2618:\tlearn: 0.9998357\ttotal: 3m\tremaining: 1m 35s\n",
            "2619:\tlearn: 0.9998357\ttotal: 3m\tremaining: 1m 35s\n",
            "2620:\tlearn: 0.9998357\ttotal: 3m\tremaining: 1m 35s\n",
            "2621:\tlearn: 0.9998357\ttotal: 3m\tremaining: 1m 35s\n",
            "2622:\tlearn: 0.9998357\ttotal: 3m\tremaining: 1m 34s\n",
            "2623:\tlearn: 0.9998357\ttotal: 3m\tremaining: 1m 34s\n",
            "2624:\tlearn: 0.9998357\ttotal: 3m 1s\tremaining: 1m 34s\n",
            "2625:\tlearn: 0.9998357\ttotal: 3m 1s\tremaining: 1m 34s\n",
            "2626:\tlearn: 0.9998357\ttotal: 3m 1s\tremaining: 1m 34s\n",
            "2627:\tlearn: 0.9998357\ttotal: 3m 1s\tremaining: 1m 34s\n",
            "2628:\tlearn: 0.9998357\ttotal: 3m 1s\tremaining: 1m 34s\n",
            "2629:\tlearn: 0.9998357\ttotal: 3m 1s\tremaining: 1m 34s\n",
            "2630:\tlearn: 0.9998357\ttotal: 3m 1s\tremaining: 1m 34s\n",
            "2631:\tlearn: 0.9998357\ttotal: 3m 1s\tremaining: 1m 34s\n",
            "2632:\tlearn: 0.9998357\ttotal: 3m 1s\tremaining: 1m 34s\n",
            "2633:\tlearn: 0.9998357\ttotal: 3m 1s\tremaining: 1m 34s\n",
            "2634:\tlearn: 0.9998357\ttotal: 3m 1s\tremaining: 1m 34s\n",
            "2635:\tlearn: 0.9998357\ttotal: 3m 1s\tremaining: 1m 34s\n",
            "2636:\tlearn: 0.9998357\ttotal: 3m 1s\tremaining: 1m 33s\n",
            "2637:\tlearn: 0.9998426\ttotal: 3m 1s\tremaining: 1m 33s\n",
            "2638:\tlearn: 0.9998426\ttotal: 3m 2s\tremaining: 1m 33s\n",
            "2639:\tlearn: 0.9998426\ttotal: 3m 2s\tremaining: 1m 33s\n",
            "2640:\tlearn: 0.9998426\ttotal: 3m 2s\tremaining: 1m 33s\n",
            "2641:\tlearn: 0.9998426\ttotal: 3m 2s\tremaining: 1m 33s\n",
            "2642:\tlearn: 0.9998426\ttotal: 3m 2s\tremaining: 1m 33s\n",
            "2643:\tlearn: 0.9998426\ttotal: 3m 2s\tremaining: 1m 33s\n",
            "2644:\tlearn: 0.9998426\ttotal: 3m 2s\tremaining: 1m 33s\n",
            "2645:\tlearn: 0.9998426\ttotal: 3m 2s\tremaining: 1m 33s\n",
            "2646:\tlearn: 0.9998426\ttotal: 3m 2s\tremaining: 1m 33s\n",
            "2647:\tlearn: 0.9998426\ttotal: 3m 2s\tremaining: 1m 33s\n",
            "2648:\tlearn: 0.9998426\ttotal: 3m 2s\tremaining: 1m 33s\n",
            "2649:\tlearn: 0.9998426\ttotal: 3m 2s\tremaining: 1m 33s\n",
            "2650:\tlearn: 0.9998426\ttotal: 3m 2s\tremaining: 1m 33s\n",
            "2651:\tlearn: 0.9998426\ttotal: 3m 2s\tremaining: 1m 32s\n",
            "2652:\tlearn: 0.9998426\ttotal: 3m 3s\tremaining: 1m 32s\n",
            "2653:\tlearn: 0.9998426\ttotal: 3m 3s\tremaining: 1m 32s\n",
            "2654:\tlearn: 0.9998426\ttotal: 3m 3s\tremaining: 1m 32s\n",
            "2655:\tlearn: 0.9998426\ttotal: 3m 3s\tremaining: 1m 32s\n",
            "2656:\tlearn: 0.9998426\ttotal: 3m 3s\tremaining: 1m 32s\n",
            "2657:\tlearn: 0.9998426\ttotal: 3m 3s\tremaining: 1m 32s\n",
            "2658:\tlearn: 0.9998426\ttotal: 3m 3s\tremaining: 1m 32s\n",
            "2659:\tlearn: 0.9998426\ttotal: 3m 3s\tremaining: 1m 32s\n",
            "2660:\tlearn: 0.9998426\ttotal: 3m 3s\tremaining: 1m 32s\n",
            "2661:\tlearn: 0.9998426\ttotal: 3m 3s\tremaining: 1m 32s\n",
            "2662:\tlearn: 0.9998426\ttotal: 3m 3s\tremaining: 1m 32s\n",
            "2663:\tlearn: 0.9998426\ttotal: 3m 3s\tremaining: 1m 32s\n",
            "2664:\tlearn: 0.9998426\ttotal: 3m 3s\tremaining: 1m 32s\n",
            "2665:\tlearn: 0.9998426\ttotal: 3m 3s\tremaining: 1m 31s\n",
            "2666:\tlearn: 0.9998426\ttotal: 3m 3s\tremaining: 1m 31s\n",
            "2667:\tlearn: 0.9998426\ttotal: 3m 3s\tremaining: 1m 31s\n",
            "2668:\tlearn: 0.9998426\ttotal: 3m 4s\tremaining: 1m 31s\n",
            "2669:\tlearn: 0.9998426\ttotal: 3m 4s\tremaining: 1m 31s\n",
            "2670:\tlearn: 0.9998426\ttotal: 3m 4s\tremaining: 1m 31s\n",
            "2671:\tlearn: 0.9998426\ttotal: 3m 4s\tremaining: 1m 31s\n",
            "2672:\tlearn: 0.9998426\ttotal: 3m 4s\tremaining: 1m 31s\n",
            "2673:\tlearn: 0.9998426\ttotal: 3m 4s\tremaining: 1m 31s\n",
            "2674:\tlearn: 0.9998426\ttotal: 3m 4s\tremaining: 1m 31s\n",
            "2675:\tlearn: 0.9998426\ttotal: 3m 4s\tremaining: 1m 31s\n",
            "2676:\tlearn: 0.9998426\ttotal: 3m 4s\tremaining: 1m 31s\n",
            "2677:\tlearn: 0.9998426\ttotal: 3m 4s\tremaining: 1m 31s\n",
            "2678:\tlearn: 0.9998426\ttotal: 3m 4s\tremaining: 1m 31s\n",
            "2679:\tlearn: 0.9998426\ttotal: 3m 4s\tremaining: 1m 31s\n",
            "2680:\tlearn: 0.9998426\ttotal: 3m 4s\tremaining: 1m 31s\n",
            "2681:\tlearn: 0.9998426\ttotal: 3m 5s\tremaining: 1m 30s\n",
            "2682:\tlearn: 0.9998426\ttotal: 3m 5s\tremaining: 1m 30s\n",
            "2683:\tlearn: 0.9998426\ttotal: 3m 5s\tremaining: 1m 30s\n",
            "2684:\tlearn: 0.9998426\ttotal: 3m 5s\tremaining: 1m 30s\n",
            "2685:\tlearn: 0.9998426\ttotal: 3m 5s\tremaining: 1m 30s\n",
            "2686:\tlearn: 0.9998426\ttotal: 3m 5s\tremaining: 1m 30s\n",
            "2687:\tlearn: 0.9998426\ttotal: 3m 5s\tremaining: 1m 30s\n",
            "2688:\tlearn: 0.9998426\ttotal: 3m 5s\tremaining: 1m 30s\n",
            "2689:\tlearn: 0.9998426\ttotal: 3m 5s\tremaining: 1m 30s\n",
            "2690:\tlearn: 0.9998426\ttotal: 3m 5s\tremaining: 1m 30s\n",
            "2691:\tlearn: 0.9998426\ttotal: 3m 5s\tremaining: 1m 30s\n",
            "2692:\tlearn: 0.9998426\ttotal: 3m 5s\tremaining: 1m 30s\n",
            "2693:\tlearn: 0.9998426\ttotal: 3m 5s\tremaining: 1m 30s\n",
            "2694:\tlearn: 0.9998426\ttotal: 3m 5s\tremaining: 1m 30s\n",
            "2695:\tlearn: 0.9998426\ttotal: 3m 6s\tremaining: 1m 29s\n",
            "2696:\tlearn: 0.9998426\ttotal: 3m 6s\tremaining: 1m 29s\n",
            "2697:\tlearn: 0.9998426\ttotal: 3m 6s\tremaining: 1m 29s\n",
            "2698:\tlearn: 0.9998426\ttotal: 3m 6s\tremaining: 1m 29s\n",
            "2699:\tlearn: 0.9998426\ttotal: 3m 6s\tremaining: 1m 29s\n",
            "2700:\tlearn: 0.9998426\ttotal: 3m 6s\tremaining: 1m 29s\n",
            "2701:\tlearn: 0.9998426\ttotal: 3m 6s\tremaining: 1m 29s\n",
            "2702:\tlearn: 0.9998426\ttotal: 3m 6s\tremaining: 1m 29s\n",
            "2703:\tlearn: 0.9998426\ttotal: 3m 6s\tremaining: 1m 29s\n",
            "2704:\tlearn: 0.9998426\ttotal: 3m 6s\tremaining: 1m 29s\n",
            "2705:\tlearn: 0.9998426\ttotal: 3m 6s\tremaining: 1m 29s\n",
            "2706:\tlearn: 0.9998426\ttotal: 3m 6s\tremaining: 1m 29s\n",
            "2707:\tlearn: 0.9998426\ttotal: 3m 6s\tremaining: 1m 29s\n",
            "2708:\tlearn: 0.9998426\ttotal: 3m 6s\tremaining: 1m 29s\n",
            "2709:\tlearn: 0.9998426\ttotal: 3m 6s\tremaining: 1m 28s\n",
            "2710:\tlearn: 0.9998426\ttotal: 3m 6s\tremaining: 1m 28s\n",
            "2711:\tlearn: 0.9998426\ttotal: 3m 6s\tremaining: 1m 28s\n",
            "2712:\tlearn: 0.9998426\ttotal: 3m 7s\tremaining: 1m 28s\n",
            "2713:\tlearn: 0.9998426\ttotal: 3m 7s\tremaining: 1m 28s\n",
            "2714:\tlearn: 0.9998426\ttotal: 3m 7s\tremaining: 1m 28s\n",
            "2715:\tlearn: 0.9998426\ttotal: 3m 7s\tremaining: 1m 28s\n",
            "2716:\tlearn: 0.9998426\ttotal: 3m 7s\tremaining: 1m 28s\n",
            "2717:\tlearn: 0.9998426\ttotal: 3m 7s\tremaining: 1m 28s\n",
            "2718:\tlearn: 0.9998426\ttotal: 3m 7s\tremaining: 1m 28s\n",
            "2719:\tlearn: 0.9998426\ttotal: 3m 7s\tremaining: 1m 28s\n",
            "2720:\tlearn: 0.9998426\ttotal: 3m 7s\tremaining: 1m 28s\n",
            "2721:\tlearn: 0.9998426\ttotal: 3m 7s\tremaining: 1m 28s\n",
            "2722:\tlearn: 0.9998426\ttotal: 3m 7s\tremaining: 1m 28s\n",
            "2723:\tlearn: 0.9998426\ttotal: 3m 7s\tremaining: 1m 27s\n",
            "2724:\tlearn: 0.9998426\ttotal: 3m 7s\tremaining: 1m 27s\n",
            "2725:\tlearn: 0.9998426\ttotal: 3m 7s\tremaining: 1m 27s\n",
            "2726:\tlearn: 0.9998426\ttotal: 3m 7s\tremaining: 1m 27s\n",
            "2727:\tlearn: 0.9998426\ttotal: 3m 7s\tremaining: 1m 27s\n",
            "2728:\tlearn: 0.9998426\ttotal: 3m 8s\tremaining: 1m 27s\n",
            "2729:\tlearn: 0.9998426\ttotal: 3m 8s\tremaining: 1m 27s\n",
            "2730:\tlearn: 0.9998426\ttotal: 3m 8s\tremaining: 1m 27s\n",
            "2731:\tlearn: 0.9998426\ttotal: 3m 8s\tremaining: 1m 27s\n",
            "2732:\tlearn: 0.9998426\ttotal: 3m 8s\tremaining: 1m 27s\n",
            "2733:\tlearn: 0.9998426\ttotal: 3m 8s\tremaining: 1m 27s\n",
            "2734:\tlearn: 0.9998426\ttotal: 3m 8s\tremaining: 1m 27s\n",
            "2735:\tlearn: 0.9998426\ttotal: 3m 8s\tremaining: 1m 27s\n",
            "2736:\tlearn: 0.9998426\ttotal: 3m 8s\tremaining: 1m 27s\n",
            "2737:\tlearn: 0.9998426\ttotal: 3m 8s\tremaining: 1m 26s\n",
            "2738:\tlearn: 0.9998426\ttotal: 3m 8s\tremaining: 1m 26s\n",
            "2739:\tlearn: 0.9998426\ttotal: 3m 8s\tremaining: 1m 26s\n",
            "2740:\tlearn: 0.9998426\ttotal: 3m 8s\tremaining: 1m 26s\n",
            "2741:\tlearn: 0.9998426\ttotal: 3m 8s\tremaining: 1m 26s\n",
            "2742:\tlearn: 0.9998426\ttotal: 3m 9s\tremaining: 1m 26s\n",
            "2743:\tlearn: 0.9998426\ttotal: 3m 9s\tremaining: 1m 26s\n",
            "2744:\tlearn: 0.9998426\ttotal: 3m 9s\tremaining: 1m 26s\n",
            "2745:\tlearn: 0.9998426\ttotal: 3m 9s\tremaining: 1m 26s\n",
            "2746:\tlearn: 0.9998426\ttotal: 3m 9s\tremaining: 1m 26s\n",
            "2747:\tlearn: 0.9998426\ttotal: 3m 9s\tremaining: 1m 26s\n",
            "2748:\tlearn: 0.9998426\ttotal: 3m 9s\tremaining: 1m 26s\n",
            "2749:\tlearn: 0.9998426\ttotal: 3m 9s\tremaining: 1m 26s\n",
            "2750:\tlearn: 0.9998426\ttotal: 3m 9s\tremaining: 1m 26s\n",
            "2751:\tlearn: 0.9998426\ttotal: 3m 9s\tremaining: 1m 26s\n",
            "2752:\tlearn: 0.9998426\ttotal: 3m 9s\tremaining: 1m 25s\n",
            "2753:\tlearn: 0.9998426\ttotal: 3m 9s\tremaining: 1m 25s\n",
            "2754:\tlearn: 0.9998426\ttotal: 3m 9s\tremaining: 1m 25s\n",
            "2755:\tlearn: 0.9998426\ttotal: 3m 9s\tremaining: 1m 25s\n",
            "2756:\tlearn: 0.9998426\ttotal: 3m 10s\tremaining: 1m 25s\n",
            "2757:\tlearn: 0.9998426\ttotal: 3m 10s\tremaining: 1m 25s\n",
            "2758:\tlearn: 0.9998426\ttotal: 3m 10s\tremaining: 1m 25s\n",
            "2759:\tlearn: 0.9998426\ttotal: 3m 10s\tremaining: 1m 25s\n",
            "2760:\tlearn: 0.9998426\ttotal: 3m 10s\tremaining: 1m 25s\n",
            "2761:\tlearn: 0.9998426\ttotal: 3m 10s\tremaining: 1m 25s\n",
            "2762:\tlearn: 0.9998426\ttotal: 3m 10s\tremaining: 1m 25s\n",
            "2763:\tlearn: 0.9998426\ttotal: 3m 10s\tremaining: 1m 25s\n",
            "2764:\tlearn: 0.9998426\ttotal: 3m 10s\tremaining: 1m 25s\n",
            "2765:\tlearn: 0.9998426\ttotal: 3m 10s\tremaining: 1m 25s\n",
            "2766:\tlearn: 0.9998426\ttotal: 3m 10s\tremaining: 1m 25s\n",
            "2767:\tlearn: 0.9998426\ttotal: 3m 10s\tremaining: 1m 24s\n",
            "2768:\tlearn: 0.9998426\ttotal: 3m 10s\tremaining: 1m 24s\n",
            "2769:\tlearn: 0.9998426\ttotal: 3m 10s\tremaining: 1m 24s\n",
            "2770:\tlearn: 0.9998426\ttotal: 3m 11s\tremaining: 1m 24s\n",
            "2771:\tlearn: 0.9998426\ttotal: 3m 11s\tremaining: 1m 24s\n",
            "2772:\tlearn: 0.9998426\ttotal: 3m 11s\tremaining: 1m 24s\n",
            "2773:\tlearn: 0.9998426\ttotal: 3m 11s\tremaining: 1m 24s\n",
            "2774:\tlearn: 0.9998426\ttotal: 3m 11s\tremaining: 1m 24s\n",
            "2775:\tlearn: 0.9998426\ttotal: 3m 11s\tremaining: 1m 24s\n",
            "2776:\tlearn: 0.9998426\ttotal: 3m 11s\tremaining: 1m 24s\n",
            "2777:\tlearn: 0.9998426\ttotal: 3m 11s\tremaining: 1m 24s\n",
            "2778:\tlearn: 0.9998426\ttotal: 3m 11s\tremaining: 1m 24s\n",
            "2779:\tlearn: 0.9998426\ttotal: 3m 11s\tremaining: 1m 24s\n",
            "2780:\tlearn: 0.9998426\ttotal: 3m 11s\tremaining: 1m 24s\n",
            "2781:\tlearn: 0.9998426\ttotal: 3m 11s\tremaining: 1m 23s\n",
            "2782:\tlearn: 0.9998426\ttotal: 3m 11s\tremaining: 1m 23s\n",
            "2783:\tlearn: 0.9998426\ttotal: 3m 11s\tremaining: 1m 23s\n",
            "2784:\tlearn: 0.9998426\ttotal: 3m 12s\tremaining: 1m 23s\n",
            "2785:\tlearn: 0.9998426\ttotal: 3m 12s\tremaining: 1m 23s\n",
            "2786:\tlearn: 0.9998426\ttotal: 3m 12s\tremaining: 1m 23s\n",
            "2787:\tlearn: 0.9998426\ttotal: 3m 12s\tremaining: 1m 23s\n",
            "2788:\tlearn: 0.9998426\ttotal: 3m 12s\tremaining: 1m 23s\n",
            "2789:\tlearn: 0.9998426\ttotal: 3m 12s\tremaining: 1m 23s\n",
            "2790:\tlearn: 0.9998426\ttotal: 3m 12s\tremaining: 1m 23s\n",
            "2791:\tlearn: 0.9998426\ttotal: 3m 12s\tremaining: 1m 23s\n",
            "2792:\tlearn: 0.9998426\ttotal: 3m 12s\tremaining: 1m 23s\n",
            "2793:\tlearn: 0.9998426\ttotal: 3m 12s\tremaining: 1m 23s\n",
            "2794:\tlearn: 0.9998426\ttotal: 3m 12s\tremaining: 1m 23s\n",
            "2795:\tlearn: 0.9998426\ttotal: 3m 12s\tremaining: 1m 23s\n",
            "2796:\tlearn: 0.9998426\ttotal: 3m 12s\tremaining: 1m 22s\n",
            "2797:\tlearn: 0.9998426\ttotal: 3m 12s\tremaining: 1m 22s\n",
            "2798:\tlearn: 0.9998426\ttotal: 3m 13s\tremaining: 1m 22s\n",
            "2799:\tlearn: 0.9998426\ttotal: 3m 13s\tremaining: 1m 22s\n",
            "2800:\tlearn: 0.9998426\ttotal: 3m 13s\tremaining: 1m 22s\n",
            "2801:\tlearn: 0.9998426\ttotal: 3m 13s\tremaining: 1m 22s\n",
            "2802:\tlearn: 0.9998426\ttotal: 3m 13s\tremaining: 1m 22s\n",
            "2803:\tlearn: 0.9998426\ttotal: 3m 13s\tremaining: 1m 22s\n",
            "2804:\tlearn: 0.9998426\ttotal: 3m 13s\tremaining: 1m 22s\n",
            "2805:\tlearn: 0.9998426\ttotal: 3m 13s\tremaining: 1m 22s\n",
            "2806:\tlearn: 0.9998426\ttotal: 3m 13s\tremaining: 1m 22s\n",
            "2807:\tlearn: 0.9998426\ttotal: 3m 13s\tremaining: 1m 22s\n",
            "2808:\tlearn: 0.9998426\ttotal: 3m 13s\tremaining: 1m 22s\n",
            "2809:\tlearn: 0.9998426\ttotal: 3m 13s\tremaining: 1m 22s\n",
            "2810:\tlearn: 0.9998426\ttotal: 3m 13s\tremaining: 1m 22s\n",
            "2811:\tlearn: 0.9998426\ttotal: 3m 14s\tremaining: 1m 21s\n",
            "2812:\tlearn: 0.9998426\ttotal: 3m 14s\tremaining: 1m 21s\n",
            "2813:\tlearn: 0.9998426\ttotal: 3m 14s\tremaining: 1m 21s\n",
            "2814:\tlearn: 0.9998426\ttotal: 3m 14s\tremaining: 1m 21s\n",
            "2815:\tlearn: 0.9998426\ttotal: 3m 14s\tremaining: 1m 21s\n",
            "2816:\tlearn: 0.9998426\ttotal: 3m 14s\tremaining: 1m 21s\n",
            "2817:\tlearn: 0.9998426\ttotal: 3m 14s\tremaining: 1m 21s\n",
            "2818:\tlearn: 0.9998426\ttotal: 3m 14s\tremaining: 1m 21s\n",
            "2819:\tlearn: 0.9998426\ttotal: 3m 14s\tremaining: 1m 21s\n",
            "2820:\tlearn: 0.9998426\ttotal: 3m 14s\tremaining: 1m 21s\n",
            "2821:\tlearn: 0.9998426\ttotal: 3m 14s\tremaining: 1m 21s\n",
            "2822:\tlearn: 0.9998426\ttotal: 3m 14s\tremaining: 1m 21s\n",
            "2823:\tlearn: 0.9998426\ttotal: 3m 14s\tremaining: 1m 21s\n",
            "2824:\tlearn: 0.9998426\ttotal: 3m 14s\tremaining: 1m 21s\n",
            "2825:\tlearn: 0.9998426\ttotal: 3m 14s\tremaining: 1m 20s\n",
            "2826:\tlearn: 0.9998426\ttotal: 3m 14s\tremaining: 1m 20s\n",
            "2827:\tlearn: 0.9998426\ttotal: 3m 15s\tremaining: 1m 20s\n",
            "2828:\tlearn: 0.9998426\ttotal: 3m 15s\tremaining: 1m 20s\n",
            "2829:\tlearn: 0.9998426\ttotal: 3m 15s\tremaining: 1m 20s\n",
            "2830:\tlearn: 0.9998426\ttotal: 3m 15s\tremaining: 1m 20s\n",
            "2831:\tlearn: 0.9998426\ttotal: 3m 15s\tremaining: 1m 20s\n",
            "2832:\tlearn: 0.9998426\ttotal: 3m 15s\tremaining: 1m 20s\n",
            "2833:\tlearn: 0.9998426\ttotal: 3m 15s\tremaining: 1m 20s\n",
            "2834:\tlearn: 0.9998426\ttotal: 3m 15s\tremaining: 1m 20s\n",
            "2835:\tlearn: 0.9998426\ttotal: 3m 15s\tremaining: 1m 20s\n",
            "2836:\tlearn: 0.9998426\ttotal: 3m 15s\tremaining: 1m 20s\n",
            "2837:\tlearn: 0.9998426\ttotal: 3m 15s\tremaining: 1m 20s\n",
            "2838:\tlearn: 0.9998426\ttotal: 3m 15s\tremaining: 1m 20s\n",
            "2839:\tlearn: 0.9998426\ttotal: 3m 15s\tremaining: 1m 19s\n",
            "2840:\tlearn: 0.9998426\ttotal: 3m 15s\tremaining: 1m 19s\n",
            "2841:\tlearn: 0.9998426\ttotal: 3m 15s\tremaining: 1m 19s\n",
            "2842:\tlearn: 0.9998426\ttotal: 3m 15s\tremaining: 1m 19s\n",
            "2843:\tlearn: 0.9998426\ttotal: 3m 16s\tremaining: 1m 19s\n",
            "2844:\tlearn: 0.9998426\ttotal: 3m 16s\tremaining: 1m 19s\n",
            "2845:\tlearn: 0.9998426\ttotal: 3m 16s\tremaining: 1m 19s\n",
            "2846:\tlearn: 0.9998426\ttotal: 3m 16s\tremaining: 1m 19s\n",
            "2847:\tlearn: 0.9998426\ttotal: 3m 16s\tremaining: 1m 19s\n",
            "2848:\tlearn: 0.9998426\ttotal: 3m 16s\tremaining: 1m 19s\n",
            "2849:\tlearn: 0.9998426\ttotal: 3m 16s\tremaining: 1m 19s\n",
            "2850:\tlearn: 0.9998426\ttotal: 3m 16s\tremaining: 1m 19s\n",
            "2851:\tlearn: 0.9998426\ttotal: 3m 16s\tremaining: 1m 19s\n",
            "2852:\tlearn: 0.9998426\ttotal: 3m 16s\tremaining: 1m 19s\n",
            "2853:\tlearn: 0.9998426\ttotal: 3m 16s\tremaining: 1m 18s\n",
            "2854:\tlearn: 0.9998426\ttotal: 3m 16s\tremaining: 1m 18s\n",
            "2855:\tlearn: 0.9998426\ttotal: 3m 16s\tremaining: 1m 18s\n",
            "2856:\tlearn: 0.9998426\ttotal: 3m 16s\tremaining: 1m 18s\n",
            "2857:\tlearn: 0.9998426\ttotal: 3m 16s\tremaining: 1m 18s\n",
            "2858:\tlearn: 0.9998426\ttotal: 3m 16s\tremaining: 1m 18s\n",
            "2859:\tlearn: 0.9998426\ttotal: 3m 17s\tremaining: 1m 18s\n",
            "2860:\tlearn: 0.9998494\ttotal: 3m 17s\tremaining: 1m 18s\n",
            "2861:\tlearn: 0.9998494\ttotal: 3m 17s\tremaining: 1m 18s\n",
            "2862:\tlearn: 0.9998494\ttotal: 3m 17s\tremaining: 1m 18s\n",
            "2863:\tlearn: 0.9998494\ttotal: 3m 17s\tremaining: 1m 18s\n",
            "2864:\tlearn: 0.9998494\ttotal: 3m 17s\tremaining: 1m 18s\n",
            "2865:\tlearn: 0.9998494\ttotal: 3m 17s\tremaining: 1m 18s\n",
            "2866:\tlearn: 0.9998494\ttotal: 3m 17s\tremaining: 1m 18s\n",
            "2867:\tlearn: 0.9998494\ttotal: 3m 17s\tremaining: 1m 17s\n",
            "2868:\tlearn: 0.9998494\ttotal: 3m 17s\tremaining: 1m 17s\n",
            "2869:\tlearn: 0.9998494\ttotal: 3m 17s\tremaining: 1m 17s\n",
            "2870:\tlearn: 0.9998494\ttotal: 3m 17s\tremaining: 1m 17s\n",
            "2871:\tlearn: 0.9998494\ttotal: 3m 17s\tremaining: 1m 17s\n",
            "2872:\tlearn: 0.9998494\ttotal: 3m 17s\tremaining: 1m 17s\n",
            "2873:\tlearn: 0.9998494\ttotal: 3m 18s\tremaining: 1m 17s\n",
            "2874:\tlearn: 0.9998494\ttotal: 3m 18s\tremaining: 1m 17s\n",
            "2875:\tlearn: 0.9998494\ttotal: 3m 18s\tremaining: 1m 17s\n",
            "2876:\tlearn: 0.9998494\ttotal: 3m 18s\tremaining: 1m 17s\n",
            "2877:\tlearn: 0.9998494\ttotal: 3m 18s\tremaining: 1m 17s\n",
            "2878:\tlearn: 0.9998494\ttotal: 3m 18s\tremaining: 1m 17s\n",
            "2879:\tlearn: 0.9998494\ttotal: 3m 18s\tremaining: 1m 17s\n",
            "2880:\tlearn: 0.9998494\ttotal: 3m 18s\tremaining: 1m 17s\n",
            "2881:\tlearn: 0.9998494\ttotal: 3m 18s\tremaining: 1m 17s\n",
            "2882:\tlearn: 0.9998494\ttotal: 3m 18s\tremaining: 1m 16s\n",
            "2883:\tlearn: 0.9998494\ttotal: 3m 18s\tremaining: 1m 16s\n",
            "2884:\tlearn: 0.9998494\ttotal: 3m 18s\tremaining: 1m 16s\n",
            "2885:\tlearn: 0.9998494\ttotal: 3m 18s\tremaining: 1m 16s\n",
            "2886:\tlearn: 0.9998494\ttotal: 3m 18s\tremaining: 1m 16s\n",
            "2887:\tlearn: 0.9998494\ttotal: 3m 18s\tremaining: 1m 16s\n",
            "2888:\tlearn: 0.9998494\ttotal: 3m 19s\tremaining: 1m 16s\n",
            "2889:\tlearn: 0.9998494\ttotal: 3m 19s\tremaining: 1m 16s\n",
            "2890:\tlearn: 0.9998494\ttotal: 3m 19s\tremaining: 1m 16s\n",
            "2891:\tlearn: 0.9998494\ttotal: 3m 19s\tremaining: 1m 16s\n",
            "2892:\tlearn: 0.9998494\ttotal: 3m 19s\tremaining: 1m 16s\n",
            "2893:\tlearn: 0.9998494\ttotal: 3m 19s\tremaining: 1m 16s\n",
            "2894:\tlearn: 0.9998494\ttotal: 3m 19s\tremaining: 1m 16s\n",
            "2895:\tlearn: 0.9998494\ttotal: 3m 19s\tremaining: 1m 16s\n",
            "2896:\tlearn: 0.9998494\ttotal: 3m 19s\tremaining: 1m 16s\n",
            "2897:\tlearn: 0.9998494\ttotal: 3m 19s\tremaining: 1m 15s\n",
            "2898:\tlearn: 0.9998494\ttotal: 3m 19s\tremaining: 1m 15s\n",
            "2899:\tlearn: 0.9998494\ttotal: 3m 19s\tremaining: 1m 15s\n",
            "2900:\tlearn: 0.9998494\ttotal: 3m 19s\tremaining: 1m 15s\n",
            "2901:\tlearn: 0.9998494\ttotal: 3m 20s\tremaining: 1m 15s\n",
            "2902:\tlearn: 0.9998494\ttotal: 3m 20s\tremaining: 1m 15s\n",
            "2903:\tlearn: 0.9998494\ttotal: 3m 20s\tremaining: 1m 15s\n",
            "2904:\tlearn: 0.9998494\ttotal: 3m 20s\tremaining: 1m 15s\n",
            "2905:\tlearn: 0.9998494\ttotal: 3m 20s\tremaining: 1m 15s\n",
            "2906:\tlearn: 0.9998494\ttotal: 3m 20s\tremaining: 1m 15s\n",
            "2907:\tlearn: 0.9998494\ttotal: 3m 20s\tremaining: 1m 15s\n",
            "2908:\tlearn: 0.9998494\ttotal: 3m 20s\tremaining: 1m 15s\n",
            "2909:\tlearn: 0.9998494\ttotal: 3m 20s\tremaining: 1m 15s\n",
            "2910:\tlearn: 0.9998494\ttotal: 3m 20s\tremaining: 1m 15s\n",
            "2911:\tlearn: 0.9998494\ttotal: 3m 20s\tremaining: 1m 15s\n",
            "2912:\tlearn: 0.9998494\ttotal: 3m 20s\tremaining: 1m 14s\n",
            "2913:\tlearn: 0.9998494\ttotal: 3m 20s\tremaining: 1m 14s\n",
            "2914:\tlearn: 0.9998494\ttotal: 3m 21s\tremaining: 1m 14s\n",
            "2915:\tlearn: 0.9998494\ttotal: 3m 21s\tremaining: 1m 14s\n",
            "2916:\tlearn: 0.9998494\ttotal: 3m 21s\tremaining: 1m 14s\n",
            "2917:\tlearn: 0.9998494\ttotal: 3m 21s\tremaining: 1m 14s\n",
            "2918:\tlearn: 0.9998494\ttotal: 3m 21s\tremaining: 1m 14s\n",
            "2919:\tlearn: 0.9998494\ttotal: 3m 21s\tremaining: 1m 14s\n",
            "2920:\tlearn: 0.9998494\ttotal: 3m 21s\tremaining: 1m 14s\n",
            "2921:\tlearn: 0.9998494\ttotal: 3m 21s\tremaining: 1m 14s\n",
            "2922:\tlearn: 0.9998494\ttotal: 3m 21s\tremaining: 1m 14s\n",
            "2923:\tlearn: 0.9998494\ttotal: 3m 21s\tremaining: 1m 14s\n",
            "2924:\tlearn: 0.9998494\ttotal: 3m 21s\tremaining: 1m 14s\n",
            "2925:\tlearn: 0.9998494\ttotal: 3m 21s\tremaining: 1m 14s\n",
            "2926:\tlearn: 0.9998494\ttotal: 3m 21s\tremaining: 1m 13s\n",
            "2927:\tlearn: 0.9998494\ttotal: 3m 21s\tremaining: 1m 13s\n",
            "2928:\tlearn: 0.9998494\ttotal: 3m 21s\tremaining: 1m 13s\n",
            "2929:\tlearn: 0.9998494\ttotal: 3m 21s\tremaining: 1m 13s\n",
            "2930:\tlearn: 0.9998494\ttotal: 3m 22s\tremaining: 1m 13s\n",
            "2931:\tlearn: 0.9998494\ttotal: 3m 22s\tremaining: 1m 13s\n",
            "2932:\tlearn: 0.9998494\ttotal: 3m 22s\tremaining: 1m 13s\n",
            "2933:\tlearn: 0.9998494\ttotal: 3m 22s\tremaining: 1m 13s\n",
            "2934:\tlearn: 0.9998494\ttotal: 3m 22s\tremaining: 1m 13s\n",
            "2935:\tlearn: 0.9998494\ttotal: 3m 22s\tremaining: 1m 13s\n",
            "2936:\tlearn: 0.9998494\ttotal: 3m 22s\tremaining: 1m 13s\n",
            "2937:\tlearn: 0.9998494\ttotal: 3m 22s\tremaining: 1m 13s\n",
            "2938:\tlearn: 0.9998494\ttotal: 3m 22s\tremaining: 1m 13s\n",
            "2939:\tlearn: 0.9998494\ttotal: 3m 22s\tremaining: 1m 13s\n",
            "2940:\tlearn: 0.9998494\ttotal: 3m 22s\tremaining: 1m 13s\n",
            "2941:\tlearn: 0.9998494\ttotal: 3m 22s\tremaining: 1m 12s\n",
            "2942:\tlearn: 0.9998494\ttotal: 3m 22s\tremaining: 1m 12s\n",
            "2943:\tlearn: 0.9998494\ttotal: 3m 23s\tremaining: 1m 12s\n",
            "2944:\tlearn: 0.9998494\ttotal: 3m 23s\tremaining: 1m 12s\n",
            "2945:\tlearn: 0.9998494\ttotal: 3m 23s\tremaining: 1m 12s\n",
            "2946:\tlearn: 0.9998494\ttotal: 3m 23s\tremaining: 1m 12s\n",
            "2947:\tlearn: 0.9998494\ttotal: 3m 23s\tremaining: 1m 12s\n",
            "2948:\tlearn: 0.9998494\ttotal: 3m 23s\tremaining: 1m 12s\n",
            "2949:\tlearn: 0.9998494\ttotal: 3m 23s\tremaining: 1m 12s\n",
            "2950:\tlearn: 0.9998494\ttotal: 3m 23s\tremaining: 1m 12s\n",
            "2951:\tlearn: 0.9998494\ttotal: 3m 23s\tremaining: 1m 12s\n",
            "2952:\tlearn: 0.9998494\ttotal: 3m 23s\tremaining: 1m 12s\n",
            "2953:\tlearn: 0.9998494\ttotal: 3m 23s\tremaining: 1m 12s\n",
            "2954:\tlearn: 0.9998494\ttotal: 3m 24s\tremaining: 1m 12s\n",
            "2955:\tlearn: 0.9998494\ttotal: 3m 24s\tremaining: 1m 12s\n",
            "2956:\tlearn: 0.9998494\ttotal: 3m 24s\tremaining: 1m 12s\n",
            "2957:\tlearn: 0.9998494\ttotal: 3m 24s\tremaining: 1m 11s\n",
            "2958:\tlearn: 0.9998494\ttotal: 3m 24s\tremaining: 1m 11s\n",
            "2959:\tlearn: 0.9998494\ttotal: 3m 24s\tremaining: 1m 11s\n",
            "2960:\tlearn: 0.9998494\ttotal: 3m 24s\tremaining: 1m 11s\n",
            "2961:\tlearn: 0.9998494\ttotal: 3m 24s\tremaining: 1m 11s\n",
            "2962:\tlearn: 0.9998494\ttotal: 3m 24s\tremaining: 1m 11s\n",
            "2963:\tlearn: 0.9998494\ttotal: 3m 24s\tremaining: 1m 11s\n",
            "2964:\tlearn: 0.9998494\ttotal: 3m 24s\tremaining: 1m 11s\n",
            "2965:\tlearn: 0.9998494\ttotal: 3m 24s\tremaining: 1m 11s\n",
            "2966:\tlearn: 0.9998494\ttotal: 3m 24s\tremaining: 1m 11s\n",
            "2967:\tlearn: 0.9998494\ttotal: 3m 25s\tremaining: 1m 11s\n",
            "2968:\tlearn: 0.9998494\ttotal: 3m 25s\tremaining: 1m 11s\n",
            "2969:\tlearn: 0.9998494\ttotal: 3m 25s\tremaining: 1m 11s\n",
            "2970:\tlearn: 0.9998494\ttotal: 3m 25s\tremaining: 1m 11s\n",
            "2971:\tlearn: 0.9998494\ttotal: 3m 25s\tremaining: 1m 11s\n",
            "2972:\tlearn: 0.9998494\ttotal: 3m 25s\tremaining: 1m 10s\n",
            "2973:\tlearn: 0.9998494\ttotal: 3m 25s\tremaining: 1m 10s\n",
            "2974:\tlearn: 0.9998494\ttotal: 3m 25s\tremaining: 1m 10s\n",
            "2975:\tlearn: 0.9998494\ttotal: 3m 25s\tremaining: 1m 10s\n",
            "2976:\tlearn: 0.9998494\ttotal: 3m 25s\tremaining: 1m 10s\n",
            "2977:\tlearn: 0.9998494\ttotal: 3m 25s\tremaining: 1m 10s\n",
            "2978:\tlearn: 0.9998494\ttotal: 3m 25s\tremaining: 1m 10s\n",
            "2979:\tlearn: 0.9998494\ttotal: 3m 25s\tremaining: 1m 10s\n",
            "2980:\tlearn: 0.9998494\ttotal: 3m 26s\tremaining: 1m 10s\n",
            "2981:\tlearn: 0.9998494\ttotal: 3m 26s\tremaining: 1m 10s\n",
            "2982:\tlearn: 0.9998494\ttotal: 3m 26s\tremaining: 1m 10s\n",
            "2983:\tlearn: 0.9998494\ttotal: 3m 26s\tremaining: 1m 10s\n",
            "2984:\tlearn: 0.9998494\ttotal: 3m 26s\tremaining: 1m 10s\n",
            "2985:\tlearn: 0.9998494\ttotal: 3m 26s\tremaining: 1m 10s\n",
            "2986:\tlearn: 0.9998494\ttotal: 3m 26s\tremaining: 1m 10s\n",
            "2987:\tlearn: 0.9998494\ttotal: 3m 26s\tremaining: 1m 9s\n",
            "2988:\tlearn: 0.9998494\ttotal: 3m 26s\tremaining: 1m 9s\n",
            "2989:\tlearn: 0.9998494\ttotal: 3m 26s\tremaining: 1m 9s\n",
            "2990:\tlearn: 0.9998494\ttotal: 3m 26s\tremaining: 1m 9s\n",
            "2991:\tlearn: 0.9998494\ttotal: 3m 26s\tremaining: 1m 9s\n",
            "2992:\tlearn: 0.9998494\ttotal: 3m 26s\tremaining: 1m 9s\n",
            "2993:\tlearn: 0.9998494\ttotal: 3m 27s\tremaining: 1m 9s\n",
            "2994:\tlearn: 0.9998494\ttotal: 3m 27s\tremaining: 1m 9s\n",
            "2995:\tlearn: 0.9998494\ttotal: 3m 27s\tremaining: 1m 9s\n",
            "2996:\tlearn: 0.9998494\ttotal: 3m 27s\tremaining: 1m 9s\n",
            "2997:\tlearn: 0.9998494\ttotal: 3m 27s\tremaining: 1m 9s\n",
            "2998:\tlearn: 0.9998494\ttotal: 3m 27s\tremaining: 1m 9s\n",
            "2999:\tlearn: 0.9998494\ttotal: 3m 27s\tremaining: 1m 9s\n",
            "3000:\tlearn: 0.9998494\ttotal: 3m 27s\tremaining: 1m 9s\n",
            "3001:\tlearn: 0.9998494\ttotal: 3m 27s\tremaining: 1m 9s\n",
            "3002:\tlearn: 0.9998494\ttotal: 3m 27s\tremaining: 1m 8s\n",
            "3003:\tlearn: 0.9998494\ttotal: 3m 27s\tremaining: 1m 8s\n",
            "3004:\tlearn: 0.9998494\ttotal: 3m 27s\tremaining: 1m 8s\n",
            "3005:\tlearn: 0.9998494\ttotal: 3m 27s\tremaining: 1m 8s\n",
            "3006:\tlearn: 0.9998494\ttotal: 3m 28s\tremaining: 1m 8s\n",
            "3007:\tlearn: 0.9998494\ttotal: 3m 28s\tremaining: 1m 8s\n",
            "3008:\tlearn: 0.9998494\ttotal: 3m 28s\tremaining: 1m 8s\n",
            "3009:\tlearn: 0.9998494\ttotal: 3m 28s\tremaining: 1m 8s\n",
            "3010:\tlearn: 0.9998494\ttotal: 3m 28s\tremaining: 1m 8s\n",
            "3011:\tlearn: 0.9998494\ttotal: 3m 28s\tremaining: 1m 8s\n",
            "3012:\tlearn: 0.9998494\ttotal: 3m 28s\tremaining: 1m 8s\n",
            "3013:\tlearn: 0.9998494\ttotal: 3m 28s\tremaining: 1m 8s\n",
            "3014:\tlearn: 0.9998494\ttotal: 3m 28s\tremaining: 1m 8s\n",
            "3015:\tlearn: 0.9998494\ttotal: 3m 28s\tremaining: 1m 8s\n",
            "3016:\tlearn: 0.9998494\ttotal: 3m 28s\tremaining: 1m 8s\n",
            "3017:\tlearn: 0.9998494\ttotal: 3m 28s\tremaining: 1m 7s\n",
            "3018:\tlearn: 0.9998494\ttotal: 3m 28s\tremaining: 1m 7s\n",
            "3019:\tlearn: 0.9998494\ttotal: 3m 29s\tremaining: 1m 7s\n",
            "3020:\tlearn: 0.9998494\ttotal: 3m 29s\tremaining: 1m 7s\n",
            "3021:\tlearn: 0.9998494\ttotal: 3m 29s\tremaining: 1m 7s\n",
            "3022:\tlearn: 0.9998494\ttotal: 3m 29s\tremaining: 1m 7s\n",
            "3023:\tlearn: 0.9998494\ttotal: 3m 29s\tremaining: 1m 7s\n",
            "3024:\tlearn: 0.9998494\ttotal: 3m 29s\tremaining: 1m 7s\n",
            "3025:\tlearn: 0.9998494\ttotal: 3m 29s\tremaining: 1m 7s\n",
            "3026:\tlearn: 0.9998494\ttotal: 3m 29s\tremaining: 1m 7s\n",
            "3027:\tlearn: 0.9998494\ttotal: 3m 29s\tremaining: 1m 7s\n",
            "3028:\tlearn: 0.9998494\ttotal: 3m 29s\tremaining: 1m 7s\n",
            "3029:\tlearn: 0.9998494\ttotal: 3m 29s\tremaining: 1m 7s\n",
            "3030:\tlearn: 0.9998494\ttotal: 3m 29s\tremaining: 1m 7s\n",
            "3031:\tlearn: 0.9998494\ttotal: 3m 30s\tremaining: 1m 7s\n",
            "3032:\tlearn: 0.9998494\ttotal: 3m 30s\tremaining: 1m 6s\n",
            "3033:\tlearn: 0.9998494\ttotal: 3m 30s\tremaining: 1m 6s\n",
            "3034:\tlearn: 0.9998494\ttotal: 3m 30s\tremaining: 1m 6s\n",
            "3035:\tlearn: 0.9998494\ttotal: 3m 30s\tremaining: 1m 6s\n",
            "3036:\tlearn: 0.9998494\ttotal: 3m 30s\tremaining: 1m 6s\n",
            "3037:\tlearn: 0.9998494\ttotal: 3m 30s\tremaining: 1m 6s\n",
            "3038:\tlearn: 0.9998494\ttotal: 3m 30s\tremaining: 1m 6s\n",
            "3039:\tlearn: 0.9998494\ttotal: 3m 30s\tremaining: 1m 6s\n",
            "3040:\tlearn: 0.9998494\ttotal: 3m 30s\tremaining: 1m 6s\n",
            "3041:\tlearn: 0.9998494\ttotal: 3m 30s\tremaining: 1m 6s\n",
            "3042:\tlearn: 0.9998494\ttotal: 3m 30s\tremaining: 1m 6s\n",
            "3043:\tlearn: 0.9998494\ttotal: 3m 31s\tremaining: 1m 6s\n",
            "3044:\tlearn: 0.9998494\ttotal: 3m 31s\tremaining: 1m 6s\n",
            "3045:\tlearn: 0.9998494\ttotal: 3m 31s\tremaining: 1m 6s\n",
            "3046:\tlearn: 0.9998494\ttotal: 3m 31s\tremaining: 1m 6s\n",
            "3047:\tlearn: 0.9998494\ttotal: 3m 31s\tremaining: 1m 6s\n",
            "3048:\tlearn: 0.9998494\ttotal: 3m 31s\tremaining: 1m 5s\n",
            "3049:\tlearn: 0.9998494\ttotal: 3m 31s\tremaining: 1m 5s\n",
            "3050:\tlearn: 0.9998494\ttotal: 3m 31s\tremaining: 1m 5s\n",
            "3051:\tlearn: 0.9998494\ttotal: 3m 31s\tremaining: 1m 5s\n",
            "3052:\tlearn: 0.9998494\ttotal: 3m 31s\tremaining: 1m 5s\n",
            "3053:\tlearn: 0.9998494\ttotal: 3m 31s\tremaining: 1m 5s\n",
            "3054:\tlearn: 0.9998494\ttotal: 3m 31s\tremaining: 1m 5s\n",
            "3055:\tlearn: 0.9998494\ttotal: 3m 32s\tremaining: 1m 5s\n",
            "3056:\tlearn: 0.9998494\ttotal: 3m 32s\tremaining: 1m 5s\n",
            "3057:\tlearn: 0.9998494\ttotal: 3m 32s\tremaining: 1m 5s\n",
            "3058:\tlearn: 0.9998494\ttotal: 3m 32s\tremaining: 1m 5s\n",
            "3059:\tlearn: 0.9998494\ttotal: 3m 32s\tremaining: 1m 5s\n",
            "3060:\tlearn: 0.9998494\ttotal: 3m 32s\tremaining: 1m 5s\n",
            "3061:\tlearn: 0.9998494\ttotal: 3m 32s\tremaining: 1m 5s\n",
            "3062:\tlearn: 0.9998494\ttotal: 3m 32s\tremaining: 1m 5s\n",
            "3063:\tlearn: 0.9998494\ttotal: 3m 32s\tremaining: 1m 4s\n",
            "3064:\tlearn: 0.9998494\ttotal: 3m 32s\tremaining: 1m 4s\n",
            "3065:\tlearn: 0.9998494\ttotal: 3m 32s\tremaining: 1m 4s\n",
            "3066:\tlearn: 0.9998494\ttotal: 3m 32s\tremaining: 1m 4s\n",
            "3067:\tlearn: 0.9998494\ttotal: 3m 32s\tremaining: 1m 4s\n",
            "3068:\tlearn: 0.9998494\ttotal: 3m 33s\tremaining: 1m 4s\n",
            "3069:\tlearn: 0.9998494\ttotal: 3m 33s\tremaining: 1m 4s\n",
            "3070:\tlearn: 0.9998494\ttotal: 3m 33s\tremaining: 1m 4s\n",
            "3071:\tlearn: 0.9998494\ttotal: 3m 33s\tremaining: 1m 4s\n",
            "3072:\tlearn: 0.9998494\ttotal: 3m 33s\tremaining: 1m 4s\n",
            "3073:\tlearn: 0.9998494\ttotal: 3m 33s\tremaining: 1m 4s\n",
            "3074:\tlearn: 0.9998494\ttotal: 3m 33s\tremaining: 1m 4s\n",
            "3075:\tlearn: 0.9998494\ttotal: 3m 33s\tremaining: 1m 4s\n",
            "3076:\tlearn: 0.9998494\ttotal: 3m 33s\tremaining: 1m 4s\n",
            "3077:\tlearn: 0.9998494\ttotal: 3m 33s\tremaining: 1m 4s\n",
            "3078:\tlearn: 0.9998494\ttotal: 3m 33s\tremaining: 1m 3s\n",
            "3079:\tlearn: 0.9998494\ttotal: 3m 33s\tremaining: 1m 3s\n",
            "3080:\tlearn: 0.9998494\ttotal: 3m 34s\tremaining: 1m 3s\n",
            "3081:\tlearn: 0.9998494\ttotal: 3m 34s\tremaining: 1m 3s\n",
            "3082:\tlearn: 0.9998494\ttotal: 3m 34s\tremaining: 1m 3s\n",
            "3083:\tlearn: 0.9998494\ttotal: 3m 34s\tremaining: 1m 3s\n",
            "3084:\tlearn: 0.9998494\ttotal: 3m 34s\tremaining: 1m 3s\n",
            "3085:\tlearn: 0.9998494\ttotal: 3m 34s\tremaining: 1m 3s\n",
            "3086:\tlearn: 0.9998494\ttotal: 3m 34s\tremaining: 1m 3s\n",
            "3087:\tlearn: 0.9998494\ttotal: 3m 34s\tremaining: 1m 3s\n",
            "3088:\tlearn: 0.9998494\ttotal: 3m 34s\tremaining: 1m 3s\n",
            "3089:\tlearn: 0.9998494\ttotal: 3m 34s\tremaining: 1m 3s\n",
            "3090:\tlearn: 0.9998494\ttotal: 3m 34s\tremaining: 1m 3s\n",
            "3091:\tlearn: 0.9998494\ttotal: 3m 34s\tremaining: 1m 3s\n",
            "3092:\tlearn: 0.9998494\ttotal: 3m 35s\tremaining: 1m 3s\n",
            "3093:\tlearn: 0.9998494\ttotal: 3m 35s\tremaining: 1m 3s\n",
            "3094:\tlearn: 0.9998494\ttotal: 3m 35s\tremaining: 1m 2s\n",
            "3095:\tlearn: 0.9998494\ttotal: 3m 35s\tremaining: 1m 2s\n",
            "3096:\tlearn: 0.9998494\ttotal: 3m 35s\tremaining: 1m 2s\n",
            "3097:\tlearn: 0.9998494\ttotal: 3m 35s\tremaining: 1m 2s\n",
            "3098:\tlearn: 0.9998494\ttotal: 3m 35s\tremaining: 1m 2s\n",
            "3099:\tlearn: 0.9998494\ttotal: 3m 35s\tremaining: 1m 2s\n",
            "3100:\tlearn: 0.9998494\ttotal: 3m 35s\tremaining: 1m 2s\n",
            "3101:\tlearn: 0.9998494\ttotal: 3m 35s\tremaining: 1m 2s\n",
            "3102:\tlearn: 0.9998494\ttotal: 3m 35s\tremaining: 1m 2s\n",
            "3103:\tlearn: 0.9998494\ttotal: 3m 35s\tremaining: 1m 2s\n",
            "3104:\tlearn: 0.9998494\ttotal: 3m 36s\tremaining: 1m 2s\n",
            "3105:\tlearn: 0.9998494\ttotal: 3m 36s\tremaining: 1m 2s\n",
            "3106:\tlearn: 0.9998494\ttotal: 3m 36s\tremaining: 1m 2s\n",
            "3107:\tlearn: 0.9998494\ttotal: 3m 36s\tremaining: 1m 2s\n",
            "3108:\tlearn: 0.9998494\ttotal: 3m 36s\tremaining: 1m 2s\n",
            "3109:\tlearn: 0.9998494\ttotal: 3m 36s\tremaining: 1m 1s\n",
            "3110:\tlearn: 0.9998494\ttotal: 3m 36s\tremaining: 1m 1s\n",
            "3111:\tlearn: 0.9998494\ttotal: 3m 36s\tremaining: 1m 1s\n",
            "3112:\tlearn: 0.9998494\ttotal: 3m 36s\tremaining: 1m 1s\n",
            "3113:\tlearn: 0.9998494\ttotal: 3m 36s\tremaining: 1m 1s\n",
            "3114:\tlearn: 0.9998494\ttotal: 3m 36s\tremaining: 1m 1s\n",
            "3115:\tlearn: 0.9998494\ttotal: 3m 36s\tremaining: 1m 1s\n",
            "3116:\tlearn: 0.9998494\ttotal: 3m 36s\tremaining: 1m 1s\n",
            "3117:\tlearn: 0.9998494\ttotal: 3m 37s\tremaining: 1m 1s\n",
            "3118:\tlearn: 0.9998494\ttotal: 3m 37s\tremaining: 1m 1s\n",
            "3119:\tlearn: 0.9998494\ttotal: 3m 37s\tremaining: 1m 1s\n",
            "3120:\tlearn: 0.9998494\ttotal: 3m 37s\tremaining: 1m 1s\n",
            "3121:\tlearn: 0.9998494\ttotal: 3m 37s\tremaining: 1m 1s\n",
            "3122:\tlearn: 0.9998494\ttotal: 3m 37s\tremaining: 1m 1s\n",
            "3123:\tlearn: 0.9998494\ttotal: 3m 37s\tremaining: 1m\n",
            "3124:\tlearn: 0.9998494\ttotal: 3m 37s\tremaining: 1m\n",
            "3125:\tlearn: 0.9998494\ttotal: 3m 37s\tremaining: 1m\n",
            "3126:\tlearn: 0.9998494\ttotal: 3m 37s\tremaining: 1m\n",
            "3127:\tlearn: 0.9998494\ttotal: 3m 37s\tremaining: 1m\n",
            "3128:\tlearn: 0.9998494\ttotal: 3m 37s\tremaining: 1m\n",
            "3129:\tlearn: 0.9998494\ttotal: 3m 37s\tremaining: 1m\n",
            "3130:\tlearn: 0.9998494\ttotal: 3m 38s\tremaining: 1m\n",
            "3131:\tlearn: 0.9998494\ttotal: 3m 38s\tremaining: 1m\n",
            "3132:\tlearn: 0.9998494\ttotal: 3m 38s\tremaining: 1m\n",
            "3133:\tlearn: 0.9998494\ttotal: 3m 38s\tremaining: 1m\n",
            "3134:\tlearn: 0.9998494\ttotal: 3m 38s\tremaining: 1m\n",
            "3135:\tlearn: 0.9998494\ttotal: 3m 38s\tremaining: 1m\n",
            "3136:\tlearn: 0.9998494\ttotal: 3m 38s\tremaining: 1m\n",
            "3137:\tlearn: 0.9998494\ttotal: 3m 38s\tremaining: 1m\n",
            "3138:\tlearn: 0.9998494\ttotal: 3m 38s\tremaining: 60s\n",
            "3139:\tlearn: 0.9998494\ttotal: 3m 38s\tremaining: 59.9s\n",
            "3140:\tlearn: 0.9998494\ttotal: 3m 38s\tremaining: 59.8s\n",
            "3141:\tlearn: 0.9998494\ttotal: 3m 38s\tremaining: 59.8s\n",
            "3142:\tlearn: 0.9998494\ttotal: 3m 39s\tremaining: 59.7s\n",
            "3143:\tlearn: 0.9998494\ttotal: 3m 39s\tremaining: 59.6s\n",
            "3144:\tlearn: 0.9998494\ttotal: 3m 39s\tremaining: 59.6s\n",
            "3145:\tlearn: 0.9998494\ttotal: 3m 39s\tremaining: 59.5s\n",
            "3146:\tlearn: 0.9998494\ttotal: 3m 39s\tremaining: 59.4s\n",
            "3147:\tlearn: 0.9998494\ttotal: 3m 39s\tremaining: 59.4s\n",
            "3148:\tlearn: 0.9998494\ttotal: 3m 39s\tremaining: 59.3s\n",
            "3149:\tlearn: 0.9998494\ttotal: 3m 39s\tremaining: 59.2s\n",
            "3150:\tlearn: 0.9998494\ttotal: 3m 39s\tremaining: 59.2s\n",
            "3151:\tlearn: 0.9998494\ttotal: 3m 39s\tremaining: 59.1s\n",
            "3152:\tlearn: 0.9998494\ttotal: 3m 39s\tremaining: 59s\n",
            "3153:\tlearn: 0.9998494\ttotal: 3m 39s\tremaining: 59s\n",
            "3154:\tlearn: 0.9998494\ttotal: 3m 39s\tremaining: 58.9s\n",
            "3155:\tlearn: 0.9998494\ttotal: 3m 40s\tremaining: 58.8s\n",
            "3156:\tlearn: 0.9998494\ttotal: 3m 40s\tremaining: 58.8s\n",
            "3157:\tlearn: 0.9998494\ttotal: 3m 40s\tremaining: 58.7s\n",
            "3158:\tlearn: 0.9998494\ttotal: 3m 40s\tremaining: 58.6s\n",
            "3159:\tlearn: 0.9998494\ttotal: 3m 40s\tremaining: 58.6s\n",
            "3160:\tlearn: 0.9998494\ttotal: 3m 40s\tremaining: 58.5s\n",
            "3161:\tlearn: 0.9998494\ttotal: 3m 40s\tremaining: 58.4s\n",
            "3162:\tlearn: 0.9998494\ttotal: 3m 40s\tremaining: 58.4s\n",
            "3163:\tlearn: 0.9998494\ttotal: 3m 40s\tremaining: 58.3s\n",
            "3164:\tlearn: 0.9998494\ttotal: 3m 40s\tremaining: 58.2s\n",
            "3165:\tlearn: 0.9998494\ttotal: 3m 40s\tremaining: 58.2s\n",
            "3166:\tlearn: 0.9998494\ttotal: 3m 40s\tremaining: 58.1s\n",
            "3167:\tlearn: 0.9998494\ttotal: 3m 40s\tremaining: 58s\n",
            "3168:\tlearn: 0.9998494\ttotal: 3m 41s\tremaining: 58s\n",
            "3169:\tlearn: 0.9998494\ttotal: 3m 41s\tremaining: 57.9s\n",
            "3170:\tlearn: 0.9998494\ttotal: 3m 41s\tremaining: 57.8s\n",
            "3171:\tlearn: 0.9998494\ttotal: 3m 41s\tremaining: 57.8s\n",
            "3172:\tlearn: 0.9998494\ttotal: 3m 41s\tremaining: 57.7s\n",
            "3173:\tlearn: 0.9998494\ttotal: 3m 41s\tremaining: 57.6s\n",
            "3174:\tlearn: 0.9998494\ttotal: 3m 41s\tremaining: 57.6s\n",
            "3175:\tlearn: 0.9998494\ttotal: 3m 41s\tremaining: 57.5s\n",
            "3176:\tlearn: 0.9998494\ttotal: 3m 41s\tremaining: 57.4s\n",
            "3177:\tlearn: 0.9998494\ttotal: 3m 41s\tremaining: 57.4s\n",
            "3178:\tlearn: 0.9998494\ttotal: 3m 41s\tremaining: 57.3s\n",
            "3179:\tlearn: 0.9998494\ttotal: 3m 41s\tremaining: 57.2s\n",
            "3180:\tlearn: 0.9998494\ttotal: 3m 41s\tremaining: 57.2s\n",
            "3181:\tlearn: 0.9998494\ttotal: 3m 42s\tremaining: 57.1s\n",
            "3182:\tlearn: 0.9998494\ttotal: 3m 42s\tremaining: 57s\n",
            "3183:\tlearn: 0.9998494\ttotal: 3m 42s\tremaining: 57s\n",
            "3184:\tlearn: 0.9998494\ttotal: 3m 42s\tremaining: 56.9s\n",
            "3185:\tlearn: 0.9998494\ttotal: 3m 42s\tremaining: 56.8s\n",
            "3186:\tlearn: 0.9998494\ttotal: 3m 42s\tremaining: 56.8s\n",
            "3187:\tlearn: 0.9998494\ttotal: 3m 42s\tremaining: 56.7s\n",
            "3188:\tlearn: 0.9998494\ttotal: 3m 42s\tremaining: 56.6s\n",
            "3189:\tlearn: 0.9998494\ttotal: 3m 42s\tremaining: 56.5s\n",
            "3190:\tlearn: 0.9998494\ttotal: 3m 42s\tremaining: 56.5s\n",
            "3191:\tlearn: 0.9998494\ttotal: 3m 42s\tremaining: 56.4s\n",
            "3192:\tlearn: 0.9998494\ttotal: 3m 42s\tremaining: 56.3s\n",
            "3193:\tlearn: 0.9998494\ttotal: 3m 43s\tremaining: 56.3s\n",
            "3194:\tlearn: 0.9998494\ttotal: 3m 43s\tremaining: 56.2s\n",
            "3195:\tlearn: 0.9998494\ttotal: 3m 43s\tremaining: 56.1s\n",
            "3196:\tlearn: 0.9998494\ttotal: 3m 43s\tremaining: 56.1s\n",
            "3197:\tlearn: 0.9998494\ttotal: 3m 43s\tremaining: 56s\n",
            "3198:\tlearn: 0.9998494\ttotal: 3m 43s\tremaining: 55.9s\n",
            "3199:\tlearn: 0.9998494\ttotal: 3m 43s\tremaining: 55.9s\n",
            "3200:\tlearn: 0.9998494\ttotal: 3m 43s\tremaining: 55.8s\n",
            "3201:\tlearn: 0.9998494\ttotal: 3m 43s\tremaining: 55.7s\n",
            "3202:\tlearn: 0.9998494\ttotal: 3m 43s\tremaining: 55.7s\n",
            "3203:\tlearn: 0.9998494\ttotal: 3m 43s\tremaining: 55.6s\n",
            "3204:\tlearn: 0.9998494\ttotal: 3m 43s\tremaining: 55.5s\n",
            "3205:\tlearn: 0.9998494\ttotal: 3m 43s\tremaining: 55.5s\n",
            "3206:\tlearn: 0.9998494\ttotal: 3m 44s\tremaining: 55.4s\n",
            "3207:\tlearn: 0.9998494\ttotal: 3m 44s\tremaining: 55.3s\n",
            "3208:\tlearn: 0.9998494\ttotal: 3m 44s\tremaining: 55.3s\n",
            "3209:\tlearn: 0.9998494\ttotal: 3m 44s\tremaining: 55.2s\n",
            "3210:\tlearn: 0.9998494\ttotal: 3m 44s\tremaining: 55.1s\n",
            "3211:\tlearn: 0.9998494\ttotal: 3m 44s\tremaining: 55.1s\n",
            "3212:\tlearn: 0.9998494\ttotal: 3m 44s\tremaining: 55s\n",
            "3213:\tlearn: 0.9998494\ttotal: 3m 44s\tremaining: 54.9s\n",
            "3214:\tlearn: 0.9998494\ttotal: 3m 44s\tremaining: 54.9s\n",
            "3215:\tlearn: 0.9998494\ttotal: 3m 44s\tremaining: 54.8s\n",
            "3216:\tlearn: 0.9998494\ttotal: 3m 44s\tremaining: 54.7s\n",
            "3217:\tlearn: 0.9998494\ttotal: 3m 44s\tremaining: 54.7s\n",
            "3218:\tlearn: 0.9998494\ttotal: 3m 44s\tremaining: 54.6s\n",
            "3219:\tlearn: 0.9998494\ttotal: 3m 45s\tremaining: 54.5s\n",
            "3220:\tlearn: 0.9998494\ttotal: 3m 45s\tremaining: 54.4s\n",
            "3221:\tlearn: 0.9998494\ttotal: 3m 45s\tremaining: 54.4s\n",
            "3222:\tlearn: 0.9998494\ttotal: 3m 45s\tremaining: 54.3s\n",
            "3223:\tlearn: 0.9998494\ttotal: 3m 45s\tremaining: 54.2s\n",
            "3224:\tlearn: 0.9998494\ttotal: 3m 45s\tremaining: 54.2s\n",
            "3225:\tlearn: 0.9998494\ttotal: 3m 45s\tremaining: 54.1s\n",
            "3226:\tlearn: 0.9998494\ttotal: 3m 45s\tremaining: 54s\n",
            "3227:\tlearn: 0.9998494\ttotal: 3m 45s\tremaining: 54s\n",
            "3228:\tlearn: 0.9998494\ttotal: 3m 45s\tremaining: 53.9s\n",
            "3229:\tlearn: 0.9998494\ttotal: 3m 45s\tremaining: 53.8s\n",
            "3230:\tlearn: 0.9998494\ttotal: 3m 45s\tremaining: 53.8s\n",
            "3231:\tlearn: 0.9998494\ttotal: 3m 45s\tremaining: 53.7s\n",
            "3232:\tlearn: 0.9998494\ttotal: 3m 46s\tremaining: 53.6s\n",
            "3233:\tlearn: 0.9998494\ttotal: 3m 46s\tremaining: 53.6s\n",
            "3234:\tlearn: 0.9998494\ttotal: 3m 46s\tremaining: 53.5s\n",
            "3235:\tlearn: 0.9998494\ttotal: 3m 46s\tremaining: 53.4s\n",
            "3236:\tlearn: 0.9998494\ttotal: 3m 46s\tremaining: 53.3s\n",
            "3237:\tlearn: 0.9998494\ttotal: 3m 46s\tremaining: 53.3s\n",
            "3238:\tlearn: 0.9998494\ttotal: 3m 46s\tremaining: 53.2s\n",
            "3239:\tlearn: 0.9998494\ttotal: 3m 46s\tremaining: 53.1s\n",
            "3240:\tlearn: 0.9998494\ttotal: 3m 46s\tremaining: 53.1s\n",
            "3241:\tlearn: 0.9998494\ttotal: 3m 46s\tremaining: 53s\n",
            "3242:\tlearn: 0.9998494\ttotal: 3m 46s\tremaining: 52.9s\n",
            "3243:\tlearn: 0.9998494\ttotal: 3m 46s\tremaining: 52.9s\n",
            "3244:\tlearn: 0.9998494\ttotal: 3m 46s\tremaining: 52.8s\n",
            "3245:\tlearn: 0.9998494\ttotal: 3m 47s\tremaining: 52.7s\n",
            "3246:\tlearn: 0.9998494\ttotal: 3m 47s\tremaining: 52.7s\n",
            "3247:\tlearn: 0.9998494\ttotal: 3m 47s\tremaining: 52.6s\n",
            "3248:\tlearn: 0.9998494\ttotal: 3m 47s\tremaining: 52.5s\n",
            "3249:\tlearn: 0.9998494\ttotal: 3m 47s\tremaining: 52.5s\n",
            "3250:\tlearn: 0.9998494\ttotal: 3m 47s\tremaining: 52.4s\n",
            "3251:\tlearn: 0.9998494\ttotal: 3m 47s\tremaining: 52.3s\n",
            "3252:\tlearn: 0.9998494\ttotal: 3m 47s\tremaining: 52.3s\n",
            "3253:\tlearn: 0.9998494\ttotal: 3m 47s\tremaining: 52.2s\n",
            "3254:\tlearn: 0.9998494\ttotal: 3m 47s\tremaining: 52.1s\n",
            "3255:\tlearn: 0.9998494\ttotal: 3m 47s\tremaining: 52.1s\n",
            "3256:\tlearn: 0.9998494\ttotal: 3m 47s\tremaining: 52s\n",
            "3257:\tlearn: 0.9998494\ttotal: 3m 48s\tremaining: 51.9s\n",
            "3258:\tlearn: 0.9998494\ttotal: 3m 48s\tremaining: 51.9s\n",
            "3259:\tlearn: 0.9998494\ttotal: 3m 48s\tremaining: 51.8s\n",
            "3260:\tlearn: 0.9998494\ttotal: 3m 48s\tremaining: 51.7s\n",
            "3261:\tlearn: 0.9998494\ttotal: 3m 48s\tremaining: 51.7s\n",
            "3262:\tlearn: 0.9998494\ttotal: 3m 48s\tremaining: 51.6s\n",
            "3263:\tlearn: 0.9998494\ttotal: 3m 48s\tremaining: 51.5s\n",
            "3264:\tlearn: 0.9998494\ttotal: 3m 48s\tremaining: 51.4s\n",
            "3265:\tlearn: 0.9998494\ttotal: 3m 48s\tremaining: 51.4s\n",
            "3266:\tlearn: 0.9998494\ttotal: 3m 48s\tremaining: 51.3s\n",
            "3267:\tlearn: 0.9998494\ttotal: 3m 48s\tremaining: 51.3s\n",
            "3268:\tlearn: 0.9998494\ttotal: 3m 48s\tremaining: 51.2s\n",
            "3269:\tlearn: 0.9998494\ttotal: 3m 48s\tremaining: 51.1s\n",
            "3270:\tlearn: 0.9998494\ttotal: 3m 49s\tremaining: 51s\n",
            "3271:\tlearn: 0.9998494\ttotal: 3m 49s\tremaining: 51s\n",
            "3272:\tlearn: 0.9998494\ttotal: 3m 49s\tremaining: 50.9s\n",
            "3273:\tlearn: 0.9998494\ttotal: 3m 49s\tremaining: 50.9s\n",
            "3274:\tlearn: 0.9998494\ttotal: 3m 49s\tremaining: 50.8s\n",
            "3275:\tlearn: 0.9998494\ttotal: 3m 49s\tremaining: 50.7s\n",
            "3276:\tlearn: 0.9998494\ttotal: 3m 49s\tremaining: 50.6s\n",
            "3277:\tlearn: 0.9998494\ttotal: 3m 49s\tremaining: 50.6s\n",
            "3278:\tlearn: 0.9998494\ttotal: 3m 49s\tremaining: 50.5s\n",
            "3279:\tlearn: 0.9998494\ttotal: 3m 49s\tremaining: 50.5s\n",
            "3280:\tlearn: 0.9998494\ttotal: 3m 49s\tremaining: 50.4s\n",
            "3281:\tlearn: 0.9998494\ttotal: 3m 50s\tremaining: 50.3s\n",
            "3282:\tlearn: 0.9998494\ttotal: 3m 50s\tremaining: 50.2s\n",
            "3283:\tlearn: 0.9998494\ttotal: 3m 50s\tremaining: 50.2s\n",
            "3284:\tlearn: 0.9998494\ttotal: 3m 50s\tremaining: 50.1s\n",
            "3285:\tlearn: 0.9998494\ttotal: 3m 50s\tremaining: 50s\n",
            "3286:\tlearn: 0.9998494\ttotal: 3m 50s\tremaining: 50s\n",
            "3287:\tlearn: 0.9998494\ttotal: 3m 50s\tremaining: 49.9s\n",
            "3288:\tlearn: 0.9998494\ttotal: 3m 50s\tremaining: 49.8s\n",
            "3289:\tlearn: 0.9998494\ttotal: 3m 50s\tremaining: 49.8s\n",
            "3290:\tlearn: 0.9998494\ttotal: 3m 50s\tremaining: 49.7s\n",
            "3291:\tlearn: 0.9998494\ttotal: 3m 50s\tremaining: 49.6s\n",
            "3292:\tlearn: 0.9998494\ttotal: 3m 50s\tremaining: 49.6s\n",
            "3293:\tlearn: 0.9998494\ttotal: 3m 50s\tremaining: 49.5s\n",
            "3294:\tlearn: 0.9998494\ttotal: 3m 51s\tremaining: 49.4s\n",
            "3295:\tlearn: 0.9998494\ttotal: 3m 51s\tremaining: 49.4s\n",
            "3296:\tlearn: 0.9998494\ttotal: 3m 51s\tremaining: 49.3s\n",
            "3297:\tlearn: 0.9998494\ttotal: 3m 51s\tremaining: 49.2s\n",
            "3298:\tlearn: 0.9998494\ttotal: 3m 51s\tremaining: 49.1s\n",
            "3299:\tlearn: 0.9998494\ttotal: 3m 51s\tremaining: 49.1s\n",
            "3300:\tlearn: 0.9998494\ttotal: 3m 51s\tremaining: 49s\n",
            "3301:\tlearn: 0.9998494\ttotal: 3m 51s\tremaining: 48.9s\n",
            "3302:\tlearn: 0.9998494\ttotal: 3m 51s\tremaining: 48.9s\n",
            "3303:\tlearn: 0.9998494\ttotal: 3m 51s\tremaining: 48.8s\n",
            "3304:\tlearn: 0.9998494\ttotal: 3m 51s\tremaining: 48.7s\n",
            "3305:\tlearn: 0.9998494\ttotal: 3m 51s\tremaining: 48.7s\n",
            "3306:\tlearn: 0.9998494\ttotal: 3m 51s\tremaining: 48.6s\n",
            "3307:\tlearn: 0.9998494\ttotal: 3m 51s\tremaining: 48.5s\n",
            "3308:\tlearn: 0.9998494\ttotal: 3m 51s\tremaining: 48.4s\n",
            "3309:\tlearn: 0.9998494\ttotal: 3m 52s\tremaining: 48.4s\n",
            "3310:\tlearn: 0.9998494\ttotal: 3m 52s\tremaining: 48.3s\n",
            "3311:\tlearn: 0.9998494\ttotal: 3m 52s\tremaining: 48.2s\n",
            "3312:\tlearn: 0.9998494\ttotal: 3m 52s\tremaining: 48.2s\n",
            "3313:\tlearn: 0.9998494\ttotal: 3m 52s\tremaining: 48.1s\n",
            "3314:\tlearn: 0.9998494\ttotal: 3m 52s\tremaining: 48s\n",
            "3315:\tlearn: 0.9998494\ttotal: 3m 52s\tremaining: 48s\n",
            "3316:\tlearn: 0.9998494\ttotal: 3m 52s\tremaining: 47.9s\n",
            "3317:\tlearn: 0.9998494\ttotal: 3m 52s\tremaining: 47.8s\n",
            "3318:\tlearn: 0.9998494\ttotal: 3m 52s\tremaining: 47.7s\n",
            "3319:\tlearn: 0.9998494\ttotal: 3m 52s\tremaining: 47.7s\n",
            "3320:\tlearn: 0.9998494\ttotal: 3m 52s\tremaining: 47.6s\n",
            "3321:\tlearn: 0.9998494\ttotal: 3m 52s\tremaining: 47.5s\n",
            "3322:\tlearn: 0.9998494\ttotal: 3m 52s\tremaining: 47.5s\n",
            "3323:\tlearn: 0.9998494\ttotal: 3m 53s\tremaining: 47.4s\n",
            "3324:\tlearn: 0.9998494\ttotal: 3m 53s\tremaining: 47.3s\n",
            "3325:\tlearn: 0.9998494\ttotal: 3m 53s\tremaining: 47.2s\n",
            "3326:\tlearn: 0.9998494\ttotal: 3m 53s\tremaining: 47.2s\n",
            "3327:\tlearn: 0.9998494\ttotal: 3m 53s\tremaining: 47.1s\n",
            "3328:\tlearn: 0.9998494\ttotal: 3m 53s\tremaining: 47s\n",
            "3329:\tlearn: 0.9998494\ttotal: 3m 53s\tremaining: 47s\n",
            "3330:\tlearn: 0.9998494\ttotal: 3m 53s\tremaining: 46.9s\n",
            "3331:\tlearn: 0.9998494\ttotal: 3m 53s\tremaining: 46.8s\n",
            "3332:\tlearn: 0.9998494\ttotal: 3m 53s\tremaining: 46.8s\n",
            "3333:\tlearn: 0.9998494\ttotal: 3m 53s\tremaining: 46.7s\n",
            "3334:\tlearn: 0.9998494\ttotal: 3m 53s\tremaining: 46.6s\n",
            "3335:\tlearn: 0.9998494\ttotal: 3m 53s\tremaining: 46.5s\n",
            "3336:\tlearn: 0.9998494\ttotal: 3m 53s\tremaining: 46.5s\n",
            "3337:\tlearn: 0.9998494\ttotal: 3m 53s\tremaining: 46.4s\n",
            "3338:\tlearn: 0.9998494\ttotal: 3m 53s\tremaining: 46.3s\n",
            "3339:\tlearn: 0.9998494\ttotal: 3m 54s\tremaining: 46.2s\n",
            "3340:\tlearn: 0.9998494\ttotal: 3m 54s\tremaining: 46.2s\n",
            "3341:\tlearn: 0.9998494\ttotal: 3m 54s\tremaining: 46.1s\n",
            "3342:\tlearn: 0.9998494\ttotal: 3m 54s\tremaining: 46s\n",
            "3343:\tlearn: 0.9998494\ttotal: 3m 54s\tremaining: 46s\n",
            "3344:\tlearn: 0.9998494\ttotal: 3m 54s\tremaining: 45.9s\n",
            "3345:\tlearn: 0.9998494\ttotal: 3m 54s\tremaining: 45.8s\n",
            "3346:\tlearn: 0.9998494\ttotal: 3m 54s\tremaining: 45.7s\n",
            "3347:\tlearn: 0.9998494\ttotal: 3m 54s\tremaining: 45.7s\n",
            "3348:\tlearn: 0.9998494\ttotal: 3m 54s\tremaining: 45.6s\n",
            "3349:\tlearn: 0.9998494\ttotal: 3m 54s\tremaining: 45.5s\n",
            "3350:\tlearn: 0.9998494\ttotal: 3m 54s\tremaining: 45.5s\n",
            "3351:\tlearn: 0.9998494\ttotal: 3m 54s\tremaining: 45.4s\n",
            "3352:\tlearn: 0.9998494\ttotal: 3m 54s\tremaining: 45.3s\n",
            "3353:\tlearn: 0.9998494\ttotal: 3m 54s\tremaining: 45.2s\n",
            "3354:\tlearn: 0.9998494\ttotal: 3m 54s\tremaining: 45.2s\n",
            "3355:\tlearn: 0.9998494\ttotal: 3m 54s\tremaining: 45.1s\n",
            "3356:\tlearn: 0.9998494\ttotal: 3m 55s\tremaining: 45s\n",
            "3357:\tlearn: 0.9998494\ttotal: 3m 55s\tremaining: 45s\n",
            "3358:\tlearn: 0.9998494\ttotal: 3m 55s\tremaining: 44.9s\n",
            "3359:\tlearn: 0.9998494\ttotal: 3m 55s\tremaining: 44.8s\n",
            "3360:\tlearn: 0.9998494\ttotal: 3m 55s\tremaining: 44.7s\n",
            "3361:\tlearn: 0.9998494\ttotal: 3m 55s\tremaining: 44.7s\n",
            "3362:\tlearn: 0.9998494\ttotal: 3m 55s\tremaining: 44.6s\n",
            "3363:\tlearn: 0.9998494\ttotal: 3m 55s\tremaining: 44.5s\n",
            "3364:\tlearn: 0.9998494\ttotal: 3m 55s\tremaining: 44.5s\n",
            "3365:\tlearn: 0.9998494\ttotal: 3m 55s\tremaining: 44.4s\n",
            "3366:\tlearn: 0.9998494\ttotal: 3m 55s\tremaining: 44.3s\n",
            "3367:\tlearn: 0.9998563\ttotal: 3m 55s\tremaining: 44.2s\n",
            "3368:\tlearn: 0.9998494\ttotal: 3m 55s\tremaining: 44.2s\n",
            "3369:\tlearn: 0.9998494\ttotal: 3m 55s\tremaining: 44.1s\n",
            "3370:\tlearn: 0.9998494\ttotal: 3m 55s\tremaining: 44s\n",
            "3371:\tlearn: 0.9998494\ttotal: 3m 56s\tremaining: 44s\n",
            "3372:\tlearn: 0.9998494\ttotal: 3m 56s\tremaining: 43.9s\n",
            "3373:\tlearn: 0.9998494\ttotal: 3m 56s\tremaining: 43.8s\n",
            "3374:\tlearn: 0.9998494\ttotal: 3m 56s\tremaining: 43.8s\n",
            "3375:\tlearn: 0.9998494\ttotal: 3m 56s\tremaining: 43.7s\n",
            "3376:\tlearn: 0.9998494\ttotal: 3m 56s\tremaining: 43.6s\n",
            "3377:\tlearn: 0.9998494\ttotal: 3m 56s\tremaining: 43.5s\n",
            "3378:\tlearn: 0.9998494\ttotal: 3m 56s\tremaining: 43.5s\n",
            "3379:\tlearn: 0.9998494\ttotal: 3m 56s\tremaining: 43.4s\n",
            "3380:\tlearn: 0.9998494\ttotal: 3m 56s\tremaining: 43.3s\n",
            "3381:\tlearn: 0.9998494\ttotal: 3m 56s\tremaining: 43.3s\n",
            "3382:\tlearn: 0.9998494\ttotal: 3m 56s\tremaining: 43.2s\n",
            "3383:\tlearn: 0.9998494\ttotal: 3m 56s\tremaining: 43.1s\n",
            "3384:\tlearn: 0.9998494\ttotal: 3m 56s\tremaining: 43.1s\n",
            "3385:\tlearn: 0.9998494\ttotal: 3m 57s\tremaining: 43s\n",
            "3386:\tlearn: 0.9998494\ttotal: 3m 57s\tremaining: 42.9s\n",
            "3387:\tlearn: 0.9998494\ttotal: 3m 57s\tremaining: 42.8s\n",
            "3388:\tlearn: 0.9998494\ttotal: 3m 57s\tremaining: 42.8s\n",
            "3389:\tlearn: 0.9998494\ttotal: 3m 57s\tremaining: 42.7s\n",
            "3390:\tlearn: 0.9998494\ttotal: 3m 57s\tremaining: 42.6s\n",
            "3391:\tlearn: 0.9998494\ttotal: 3m 57s\tremaining: 42.6s\n",
            "3392:\tlearn: 0.9998494\ttotal: 3m 57s\tremaining: 42.5s\n",
            "3393:\tlearn: 0.9998494\ttotal: 3m 57s\tremaining: 42.4s\n",
            "3394:\tlearn: 0.9998494\ttotal: 3m 57s\tremaining: 42.3s\n",
            "3395:\tlearn: 0.9998494\ttotal: 3m 57s\tremaining: 42.3s\n",
            "3396:\tlearn: 0.9998494\ttotal: 3m 57s\tremaining: 42.2s\n",
            "3397:\tlearn: 0.9998494\ttotal: 3m 57s\tremaining: 42.1s\n",
            "3398:\tlearn: 0.9998494\ttotal: 3m 57s\tremaining: 42.1s\n",
            "3399:\tlearn: 0.9998494\ttotal: 3m 57s\tremaining: 42s\n",
            "3400:\tlearn: 0.9998494\ttotal: 3m 58s\tremaining: 41.9s\n",
            "3401:\tlearn: 0.9998494\ttotal: 3m 58s\tremaining: 41.8s\n",
            "3402:\tlearn: 0.9998494\ttotal: 3m 58s\tremaining: 41.8s\n",
            "3403:\tlearn: 0.9998494\ttotal: 3m 58s\tremaining: 41.7s\n",
            "3404:\tlearn: 0.9998494\ttotal: 3m 58s\tremaining: 41.6s\n",
            "3405:\tlearn: 0.9998494\ttotal: 3m 58s\tremaining: 41.6s\n",
            "3406:\tlearn: 0.9998494\ttotal: 3m 58s\tremaining: 41.5s\n",
            "3407:\tlearn: 0.9998494\ttotal: 3m 58s\tremaining: 41.4s\n",
            "3408:\tlearn: 0.9998494\ttotal: 3m 58s\tremaining: 41.4s\n",
            "3409:\tlearn: 0.9998494\ttotal: 3m 58s\tremaining: 41.3s\n",
            "3410:\tlearn: 0.9998494\ttotal: 3m 58s\tremaining: 41.2s\n",
            "3411:\tlearn: 0.9998494\ttotal: 3m 58s\tremaining: 41.2s\n",
            "3412:\tlearn: 0.9998494\ttotal: 3m 58s\tremaining: 41.1s\n",
            "3413:\tlearn: 0.9998494\ttotal: 3m 58s\tremaining: 41s\n",
            "3414:\tlearn: 0.9998494\ttotal: 3m 59s\tremaining: 41s\n",
            "3415:\tlearn: 0.9998494\ttotal: 3m 59s\tremaining: 40.9s\n",
            "3416:\tlearn: 0.9998494\ttotal: 3m 59s\tremaining: 40.8s\n",
            "3417:\tlearn: 0.9998494\ttotal: 3m 59s\tremaining: 40.7s\n",
            "3418:\tlearn: 0.9998494\ttotal: 3m 59s\tremaining: 40.7s\n",
            "3419:\tlearn: 0.9998494\ttotal: 3m 59s\tremaining: 40.6s\n",
            "3420:\tlearn: 0.9998494\ttotal: 3m 59s\tremaining: 40.5s\n",
            "3421:\tlearn: 0.9998494\ttotal: 3m 59s\tremaining: 40.5s\n",
            "3422:\tlearn: 0.9998494\ttotal: 3m 59s\tremaining: 40.4s\n",
            "3423:\tlearn: 0.9998494\ttotal: 3m 59s\tremaining: 40.3s\n",
            "3424:\tlearn: 0.9998494\ttotal: 3m 59s\tremaining: 40.3s\n",
            "3425:\tlearn: 0.9998494\ttotal: 3m 59s\tremaining: 40.2s\n",
            "3426:\tlearn: 0.9998494\ttotal: 3m 59s\tremaining: 40.1s\n",
            "3427:\tlearn: 0.9998494\ttotal: 3m 59s\tremaining: 40s\n",
            "3428:\tlearn: 0.9998494\ttotal: 4m\tremaining: 40s\n",
            "3429:\tlearn: 0.9998494\ttotal: 4m\tremaining: 39.9s\n",
            "3430:\tlearn: 0.9998494\ttotal: 4m\tremaining: 39.8s\n",
            "3431:\tlearn: 0.9998494\ttotal: 4m\tremaining: 39.8s\n",
            "3432:\tlearn: 0.9998494\ttotal: 4m\tremaining: 39.7s\n",
            "3433:\tlearn: 0.9998494\ttotal: 4m\tremaining: 39.6s\n",
            "3434:\tlearn: 0.9998494\ttotal: 4m\tremaining: 39.5s\n",
            "3435:\tlearn: 0.9998494\ttotal: 4m\tremaining: 39.5s\n",
            "3436:\tlearn: 0.9998494\ttotal: 4m\tremaining: 39.4s\n",
            "3437:\tlearn: 0.9998494\ttotal: 4m\tremaining: 39.3s\n",
            "3438:\tlearn: 0.9998494\ttotal: 4m\tremaining: 39.3s\n",
            "3439:\tlearn: 0.9998494\ttotal: 4m\tremaining: 39.2s\n",
            "3440:\tlearn: 0.9998494\ttotal: 4m\tremaining: 39.1s\n",
            "3441:\tlearn: 0.9998494\ttotal: 4m\tremaining: 39s\n",
            "3442:\tlearn: 0.9998494\ttotal: 4m\tremaining: 39s\n",
            "3443:\tlearn: 0.9998494\ttotal: 4m\tremaining: 38.9s\n",
            "3444:\tlearn: 0.9998494\ttotal: 4m 1s\tremaining: 38.8s\n",
            "3445:\tlearn: 0.9998494\ttotal: 4m 1s\tremaining: 38.8s\n",
            "3446:\tlearn: 0.9998494\ttotal: 4m 1s\tremaining: 38.7s\n",
            "3447:\tlearn: 0.9998494\ttotal: 4m 1s\tremaining: 38.6s\n",
            "3448:\tlearn: 0.9998494\ttotal: 4m 1s\tremaining: 38.5s\n",
            "3449:\tlearn: 0.9998494\ttotal: 4m 1s\tremaining: 38.5s\n",
            "3450:\tlearn: 0.9998494\ttotal: 4m 1s\tremaining: 38.4s\n",
            "3451:\tlearn: 0.9998494\ttotal: 4m 1s\tremaining: 38.3s\n",
            "3452:\tlearn: 0.9998494\ttotal: 4m 1s\tremaining: 38.3s\n",
            "3453:\tlearn: 0.9998494\ttotal: 4m 1s\tremaining: 38.2s\n",
            "3454:\tlearn: 0.9998494\ttotal: 4m 1s\tremaining: 38.1s\n",
            "3455:\tlearn: 0.9998494\ttotal: 4m 1s\tremaining: 38s\n",
            "3456:\tlearn: 0.9998494\ttotal: 4m 1s\tremaining: 38s\n",
            "3457:\tlearn: 0.9998494\ttotal: 4m 1s\tremaining: 37.9s\n",
            "3458:\tlearn: 0.9998494\ttotal: 4m 1s\tremaining: 37.8s\n",
            "3459:\tlearn: 0.9998494\ttotal: 4m 1s\tremaining: 37.8s\n",
            "3460:\tlearn: 0.9998494\ttotal: 4m 2s\tremaining: 37.7s\n",
            "3461:\tlearn: 0.9998494\ttotal: 4m 2s\tremaining: 37.6s\n",
            "3462:\tlearn: 0.9998494\ttotal: 4m 2s\tremaining: 37.5s\n",
            "3463:\tlearn: 0.9998494\ttotal: 4m 2s\tremaining: 37.5s\n",
            "3464:\tlearn: 0.9998494\ttotal: 4m 2s\tremaining: 37.4s\n",
            "3465:\tlearn: 0.9998494\ttotal: 4m 2s\tremaining: 37.3s\n",
            "3466:\tlearn: 0.9998494\ttotal: 4m 2s\tremaining: 37.3s\n",
            "3467:\tlearn: 0.9998494\ttotal: 4m 2s\tremaining: 37.2s\n",
            "3468:\tlearn: 0.9998494\ttotal: 4m 2s\tremaining: 37.1s\n",
            "3469:\tlearn: 0.9998494\ttotal: 4m 2s\tremaining: 37.1s\n",
            "3470:\tlearn: 0.9998494\ttotal: 4m 2s\tremaining: 37s\n",
            "3471:\tlearn: 0.9998494\ttotal: 4m 2s\tremaining: 36.9s\n",
            "3472:\tlearn: 0.9998494\ttotal: 4m 2s\tremaining: 36.8s\n",
            "3473:\tlearn: 0.9998494\ttotal: 4m 2s\tremaining: 36.8s\n",
            "3474:\tlearn: 0.9998494\ttotal: 4m 2s\tremaining: 36.7s\n",
            "3475:\tlearn: 0.9998494\ttotal: 4m 2s\tremaining: 36.6s\n",
            "3476:\tlearn: 0.9998494\ttotal: 4m 3s\tremaining: 36.6s\n",
            "3477:\tlearn: 0.9998494\ttotal: 4m 3s\tremaining: 36.5s\n",
            "3478:\tlearn: 0.9998494\ttotal: 4m 3s\tremaining: 36.4s\n",
            "3479:\tlearn: 0.9998494\ttotal: 4m 3s\tremaining: 36.3s\n",
            "3480:\tlearn: 0.9998494\ttotal: 4m 3s\tremaining: 36.3s\n",
            "3481:\tlearn: 0.9998494\ttotal: 4m 3s\tremaining: 36.2s\n",
            "3482:\tlearn: 0.9998494\ttotal: 4m 3s\tremaining: 36.1s\n",
            "3483:\tlearn: 0.9998494\ttotal: 4m 3s\tremaining: 36.1s\n",
            "3484:\tlearn: 0.9998494\ttotal: 4m 3s\tremaining: 36s\n",
            "3485:\tlearn: 0.9998494\ttotal: 4m 3s\tremaining: 35.9s\n",
            "3486:\tlearn: 0.9998494\ttotal: 4m 3s\tremaining: 35.8s\n",
            "3487:\tlearn: 0.9998494\ttotal: 4m 3s\tremaining: 35.8s\n",
            "3488:\tlearn: 0.9998494\ttotal: 4m 3s\tremaining: 35.7s\n",
            "3489:\tlearn: 0.9998494\ttotal: 4m 3s\tremaining: 35.6s\n",
            "3490:\tlearn: 0.9998494\ttotal: 4m 3s\tremaining: 35.6s\n",
            "3491:\tlearn: 0.9998494\ttotal: 4m 3s\tremaining: 35.5s\n",
            "3492:\tlearn: 0.9998494\ttotal: 4m 3s\tremaining: 35.4s\n",
            "3493:\tlearn: 0.9998494\ttotal: 4m 4s\tremaining: 35.3s\n",
            "3494:\tlearn: 0.9998494\ttotal: 4m 4s\tremaining: 35.3s\n",
            "3495:\tlearn: 0.9998494\ttotal: 4m 4s\tremaining: 35.2s\n",
            "3496:\tlearn: 0.9998494\ttotal: 4m 4s\tremaining: 35.1s\n",
            "3497:\tlearn: 0.9998494\ttotal: 4m 4s\tremaining: 35.1s\n",
            "3498:\tlearn: 0.9998494\ttotal: 4m 4s\tremaining: 35s\n",
            "3499:\tlearn: 0.9998494\ttotal: 4m 4s\tremaining: 34.9s\n",
            "3500:\tlearn: 0.9998494\ttotal: 4m 4s\tremaining: 34.9s\n",
            "3501:\tlearn: 0.9998494\ttotal: 4m 4s\tremaining: 34.8s\n",
            "3502:\tlearn: 0.9998494\ttotal: 4m 4s\tremaining: 34.7s\n",
            "3503:\tlearn: 0.9998494\ttotal: 4m 4s\tremaining: 34.6s\n",
            "3504:\tlearn: 0.9998494\ttotal: 4m 4s\tremaining: 34.6s\n",
            "3505:\tlearn: 0.9998494\ttotal: 4m 4s\tremaining: 34.5s\n",
            "3506:\tlearn: 0.9998494\ttotal: 4m 4s\tremaining: 34.4s\n",
            "3507:\tlearn: 0.9998494\ttotal: 4m 5s\tremaining: 34.4s\n",
            "3508:\tlearn: 0.9998494\ttotal: 4m 5s\tremaining: 34.3s\n",
            "3509:\tlearn: 0.9998494\ttotal: 4m 5s\tremaining: 34.2s\n",
            "3510:\tlearn: 0.9998494\ttotal: 4m 5s\tremaining: 34.2s\n",
            "3511:\tlearn: 0.9998494\ttotal: 4m 5s\tremaining: 34.1s\n",
            "3512:\tlearn: 0.9998494\ttotal: 4m 5s\tremaining: 34s\n",
            "3513:\tlearn: 0.9998494\ttotal: 4m 5s\tremaining: 33.9s\n",
            "3514:\tlearn: 0.9998494\ttotal: 4m 5s\tremaining: 33.9s\n",
            "3515:\tlearn: 0.9998494\ttotal: 4m 5s\tremaining: 33.8s\n",
            "3516:\tlearn: 0.9998494\ttotal: 4m 5s\tremaining: 33.7s\n",
            "3517:\tlearn: 0.9998494\ttotal: 4m 5s\tremaining: 33.7s\n",
            "3518:\tlearn: 0.9998494\ttotal: 4m 5s\tremaining: 33.6s\n",
            "3519:\tlearn: 0.9998494\ttotal: 4m 5s\tremaining: 33.5s\n",
            "3520:\tlearn: 0.9998494\ttotal: 4m 5s\tremaining: 33.5s\n",
            "3521:\tlearn: 0.9998494\ttotal: 4m 5s\tremaining: 33.4s\n",
            "3522:\tlearn: 0.9998494\ttotal: 4m 6s\tremaining: 33.3s\n",
            "3523:\tlearn: 0.9998494\ttotal: 4m 6s\tremaining: 33.2s\n",
            "3524:\tlearn: 0.9998494\ttotal: 4m 6s\tremaining: 33.2s\n",
            "3525:\tlearn: 0.9998494\ttotal: 4m 6s\tremaining: 33.1s\n",
            "3526:\tlearn: 0.9998494\ttotal: 4m 6s\tremaining: 33s\n",
            "3527:\tlearn: 0.9998494\ttotal: 4m 6s\tremaining: 33s\n",
            "3528:\tlearn: 0.9998494\ttotal: 4m 6s\tremaining: 32.9s\n",
            "3529:\tlearn: 0.9998494\ttotal: 4m 6s\tremaining: 32.8s\n",
            "3530:\tlearn: 0.9998494\ttotal: 4m 6s\tremaining: 32.8s\n",
            "3531:\tlearn: 0.9998494\ttotal: 4m 6s\tremaining: 32.7s\n",
            "3532:\tlearn: 0.9998494\ttotal: 4m 6s\tremaining: 32.6s\n",
            "3533:\tlearn: 0.9998494\ttotal: 4m 6s\tremaining: 32.5s\n",
            "3534:\tlearn: 0.9998494\ttotal: 4m 6s\tremaining: 32.5s\n",
            "3535:\tlearn: 0.9998494\ttotal: 4m 6s\tremaining: 32.4s\n",
            "3536:\tlearn: 0.9998494\ttotal: 4m 7s\tremaining: 32.3s\n",
            "3537:\tlearn: 0.9998494\ttotal: 4m 7s\tremaining: 32.3s\n",
            "3538:\tlearn: 0.9998494\ttotal: 4m 7s\tremaining: 32.2s\n",
            "3539:\tlearn: 0.9998494\ttotal: 4m 7s\tremaining: 32.1s\n",
            "3540:\tlearn: 0.9998494\ttotal: 4m 7s\tremaining: 32.1s\n",
            "3541:\tlearn: 0.9998494\ttotal: 4m 7s\tremaining: 32s\n",
            "3542:\tlearn: 0.9998494\ttotal: 4m 7s\tremaining: 31.9s\n",
            "3543:\tlearn: 0.9998494\ttotal: 4m 7s\tremaining: 31.8s\n",
            "3544:\tlearn: 0.9998494\ttotal: 4m 7s\tremaining: 31.8s\n",
            "3545:\tlearn: 0.9998494\ttotal: 4m 7s\tremaining: 31.7s\n",
            "3546:\tlearn: 0.9998494\ttotal: 4m 7s\tremaining: 31.6s\n",
            "3547:\tlearn: 0.9998494\ttotal: 4m 7s\tremaining: 31.6s\n",
            "3548:\tlearn: 0.9998494\ttotal: 4m 7s\tremaining: 31.5s\n",
            "3549:\tlearn: 0.9998494\ttotal: 4m 7s\tremaining: 31.4s\n",
            "3550:\tlearn: 0.9998494\ttotal: 4m 7s\tremaining: 31.4s\n",
            "3551:\tlearn: 0.9998494\ttotal: 4m 8s\tremaining: 31.3s\n",
            "3552:\tlearn: 0.9998494\ttotal: 4m 8s\tremaining: 31.2s\n",
            "3553:\tlearn: 0.9998494\ttotal: 4m 8s\tremaining: 31.1s\n",
            "3554:\tlearn: 0.9998494\ttotal: 4m 8s\tremaining: 31.1s\n",
            "3555:\tlearn: 0.9998494\ttotal: 4m 8s\tremaining: 31s\n",
            "3556:\tlearn: 0.9998494\ttotal: 4m 8s\tremaining: 30.9s\n",
            "3557:\tlearn: 0.9998494\ttotal: 4m 8s\tremaining: 30.9s\n",
            "3558:\tlearn: 0.9998494\ttotal: 4m 8s\tremaining: 30.8s\n",
            "3559:\tlearn: 0.9998494\ttotal: 4m 8s\tremaining: 30.7s\n",
            "3560:\tlearn: 0.9998494\ttotal: 4m 8s\tremaining: 30.6s\n",
            "3561:\tlearn: 0.9998494\ttotal: 4m 8s\tremaining: 30.6s\n",
            "3562:\tlearn: 0.9998494\ttotal: 4m 8s\tremaining: 30.5s\n",
            "3563:\tlearn: 0.9998494\ttotal: 4m 8s\tremaining: 30.4s\n",
            "3564:\tlearn: 0.9998494\ttotal: 4m 8s\tremaining: 30.4s\n",
            "3565:\tlearn: 0.9998494\ttotal: 4m 8s\tremaining: 30.3s\n",
            "3566:\tlearn: 0.9998494\ttotal: 4m 9s\tremaining: 30.2s\n",
            "3567:\tlearn: 0.9998494\ttotal: 4m 9s\tremaining: 30.2s\n",
            "3568:\tlearn: 0.9998494\ttotal: 4m 9s\tremaining: 30.1s\n",
            "3569:\tlearn: 0.9998494\ttotal: 4m 9s\tremaining: 30s\n",
            "3570:\tlearn: 0.9998494\ttotal: 4m 9s\tremaining: 29.9s\n",
            "3571:\tlearn: 0.9998494\ttotal: 4m 9s\tremaining: 29.9s\n",
            "3572:\tlearn: 0.9998494\ttotal: 4m 9s\tremaining: 29.8s\n",
            "3573:\tlearn: 0.9998494\ttotal: 4m 9s\tremaining: 29.7s\n",
            "3574:\tlearn: 0.9998494\ttotal: 4m 9s\tremaining: 29.7s\n",
            "3575:\tlearn: 0.9998494\ttotal: 4m 9s\tremaining: 29.6s\n",
            "3576:\tlearn: 0.9998494\ttotal: 4m 9s\tremaining: 29.5s\n",
            "3577:\tlearn: 0.9998494\ttotal: 4m 9s\tremaining: 29.5s\n",
            "3578:\tlearn: 0.9998494\ttotal: 4m 9s\tremaining: 29.4s\n",
            "3579:\tlearn: 0.9998494\ttotal: 4m 9s\tremaining: 29.3s\n",
            "3580:\tlearn: 0.9998494\ttotal: 4m 9s\tremaining: 29.3s\n",
            "3581:\tlearn: 0.9998494\ttotal: 4m 10s\tremaining: 29.2s\n",
            "3582:\tlearn: 0.9998494\ttotal: 4m 10s\tremaining: 29.1s\n",
            "3583:\tlearn: 0.9998494\ttotal: 4m 10s\tremaining: 29s\n",
            "3584:\tlearn: 0.9998494\ttotal: 4m 10s\tremaining: 29s\n",
            "3585:\tlearn: 0.9998494\ttotal: 4m 10s\tremaining: 28.9s\n",
            "3586:\tlearn: 0.9998494\ttotal: 4m 10s\tremaining: 28.8s\n",
            "3587:\tlearn: 0.9998494\ttotal: 4m 10s\tremaining: 28.8s\n",
            "3588:\tlearn: 0.9998494\ttotal: 4m 10s\tremaining: 28.7s\n",
            "3589:\tlearn: 0.9998494\ttotal: 4m 10s\tremaining: 28.6s\n",
            "3590:\tlearn: 0.9998494\ttotal: 4m 10s\tremaining: 28.6s\n",
            "3591:\tlearn: 0.9998494\ttotal: 4m 10s\tremaining: 28.5s\n",
            "3592:\tlearn: 0.9998494\ttotal: 4m 10s\tremaining: 28.4s\n",
            "3593:\tlearn: 0.9998494\ttotal: 4m 10s\tremaining: 28.3s\n",
            "3594:\tlearn: 0.9998494\ttotal: 4m 10s\tremaining: 28.3s\n",
            "3595:\tlearn: 0.9998494\ttotal: 4m 10s\tremaining: 28.2s\n",
            "3596:\tlearn: 0.9998494\ttotal: 4m 11s\tremaining: 28.1s\n",
            "3597:\tlearn: 0.9998494\ttotal: 4m 11s\tremaining: 28.1s\n",
            "3598:\tlearn: 0.9998494\ttotal: 4m 11s\tremaining: 28s\n",
            "3599:\tlearn: 0.9998494\ttotal: 4m 11s\tremaining: 27.9s\n",
            "3600:\tlearn: 0.9998494\ttotal: 4m 11s\tremaining: 27.8s\n",
            "3601:\tlearn: 0.9998494\ttotal: 4m 11s\tremaining: 27.8s\n",
            "3602:\tlearn: 0.9998494\ttotal: 4m 11s\tremaining: 27.7s\n",
            "3603:\tlearn: 0.9998494\ttotal: 4m 11s\tremaining: 27.6s\n",
            "3604:\tlearn: 0.9998494\ttotal: 4m 11s\tremaining: 27.6s\n",
            "3605:\tlearn: 0.9998494\ttotal: 4m 11s\tremaining: 27.5s\n",
            "3606:\tlearn: 0.9998494\ttotal: 4m 11s\tremaining: 27.4s\n",
            "3607:\tlearn: 0.9998494\ttotal: 4m 11s\tremaining: 27.4s\n",
            "3608:\tlearn: 0.9998494\ttotal: 4m 11s\tremaining: 27.3s\n",
            "3609:\tlearn: 0.9998494\ttotal: 4m 11s\tremaining: 27.2s\n",
            "3610:\tlearn: 0.9998494\ttotal: 4m 12s\tremaining: 27.1s\n",
            "3611:\tlearn: 0.9998494\ttotal: 4m 12s\tremaining: 27.1s\n",
            "3612:\tlearn: 0.9998494\ttotal: 4m 12s\tremaining: 27s\n",
            "3613:\tlearn: 0.9998494\ttotal: 4m 12s\tremaining: 26.9s\n",
            "3614:\tlearn: 0.9998494\ttotal: 4m 12s\tremaining: 26.9s\n",
            "3615:\tlearn: 0.9998494\ttotal: 4m 12s\tremaining: 26.8s\n",
            "3616:\tlearn: 0.9998494\ttotal: 4m 12s\tremaining: 26.7s\n",
            "3617:\tlearn: 0.9998494\ttotal: 4m 12s\tremaining: 26.7s\n",
            "3618:\tlearn: 0.9998494\ttotal: 4m 12s\tremaining: 26.6s\n",
            "3619:\tlearn: 0.9998494\ttotal: 4m 12s\tremaining: 26.5s\n",
            "3620:\tlearn: 0.9998494\ttotal: 4m 12s\tremaining: 26.4s\n",
            "3621:\tlearn: 0.9998494\ttotal: 4m 12s\tremaining: 26.4s\n",
            "3622:\tlearn: 0.9998494\ttotal: 4m 12s\tremaining: 26.3s\n",
            "3623:\tlearn: 0.9998494\ttotal: 4m 12s\tremaining: 26.2s\n",
            "3624:\tlearn: 0.9998494\ttotal: 4m 12s\tremaining: 26.2s\n",
            "3625:\tlearn: 0.9998494\ttotal: 4m 12s\tremaining: 26.1s\n",
            "3626:\tlearn: 0.9998494\ttotal: 4m 13s\tremaining: 26s\n",
            "3627:\tlearn: 0.9998494\ttotal: 4m 13s\tremaining: 26s\n",
            "3628:\tlearn: 0.9998494\ttotal: 4m 13s\tremaining: 25.9s\n",
            "3629:\tlearn: 0.9998494\ttotal: 4m 13s\tremaining: 25.8s\n",
            "3630:\tlearn: 0.9998494\ttotal: 4m 13s\tremaining: 25.7s\n",
            "3631:\tlearn: 0.9998494\ttotal: 4m 13s\tremaining: 25.7s\n",
            "3632:\tlearn: 0.9998494\ttotal: 4m 13s\tremaining: 25.6s\n",
            "3633:\tlearn: 0.9998494\ttotal: 4m 13s\tremaining: 25.5s\n",
            "3634:\tlearn: 0.9998494\ttotal: 4m 13s\tremaining: 25.5s\n",
            "3635:\tlearn: 0.9998494\ttotal: 4m 13s\tremaining: 25.4s\n",
            "3636:\tlearn: 0.9998494\ttotal: 4m 13s\tremaining: 25.3s\n",
            "3637:\tlearn: 0.9998494\ttotal: 4m 13s\tremaining: 25.3s\n",
            "3638:\tlearn: 0.9998494\ttotal: 4m 13s\tremaining: 25.2s\n",
            "3639:\tlearn: 0.9998494\ttotal: 4m 13s\tremaining: 25.1s\n",
            "3640:\tlearn: 0.9998494\ttotal: 4m 14s\tremaining: 25.1s\n",
            "3641:\tlearn: 0.9998494\ttotal: 4m 14s\tremaining: 25s\n",
            "3642:\tlearn: 0.9998494\ttotal: 4m 14s\tremaining: 24.9s\n",
            "3643:\tlearn: 0.9998494\ttotal: 4m 14s\tremaining: 24.8s\n",
            "3644:\tlearn: 0.9998494\ttotal: 4m 14s\tremaining: 24.8s\n",
            "3645:\tlearn: 0.9998494\ttotal: 4m 14s\tremaining: 24.7s\n",
            "3646:\tlearn: 0.9998494\ttotal: 4m 14s\tremaining: 24.6s\n",
            "3647:\tlearn: 0.9998494\ttotal: 4m 14s\tremaining: 24.6s\n",
            "3648:\tlearn: 0.9998494\ttotal: 4m 14s\tremaining: 24.5s\n",
            "3649:\tlearn: 0.9998494\ttotal: 4m 14s\tremaining: 24.4s\n",
            "3650:\tlearn: 0.9998494\ttotal: 4m 14s\tremaining: 24.4s\n",
            "3651:\tlearn: 0.9998494\ttotal: 4m 14s\tremaining: 24.3s\n",
            "3652:\tlearn: 0.9998494\ttotal: 4m 14s\tremaining: 24.2s\n",
            "3653:\tlearn: 0.9998494\ttotal: 4m 14s\tremaining: 24.1s\n",
            "3654:\tlearn: 0.9998494\ttotal: 4m 15s\tremaining: 24.1s\n",
            "3655:\tlearn: 0.9998494\ttotal: 4m 15s\tremaining: 24s\n",
            "3656:\tlearn: 0.9998494\ttotal: 4m 15s\tremaining: 23.9s\n",
            "3657:\tlearn: 0.9998494\ttotal: 4m 15s\tremaining: 23.9s\n",
            "3658:\tlearn: 0.9998494\ttotal: 4m 15s\tremaining: 23.8s\n",
            "3659:\tlearn: 0.9998494\ttotal: 4m 15s\tremaining: 23.7s\n",
            "3660:\tlearn: 0.9998494\ttotal: 4m 15s\tremaining: 23.7s\n",
            "3661:\tlearn: 0.9998494\ttotal: 4m 15s\tremaining: 23.6s\n",
            "3662:\tlearn: 0.9998494\ttotal: 4m 15s\tremaining: 23.5s\n",
            "3663:\tlearn: 0.9998494\ttotal: 4m 15s\tremaining: 23.4s\n",
            "3664:\tlearn: 0.9998494\ttotal: 4m 15s\tremaining: 23.4s\n",
            "3665:\tlearn: 0.9998494\ttotal: 4m 15s\tremaining: 23.3s\n",
            "3666:\tlearn: 0.9998494\ttotal: 4m 15s\tremaining: 23.2s\n",
            "3667:\tlearn: 0.9998494\ttotal: 4m 15s\tremaining: 23.2s\n",
            "3668:\tlearn: 0.9998494\ttotal: 4m 15s\tremaining: 23.1s\n",
            "3669:\tlearn: 0.9998494\ttotal: 4m 16s\tremaining: 23s\n",
            "3670:\tlearn: 0.9998494\ttotal: 4m 16s\tremaining: 22.9s\n",
            "3671:\tlearn: 0.9998494\ttotal: 4m 16s\tremaining: 22.9s\n",
            "3672:\tlearn: 0.9998494\ttotal: 4m 16s\tremaining: 22.8s\n",
            "3673:\tlearn: 0.9998494\ttotal: 4m 16s\tremaining: 22.7s\n",
            "3674:\tlearn: 0.9998494\ttotal: 4m 16s\tremaining: 22.7s\n",
            "3675:\tlearn: 0.9998494\ttotal: 4m 16s\tremaining: 22.6s\n",
            "3676:\tlearn: 0.9998494\ttotal: 4m 16s\tremaining: 22.5s\n",
            "3677:\tlearn: 0.9998494\ttotal: 4m 16s\tremaining: 22.5s\n",
            "3678:\tlearn: 0.9998494\ttotal: 4m 16s\tremaining: 22.4s\n",
            "3679:\tlearn: 0.9998494\ttotal: 4m 16s\tremaining: 22.3s\n",
            "3680:\tlearn: 0.9998494\ttotal: 4m 16s\tremaining: 22.2s\n",
            "3681:\tlearn: 0.9998494\ttotal: 4m 16s\tremaining: 22.2s\n",
            "3682:\tlearn: 0.9998494\ttotal: 4m 16s\tremaining: 22.1s\n",
            "3683:\tlearn: 0.9998494\ttotal: 4m 16s\tremaining: 22s\n",
            "3684:\tlearn: 0.9998494\ttotal: 4m 16s\tremaining: 22s\n",
            "3685:\tlearn: 0.9998494\ttotal: 4m 17s\tremaining: 21.9s\n",
            "3686:\tlearn: 0.9998494\ttotal: 4m 17s\tremaining: 21.8s\n",
            "3687:\tlearn: 0.9998494\ttotal: 4m 17s\tremaining: 21.8s\n",
            "3688:\tlearn: 0.9998494\ttotal: 4m 17s\tremaining: 21.7s\n",
            "3689:\tlearn: 0.9998494\ttotal: 4m 17s\tremaining: 21.6s\n",
            "3690:\tlearn: 0.9998494\ttotal: 4m 17s\tremaining: 21.5s\n",
            "3691:\tlearn: 0.9998494\ttotal: 4m 17s\tremaining: 21.5s\n",
            "3692:\tlearn: 0.9998494\ttotal: 4m 17s\tremaining: 21.4s\n",
            "3693:\tlearn: 0.9998494\ttotal: 4m 17s\tremaining: 21.3s\n",
            "3694:\tlearn: 0.9998494\ttotal: 4m 17s\tremaining: 21.3s\n",
            "3695:\tlearn: 0.9998563\ttotal: 4m 17s\tremaining: 21.2s\n",
            "3696:\tlearn: 0.9998563\ttotal: 4m 17s\tremaining: 21.1s\n",
            "3697:\tlearn: 0.9998563\ttotal: 4m 17s\tremaining: 21.1s\n",
            "3698:\tlearn: 0.9998563\ttotal: 4m 17s\tremaining: 21s\n",
            "3699:\tlearn: 0.9998563\ttotal: 4m 17s\tremaining: 20.9s\n",
            "3700:\tlearn: 0.9998563\ttotal: 4m 17s\tremaining: 20.8s\n",
            "3701:\tlearn: 0.9998563\ttotal: 4m 18s\tremaining: 20.8s\n",
            "3702:\tlearn: 0.9998563\ttotal: 4m 18s\tremaining: 20.7s\n",
            "3703:\tlearn: 0.9998563\ttotal: 4m 18s\tremaining: 20.6s\n",
            "3704:\tlearn: 0.9998563\ttotal: 4m 18s\tremaining: 20.6s\n",
            "3705:\tlearn: 0.9998563\ttotal: 4m 18s\tremaining: 20.5s\n",
            "3706:\tlearn: 0.9998563\ttotal: 4m 18s\tremaining: 20.4s\n",
            "3707:\tlearn: 0.9998563\ttotal: 4m 18s\tremaining: 20.3s\n",
            "3708:\tlearn: 0.9998563\ttotal: 4m 18s\tremaining: 20.3s\n",
            "3709:\tlearn: 0.9998563\ttotal: 4m 18s\tremaining: 20.2s\n",
            "3710:\tlearn: 0.9998563\ttotal: 4m 18s\tremaining: 20.1s\n",
            "3711:\tlearn: 0.9998563\ttotal: 4m 18s\tremaining: 20.1s\n",
            "3712:\tlearn: 0.9998563\ttotal: 4m 18s\tremaining: 20s\n",
            "3713:\tlearn: 0.9998563\ttotal: 4m 18s\tremaining: 19.9s\n",
            "3714:\tlearn: 0.9998563\ttotal: 4m 18s\tremaining: 19.9s\n",
            "3715:\tlearn: 0.9998563\ttotal: 4m 18s\tremaining: 19.8s\n",
            "3716:\tlearn: 0.9998563\ttotal: 4m 19s\tremaining: 19.7s\n",
            "3717:\tlearn: 0.9998563\ttotal: 4m 19s\tremaining: 19.7s\n",
            "3718:\tlearn: 0.9998563\ttotal: 4m 19s\tremaining: 19.6s\n",
            "3719:\tlearn: 0.9998563\ttotal: 4m 19s\tremaining: 19.5s\n",
            "3720:\tlearn: 0.9998563\ttotal: 4m 19s\tremaining: 19.4s\n",
            "3721:\tlearn: 0.9998563\ttotal: 4m 19s\tremaining: 19.4s\n",
            "3722:\tlearn: 0.9998563\ttotal: 4m 19s\tremaining: 19.3s\n",
            "3723:\tlearn: 0.9998563\ttotal: 4m 19s\tremaining: 19.2s\n",
            "3724:\tlearn: 0.9998563\ttotal: 4m 19s\tremaining: 19.2s\n",
            "3725:\tlearn: 0.9998563\ttotal: 4m 19s\tremaining: 19.1s\n",
            "3726:\tlearn: 0.9998563\ttotal: 4m 19s\tremaining: 19s\n",
            "3727:\tlearn: 0.9998563\ttotal: 4m 19s\tremaining: 18.9s\n",
            "3728:\tlearn: 0.9998563\ttotal: 4m 19s\tremaining: 18.9s\n",
            "3729:\tlearn: 0.9998563\ttotal: 4m 19s\tremaining: 18.8s\n",
            "3730:\tlearn: 0.9998563\ttotal: 4m 19s\tremaining: 18.7s\n",
            "3731:\tlearn: 0.9998563\ttotal: 4m 19s\tremaining: 18.7s\n",
            "3732:\tlearn: 0.9998563\ttotal: 4m 20s\tremaining: 18.6s\n",
            "3733:\tlearn: 0.9998563\ttotal: 4m 20s\tremaining: 18.5s\n",
            "3734:\tlearn: 0.9998563\ttotal: 4m 20s\tremaining: 18.5s\n",
            "3735:\tlearn: 0.9998563\ttotal: 4m 20s\tremaining: 18.4s\n",
            "3736:\tlearn: 0.9998563\ttotal: 4m 20s\tremaining: 18.3s\n",
            "3737:\tlearn: 0.9998563\ttotal: 4m 20s\tremaining: 18.2s\n",
            "3738:\tlearn: 0.9998563\ttotal: 4m 20s\tremaining: 18.2s\n",
            "3739:\tlearn: 0.9998563\ttotal: 4m 20s\tremaining: 18.1s\n",
            "3740:\tlearn: 0.9998563\ttotal: 4m 20s\tremaining: 18s\n",
            "3741:\tlearn: 0.9998563\ttotal: 4m 20s\tremaining: 18s\n",
            "3742:\tlearn: 0.9998563\ttotal: 4m 20s\tremaining: 17.9s\n",
            "3743:\tlearn: 0.9998563\ttotal: 4m 20s\tremaining: 17.8s\n",
            "3744:\tlearn: 0.9998563\ttotal: 4m 20s\tremaining: 17.8s\n",
            "3745:\tlearn: 0.9998563\ttotal: 4m 20s\tremaining: 17.7s\n",
            "3746:\tlearn: 0.9998563\ttotal: 4m 20s\tremaining: 17.6s\n",
            "3747:\tlearn: 0.9998563\ttotal: 4m 21s\tremaining: 17.5s\n",
            "3748:\tlearn: 0.9998563\ttotal: 4m 21s\tremaining: 17.5s\n",
            "3749:\tlearn: 0.9998563\ttotal: 4m 21s\tremaining: 17.4s\n",
            "3750:\tlearn: 0.9998563\ttotal: 4m 21s\tremaining: 17.3s\n",
            "3751:\tlearn: 0.9998563\ttotal: 4m 21s\tremaining: 17.3s\n",
            "3752:\tlearn: 0.9998563\ttotal: 4m 21s\tremaining: 17.2s\n",
            "3753:\tlearn: 0.9998563\ttotal: 4m 21s\tremaining: 17.1s\n",
            "3754:\tlearn: 0.9998563\ttotal: 4m 21s\tremaining: 17.1s\n",
            "3755:\tlearn: 0.9998563\ttotal: 4m 21s\tremaining: 17s\n",
            "3756:\tlearn: 0.9998563\ttotal: 4m 21s\tremaining: 16.9s\n",
            "3757:\tlearn: 0.9998563\ttotal: 4m 21s\tremaining: 16.9s\n",
            "3758:\tlearn: 0.9998563\ttotal: 4m 21s\tremaining: 16.8s\n",
            "3759:\tlearn: 0.9998563\ttotal: 4m 21s\tremaining: 16.7s\n",
            "3760:\tlearn: 0.9998563\ttotal: 4m 21s\tremaining: 16.6s\n",
            "3761:\tlearn: 0.9998563\ttotal: 4m 21s\tremaining: 16.6s\n",
            "3762:\tlearn: 0.9998563\ttotal: 4m 22s\tremaining: 16.5s\n",
            "3763:\tlearn: 0.9998563\ttotal: 4m 22s\tremaining: 16.4s\n",
            "3764:\tlearn: 0.9998563\ttotal: 4m 22s\tremaining: 16.4s\n",
            "3765:\tlearn: 0.9998563\ttotal: 4m 22s\tremaining: 16.3s\n",
            "3766:\tlearn: 0.9998563\ttotal: 4m 22s\tremaining: 16.2s\n",
            "3767:\tlearn: 0.9998563\ttotal: 4m 22s\tremaining: 16.2s\n",
            "3768:\tlearn: 0.9998563\ttotal: 4m 22s\tremaining: 16.1s\n",
            "3769:\tlearn: 0.9998563\ttotal: 4m 22s\tremaining: 16s\n",
            "3770:\tlearn: 0.9998563\ttotal: 4m 22s\tremaining: 15.9s\n",
            "3771:\tlearn: 0.9998563\ttotal: 4m 22s\tremaining: 15.9s\n",
            "3772:\tlearn: 0.9998563\ttotal: 4m 22s\tremaining: 15.8s\n",
            "3773:\tlearn: 0.9998563\ttotal: 4m 22s\tremaining: 15.7s\n",
            "3774:\tlearn: 0.9998563\ttotal: 4m 22s\tremaining: 15.7s\n",
            "3775:\tlearn: 0.9998563\ttotal: 4m 22s\tremaining: 15.6s\n",
            "3776:\tlearn: 0.9998563\ttotal: 4m 23s\tremaining: 15.5s\n",
            "3777:\tlearn: 0.9998563\ttotal: 4m 23s\tremaining: 15.5s\n",
            "3778:\tlearn: 0.9998563\ttotal: 4m 23s\tremaining: 15.4s\n",
            "3779:\tlearn: 0.9998563\ttotal: 4m 23s\tremaining: 15.3s\n",
            "3780:\tlearn: 0.9998563\ttotal: 4m 23s\tremaining: 15.3s\n",
            "3781:\tlearn: 0.9998563\ttotal: 4m 23s\tremaining: 15.2s\n",
            "3782:\tlearn: 0.9998563\ttotal: 4m 23s\tremaining: 15.1s\n",
            "3783:\tlearn: 0.9998563\ttotal: 4m 23s\tremaining: 15s\n",
            "3784:\tlearn: 0.9998563\ttotal: 4m 23s\tremaining: 15s\n",
            "3785:\tlearn: 0.9998563\ttotal: 4m 23s\tremaining: 14.9s\n",
            "3786:\tlearn: 0.9998563\ttotal: 4m 23s\tremaining: 14.8s\n",
            "3787:\tlearn: 0.9998563\ttotal: 4m 23s\tremaining: 14.8s\n",
            "3788:\tlearn: 0.9998563\ttotal: 4m 23s\tremaining: 14.7s\n",
            "3789:\tlearn: 0.9998563\ttotal: 4m 23s\tremaining: 14.6s\n",
            "3790:\tlearn: 0.9998563\ttotal: 4m 23s\tremaining: 14.6s\n",
            "3791:\tlearn: 0.9998563\ttotal: 4m 24s\tremaining: 14.5s\n",
            "3792:\tlearn: 0.9998563\ttotal: 4m 24s\tremaining: 14.4s\n",
            "3793:\tlearn: 0.9998563\ttotal: 4m 24s\tremaining: 14.3s\n",
            "3794:\tlearn: 0.9998563\ttotal: 4m 24s\tremaining: 14.3s\n",
            "3795:\tlearn: 0.9998563\ttotal: 4m 24s\tremaining: 14.2s\n",
            "3796:\tlearn: 0.9998563\ttotal: 4m 24s\tremaining: 14.1s\n",
            "3797:\tlearn: 0.9998563\ttotal: 4m 24s\tremaining: 14.1s\n",
            "3798:\tlearn: 0.9998563\ttotal: 4m 24s\tremaining: 14s\n",
            "3799:\tlearn: 0.9998563\ttotal: 4m 24s\tremaining: 13.9s\n",
            "3800:\tlearn: 0.9998563\ttotal: 4m 24s\tremaining: 13.9s\n",
            "3801:\tlearn: 0.9998563\ttotal: 4m 24s\tremaining: 13.8s\n",
            "3802:\tlearn: 0.9998563\ttotal: 4m 24s\tremaining: 13.7s\n",
            "3803:\tlearn: 0.9998563\ttotal: 4m 24s\tremaining: 13.6s\n",
            "3804:\tlearn: 0.9998563\ttotal: 4m 24s\tremaining: 13.6s\n",
            "3805:\tlearn: 0.9998563\ttotal: 4m 24s\tremaining: 13.5s\n",
            "3806:\tlearn: 0.9998563\ttotal: 4m 25s\tremaining: 13.4s\n",
            "3807:\tlearn: 0.9998563\ttotal: 4m 25s\tremaining: 13.4s\n",
            "3808:\tlearn: 0.9998563\ttotal: 4m 25s\tremaining: 13.3s\n",
            "3809:\tlearn: 0.9998563\ttotal: 4m 25s\tremaining: 13.2s\n",
            "3810:\tlearn: 0.9998563\ttotal: 4m 25s\tremaining: 13.2s\n",
            "3811:\tlearn: 0.9998563\ttotal: 4m 25s\tremaining: 13.1s\n",
            "3812:\tlearn: 0.9998563\ttotal: 4m 25s\tremaining: 13s\n",
            "3813:\tlearn: 0.9998563\ttotal: 4m 25s\tremaining: 12.9s\n",
            "3814:\tlearn: 0.9998563\ttotal: 4m 25s\tremaining: 12.9s\n",
            "3815:\tlearn: 0.9998563\ttotal: 4m 25s\tremaining: 12.8s\n",
            "3816:\tlearn: 0.9998563\ttotal: 4m 25s\tremaining: 12.7s\n",
            "3817:\tlearn: 0.9998563\ttotal: 4m 25s\tremaining: 12.7s\n",
            "3818:\tlearn: 0.9998563\ttotal: 4m 25s\tremaining: 12.6s\n",
            "3819:\tlearn: 0.9998563\ttotal: 4m 25s\tremaining: 12.5s\n",
            "3820:\tlearn: 0.9998563\ttotal: 4m 25s\tremaining: 12.5s\n",
            "3821:\tlearn: 0.9998563\ttotal: 4m 26s\tremaining: 12.4s\n",
            "3822:\tlearn: 0.9998563\ttotal: 4m 26s\tremaining: 12.3s\n",
            "3823:\tlearn: 0.9998563\ttotal: 4m 26s\tremaining: 12.3s\n",
            "3824:\tlearn: 0.9998563\ttotal: 4m 26s\tremaining: 12.2s\n",
            "3825:\tlearn: 0.9998563\ttotal: 4m 26s\tremaining: 12.1s\n",
            "3826:\tlearn: 0.9998563\ttotal: 4m 26s\tremaining: 12s\n",
            "3827:\tlearn: 0.9998563\ttotal: 4m 26s\tremaining: 12s\n",
            "3828:\tlearn: 0.9998563\ttotal: 4m 26s\tremaining: 11.9s\n",
            "3829:\tlearn: 0.9998563\ttotal: 4m 26s\tremaining: 11.8s\n",
            "3830:\tlearn: 0.9998563\ttotal: 4m 26s\tremaining: 11.8s\n",
            "3831:\tlearn: 0.9998563\ttotal: 4m 26s\tremaining: 11.7s\n",
            "3832:\tlearn: 0.9998563\ttotal: 4m 26s\tremaining: 11.6s\n",
            "3833:\tlearn: 0.9998563\ttotal: 4m 26s\tremaining: 11.6s\n",
            "3834:\tlearn: 0.9998563\ttotal: 4m 26s\tremaining: 11.5s\n",
            "3835:\tlearn: 0.9998563\ttotal: 4m 26s\tremaining: 11.4s\n",
            "3836:\tlearn: 0.9998563\ttotal: 4m 27s\tremaining: 11.3s\n",
            "3837:\tlearn: 0.9998563\ttotal: 4m 27s\tremaining: 11.3s\n",
            "3838:\tlearn: 0.9998563\ttotal: 4m 27s\tremaining: 11.2s\n",
            "3839:\tlearn: 0.9998563\ttotal: 4m 27s\tremaining: 11.1s\n",
            "3840:\tlearn: 0.9998563\ttotal: 4m 27s\tremaining: 11.1s\n",
            "3841:\tlearn: 0.9998563\ttotal: 4m 27s\tremaining: 11s\n",
            "3842:\tlearn: 0.9998563\ttotal: 4m 27s\tremaining: 10.9s\n",
            "3843:\tlearn: 0.9998563\ttotal: 4m 27s\tremaining: 10.9s\n",
            "3844:\tlearn: 0.9998563\ttotal: 4m 27s\tremaining: 10.8s\n",
            "3845:\tlearn: 0.9998563\ttotal: 4m 27s\tremaining: 10.7s\n",
            "3846:\tlearn: 0.9998563\ttotal: 4m 27s\tremaining: 10.6s\n",
            "3847:\tlearn: 0.9998563\ttotal: 4m 27s\tremaining: 10.6s\n",
            "3848:\tlearn: 0.9998563\ttotal: 4m 27s\tremaining: 10.5s\n",
            "3849:\tlearn: 0.9998563\ttotal: 4m 27s\tremaining: 10.4s\n",
            "3850:\tlearn: 0.9998563\ttotal: 4m 27s\tremaining: 10.4s\n",
            "3851:\tlearn: 0.9998563\ttotal: 4m 27s\tremaining: 10.3s\n",
            "3852:\tlearn: 0.9998563\ttotal: 4m 28s\tremaining: 10.2s\n",
            "3853:\tlearn: 0.9998563\ttotal: 4m 28s\tremaining: 10.2s\n",
            "3854:\tlearn: 0.9998563\ttotal: 4m 28s\tremaining: 10.1s\n",
            "3855:\tlearn: 0.9998563\ttotal: 4m 28s\tremaining: 10s\n",
            "3856:\tlearn: 0.9998563\ttotal: 4m 28s\tremaining: 9.95s\n",
            "3857:\tlearn: 0.9998563\ttotal: 4m 28s\tremaining: 9.88s\n",
            "3858:\tlearn: 0.9998563\ttotal: 4m 28s\tremaining: 9.81s\n",
            "3859:\tlearn: 0.9998563\ttotal: 4m 28s\tremaining: 9.74s\n",
            "3860:\tlearn: 0.9998563\ttotal: 4m 28s\tremaining: 9.67s\n",
            "3861:\tlearn: 0.9998563\ttotal: 4m 28s\tremaining: 9.6s\n",
            "3862:\tlearn: 0.9998563\ttotal: 4m 28s\tremaining: 9.53s\n",
            "3863:\tlearn: 0.9998563\ttotal: 4m 28s\tremaining: 9.46s\n",
            "3864:\tlearn: 0.9998563\ttotal: 4m 28s\tremaining: 9.39s\n",
            "3865:\tlearn: 0.9998563\ttotal: 4m 29s\tremaining: 9.32s\n",
            "3866:\tlearn: 0.9998563\ttotal: 4m 29s\tremaining: 9.25s\n",
            "3867:\tlearn: 0.9998563\ttotal: 4m 29s\tremaining: 9.18s\n",
            "3868:\tlearn: 0.9998563\ttotal: 4m 29s\tremaining: 9.12s\n",
            "3869:\tlearn: 0.9998563\ttotal: 4m 29s\tremaining: 9.04s\n",
            "3870:\tlearn: 0.9998563\ttotal: 4m 29s\tremaining: 8.97s\n",
            "3871:\tlearn: 0.9998563\ttotal: 4m 29s\tremaining: 8.91s\n",
            "3872:\tlearn: 0.9998563\ttotal: 4m 29s\tremaining: 8.84s\n",
            "3873:\tlearn: 0.9998563\ttotal: 4m 29s\tremaining: 8.77s\n",
            "3874:\tlearn: 0.9998563\ttotal: 4m 29s\tremaining: 8.7s\n",
            "3875:\tlearn: 0.9998563\ttotal: 4m 29s\tremaining: 8.63s\n",
            "3876:\tlearn: 0.9998563\ttotal: 4m 29s\tremaining: 8.56s\n",
            "3877:\tlearn: 0.9998563\ttotal: 4m 29s\tremaining: 8.49s\n",
            "3878:\tlearn: 0.9998563\ttotal: 4m 29s\tremaining: 8.42s\n",
            "3879:\tlearn: 0.9998563\ttotal: 4m 29s\tremaining: 8.35s\n",
            "3880:\tlearn: 0.9998563\ttotal: 4m 30s\tremaining: 8.28s\n",
            "3881:\tlearn: 0.9998563\ttotal: 4m 30s\tremaining: 8.21s\n",
            "3882:\tlearn: 0.9998563\ttotal: 4m 30s\tremaining: 8.14s\n",
            "3883:\tlearn: 0.9998563\ttotal: 4m 30s\tremaining: 8.07s\n",
            "3884:\tlearn: 0.9998563\ttotal: 4m 30s\tremaining: 8s\n",
            "3885:\tlearn: 0.9998563\ttotal: 4m 30s\tremaining: 7.93s\n",
            "3886:\tlearn: 0.9998563\ttotal: 4m 30s\tremaining: 7.86s\n",
            "3887:\tlearn: 0.9998563\ttotal: 4m 30s\tremaining: 7.79s\n",
            "3888:\tlearn: 0.9998563\ttotal: 4m 30s\tremaining: 7.72s\n",
            "3889:\tlearn: 0.9998563\ttotal: 4m 30s\tremaining: 7.65s\n",
            "3890:\tlearn: 0.9998563\ttotal: 4m 30s\tremaining: 7.58s\n",
            "3891:\tlearn: 0.9998563\ttotal: 4m 30s\tremaining: 7.51s\n",
            "3892:\tlearn: 0.9998563\ttotal: 4m 30s\tremaining: 7.44s\n",
            "3893:\tlearn: 0.9998563\ttotal: 4m 30s\tremaining: 7.37s\n",
            "3894:\tlearn: 0.9998563\ttotal: 4m 30s\tremaining: 7.3s\n",
            "3895:\tlearn: 0.9998563\ttotal: 4m 30s\tremaining: 7.23s\n",
            "3896:\tlearn: 0.9998563\ttotal: 4m 31s\tremaining: 7.16s\n",
            "3897:\tlearn: 0.9998563\ttotal: 4m 31s\tremaining: 7.09s\n",
            "3898:\tlearn: 0.9998563\ttotal: 4m 31s\tremaining: 7.02s\n",
            "3899:\tlearn: 0.9998563\ttotal: 4m 31s\tremaining: 6.95s\n",
            "3900:\tlearn: 0.9998563\ttotal: 4m 31s\tremaining: 6.88s\n",
            "3901:\tlearn: 0.9998563\ttotal: 4m 31s\tremaining: 6.81s\n",
            "3902:\tlearn: 0.9998563\ttotal: 4m 31s\tremaining: 6.74s\n",
            "3903:\tlearn: 0.9998563\ttotal: 4m 31s\tremaining: 6.67s\n",
            "3904:\tlearn: 0.9998563\ttotal: 4m 31s\tremaining: 6.61s\n",
            "3905:\tlearn: 0.9998563\ttotal: 4m 31s\tremaining: 6.54s\n",
            "3906:\tlearn: 0.9998563\ttotal: 4m 31s\tremaining: 6.47s\n",
            "3907:\tlearn: 0.9998563\ttotal: 4m 31s\tremaining: 6.4s\n",
            "3908:\tlearn: 0.9998563\ttotal: 4m 31s\tremaining: 6.33s\n",
            "3909:\tlearn: 0.9998563\ttotal: 4m 31s\tremaining: 6.26s\n",
            "3910:\tlearn: 0.9998563\ttotal: 4m 31s\tremaining: 6.19s\n",
            "3911:\tlearn: 0.9998563\ttotal: 4m 31s\tremaining: 6.12s\n",
            "3912:\tlearn: 0.9998563\ttotal: 4m 32s\tremaining: 6.05s\n",
            "3913:\tlearn: 0.9998563\ttotal: 4m 32s\tremaining: 5.98s\n",
            "3914:\tlearn: 0.9998563\ttotal: 4m 32s\tremaining: 5.91s\n",
            "3915:\tlearn: 0.9998563\ttotal: 4m 32s\tremaining: 5.84s\n",
            "3916:\tlearn: 0.9998563\ttotal: 4m 32s\tremaining: 5.77s\n",
            "3917:\tlearn: 0.9998563\ttotal: 4m 32s\tremaining: 5.7s\n",
            "3918:\tlearn: 0.9998563\ttotal: 4m 32s\tremaining: 5.63s\n",
            "3919:\tlearn: 0.9998563\ttotal: 4m 32s\tremaining: 5.56s\n",
            "3920:\tlearn: 0.9998563\ttotal: 4m 32s\tremaining: 5.49s\n",
            "3921:\tlearn: 0.9998563\ttotal: 4m 32s\tremaining: 5.42s\n",
            "3922:\tlearn: 0.9998563\ttotal: 4m 32s\tremaining: 5.35s\n",
            "3923:\tlearn: 0.9998563\ttotal: 4m 32s\tremaining: 5.28s\n",
            "3924:\tlearn: 0.9998563\ttotal: 4m 32s\tremaining: 5.21s\n",
            "3925:\tlearn: 0.9998563\ttotal: 4m 32s\tremaining: 5.14s\n",
            "3926:\tlearn: 0.9998563\ttotal: 4m 32s\tremaining: 5.07s\n",
            "3927:\tlearn: 0.9998563\ttotal: 4m 33s\tremaining: 5s\n",
            "3928:\tlearn: 0.9998563\ttotal: 4m 33s\tremaining: 4.93s\n",
            "3929:\tlearn: 0.9998563\ttotal: 4m 33s\tremaining: 4.87s\n",
            "3930:\tlearn: 0.9998631\ttotal: 4m 33s\tremaining: 4.79s\n",
            "3931:\tlearn: 0.9998631\ttotal: 4m 33s\tremaining: 4.73s\n",
            "3932:\tlearn: 0.9998631\ttotal: 4m 33s\tremaining: 4.66s\n",
            "3933:\tlearn: 0.9998631\ttotal: 4m 33s\tremaining: 4.59s\n",
            "3934:\tlearn: 0.9998631\ttotal: 4m 33s\tremaining: 4.52s\n",
            "3935:\tlearn: 0.9998631\ttotal: 4m 33s\tremaining: 4.45s\n",
            "3936:\tlearn: 0.9998631\ttotal: 4m 33s\tremaining: 4.38s\n",
            "3937:\tlearn: 0.9998631\ttotal: 4m 33s\tremaining: 4.31s\n",
            "3938:\tlearn: 0.9998631\ttotal: 4m 33s\tremaining: 4.24s\n",
            "3939:\tlearn: 0.9998631\ttotal: 4m 33s\tremaining: 4.17s\n",
            "3940:\tlearn: 0.9998631\ttotal: 4m 33s\tremaining: 4.1s\n",
            "3941:\tlearn: 0.9998631\ttotal: 4m 33s\tremaining: 4.03s\n",
            "3942:\tlearn: 0.9998631\ttotal: 4m 34s\tremaining: 3.96s\n",
            "3943:\tlearn: 0.9998631\ttotal: 4m 34s\tremaining: 3.89s\n",
            "3944:\tlearn: 0.9998631\ttotal: 4m 34s\tremaining: 3.82s\n",
            "3945:\tlearn: 0.9998631\ttotal: 4m 34s\tremaining: 3.75s\n",
            "3946:\tlearn: 0.9998631\ttotal: 4m 34s\tremaining: 3.68s\n",
            "3947:\tlearn: 0.9998631\ttotal: 4m 34s\tremaining: 3.61s\n",
            "3948:\tlearn: 0.9998631\ttotal: 4m 34s\tremaining: 3.54s\n",
            "3949:\tlearn: 0.9998631\ttotal: 4m 34s\tremaining: 3.47s\n",
            "3950:\tlearn: 0.9998631\ttotal: 4m 34s\tremaining: 3.4s\n",
            "3951:\tlearn: 0.9998631\ttotal: 4m 34s\tremaining: 3.33s\n",
            "3952:\tlearn: 0.9998631\ttotal: 4m 34s\tremaining: 3.27s\n",
            "3953:\tlearn: 0.9998631\ttotal: 4m 34s\tremaining: 3.19s\n",
            "3954:\tlearn: 0.9998631\ttotal: 4m 34s\tremaining: 3.13s\n",
            "3955:\tlearn: 0.9998631\ttotal: 4m 34s\tremaining: 3.06s\n",
            "3956:\tlearn: 0.9998631\ttotal: 4m 34s\tremaining: 2.99s\n",
            "3957:\tlearn: 0.9998631\ttotal: 4m 34s\tremaining: 2.92s\n",
            "3958:\tlearn: 0.9998631\ttotal: 4m 35s\tremaining: 2.85s\n",
            "3959:\tlearn: 0.9998631\ttotal: 4m 35s\tremaining: 2.78s\n",
            "3960:\tlearn: 0.9998631\ttotal: 4m 35s\tremaining: 2.71s\n",
            "3961:\tlearn: 0.9998631\ttotal: 4m 35s\tremaining: 2.64s\n",
            "3962:\tlearn: 0.9998631\ttotal: 4m 35s\tremaining: 2.57s\n",
            "3963:\tlearn: 0.9998631\ttotal: 4m 35s\tremaining: 2.5s\n",
            "3964:\tlearn: 0.9998631\ttotal: 4m 35s\tremaining: 2.43s\n",
            "3965:\tlearn: 0.9998631\ttotal: 4m 35s\tremaining: 2.36s\n",
            "3966:\tlearn: 0.9998631\ttotal: 4m 35s\tremaining: 2.29s\n",
            "3967:\tlearn: 0.9998631\ttotal: 4m 35s\tremaining: 2.22s\n",
            "3968:\tlearn: 0.9998631\ttotal: 4m 35s\tremaining: 2.15s\n",
            "3969:\tlearn: 0.9998631\ttotal: 4m 35s\tremaining: 2.08s\n",
            "3970:\tlearn: 0.9998631\ttotal: 4m 35s\tremaining: 2.01s\n",
            "3971:\tlearn: 0.9998631\ttotal: 4m 35s\tremaining: 1.94s\n",
            "3972:\tlearn: 0.9998631\ttotal: 4m 35s\tremaining: 1.87s\n",
            "3973:\tlearn: 0.9998631\ttotal: 4m 35s\tremaining: 1.8s\n",
            "3974:\tlearn: 0.9998631\ttotal: 4m 36s\tremaining: 1.74s\n",
            "3975:\tlearn: 0.9998631\ttotal: 4m 36s\tremaining: 1.67s\n",
            "3976:\tlearn: 0.9998631\ttotal: 4m 36s\tremaining: 1.6s\n",
            "3977:\tlearn: 0.9998631\ttotal: 4m 36s\tremaining: 1.53s\n",
            "3978:\tlearn: 0.9998631\ttotal: 4m 36s\tremaining: 1.46s\n",
            "3979:\tlearn: 0.9998631\ttotal: 4m 36s\tremaining: 1.39s\n",
            "3980:\tlearn: 0.9998631\ttotal: 4m 36s\tremaining: 1.32s\n",
            "3981:\tlearn: 0.9998631\ttotal: 4m 36s\tremaining: 1.25s\n",
            "3982:\tlearn: 0.9998631\ttotal: 4m 36s\tremaining: 1.18s\n",
            "3983:\tlearn: 0.9998631\ttotal: 4m 36s\tremaining: 1.11s\n",
            "3984:\tlearn: 0.9998631\ttotal: 4m 36s\tremaining: 1.04s\n",
            "3985:\tlearn: 0.9998631\ttotal: 4m 36s\tremaining: 972ms\n",
            "3986:\tlearn: 0.9998631\ttotal: 4m 36s\tremaining: 903ms\n",
            "3987:\tlearn: 0.9998631\ttotal: 4m 36s\tremaining: 833ms\n",
            "3988:\tlearn: 0.9998631\ttotal: 4m 36s\tremaining: 764ms\n",
            "3989:\tlearn: 0.9998631\ttotal: 4m 37s\tremaining: 694ms\n",
            "3990:\tlearn: 0.9998631\ttotal: 4m 37s\tremaining: 625ms\n",
            "3991:\tlearn: 0.9998631\ttotal: 4m 37s\tremaining: 555ms\n",
            "3992:\tlearn: 0.9998631\ttotal: 4m 37s\tremaining: 486ms\n",
            "3993:\tlearn: 0.9998631\ttotal: 4m 37s\tremaining: 417ms\n",
            "3994:\tlearn: 0.9998631\ttotal: 4m 37s\tremaining: 347ms\n",
            "3995:\tlearn: 0.9998631\ttotal: 4m 37s\tremaining: 278ms\n",
            "3996:\tlearn: 0.9998631\ttotal: 4m 37s\tremaining: 208ms\n",
            "3997:\tlearn: 0.9998631\ttotal: 4m 37s\tremaining: 139ms\n",
            "3998:\tlearn: 0.9998631\ttotal: 4m 37s\tremaining: 69.4ms\n",
            "3999:\tlearn: 0.9998631\ttotal: 4m 37s\tremaining: 0us\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Кажется, он достиг предела своего великолепия на 2860-м дереве"
      ],
      "metadata": {
        "id": "M3UnbHYSiV0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f1_7 = float(f1_m(Y_test, K.round(predictions_cat)))\n",
        "print(f1_7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9c475k_ghA7",
        "outputId": "1ae5b9e3-9929-450f-9f12-3125d09a46e5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9684813022613525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1_7 = 0.9684813022613525"
      ],
      "metadata": {
        "id": "IlXMVYINlgza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvlliUShlVWk"
      },
      "source": [
        "Большое количество деревьев творит чудеса"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJOGzzXLGn2v"
      },
      "source": [
        "K ближайших соседей"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yS9x5pixatLD",
        "outputId": "ac5094ed-4db1-4ec1-dcdc-f184ed134e86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7970627546310425\n"
          ]
        }
      ],
      "source": [
        "neigh_5 = KNeighborsClassifier(n_neighbors=5)\n",
        "neigh_5.fit(X_train, Y_train)\n",
        "f1_8 = float(f1_m(Y_test, K.round(neigh_5.predict_proba(X_test)[:,1])))\n",
        "print(f1_8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TyoVHAlbkD9",
        "outputId": "0287066a-8525-4495-e1a1-fdedf1878f68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.787982165813446\n"
          ]
        }
      ],
      "source": [
        "neigh_2 = KNeighborsClassifier(n_neighbors=2)\n",
        "neigh_2.fit(X_train, Y_train)\n",
        "f1_8 = float(f1_m(Y_test, K.round(neigh_2.predict_proba(X_test)[:,1])))\n",
        "print(f1_8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzOmCHXWb2I6",
        "outputId": "356c1e8b-0d46-4b84-8872-201ec41db43e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8346951007843018\n"
          ]
        }
      ],
      "source": [
        "neigh_ = KNeighborsClassifier(n_neighbors=3, weights='distance')\n",
        "neigh_.fit(X_train, Y_train)\n",
        "f1_8 = float(f1_m(Y_test, K.round(neigh_.predict_proba(X_test)[:,1])))\n",
        "print(f1_8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5a7CBseGsQx",
        "outputId": "07974854-ed99-4c3a-e7d8-5164ab1ce227"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8346951007843018\n"
          ]
        }
      ],
      "source": [
        "neigh = KNeighborsClassifier(n_neighbors=3) # uniform\n",
        "neigh.fit(X_train, Y_train)\n",
        "f1_8 = float(f1_m(Y_test, K.round(neigh.predict_proba(X_test)[:,1])))\n",
        "print(f1_8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItydsU0EGss2"
      },
      "source": [
        "Деревья решений"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8qgAT5VGuzm",
        "outputId": "dc8d90e7-4dee-4c82-81a1-f925e719ce56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8192205429077148\n"
          ]
        }
      ],
      "source": [
        "tree = tree.DecisionTreeClassifier(class_weight=class_weight, random_state=42)\n",
        "tree.fit(X_train, Y_train)\n",
        "f1_9 = float(f1_m(Y_test, K.round(tree.predict_proba(X_test)[:,1])))\n",
        "print(f1_9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnmsArHICUCX"
      },
      "source": [
        "Наивный байесовский классификатор"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDLfXRj6WNIC",
        "outputId": "1333a6f6-a258-45cd-9678-341ed4b41ad3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.1783931702375412\n"
          ]
        }
      ],
      "source": [
        "nb = GaussianNB()\n",
        "nb.fit(X_train, Y_train)\n",
        "f1_10 = float(f1_m(Y_test, K.round(nb.predict_proba(X_test)[:,1])))\n",
        "print(f1_10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOexBALKRbsg"
      },
      "source": [
        "Экстремальные деревья. Он здесь преимущественно для того, чтобы использовать его в голосующем классификаторе ниже"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MiwBxZvoxwN",
        "outputId": "4721dddb-0b83-48ce-c980-aa8d0067b33f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7623796463012695\n"
          ]
        }
      ],
      "source": [
        "etc = ExtraTreesClassifier(n_estimators=200, class_weight=class_weight, \n",
        "                           random_state=42)\n",
        "etc.fit(X_train, Y_train)\n",
        "f1_11 = float(f1_m(Y_test, K.round(etc.predict_proba(X_test)[:,1])))\n",
        "print(f1_11)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfV6GBnJgx9f"
      },
      "source": [
        "Попробуем последний эксперимент: сбалансированный лес"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANId3LIRg2HL",
        "outputId": "b0b7aa22-4474-4311-907c-acb560cd3798"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BalancedRandomForestClassifier(class_weight={0: 0.5474352657798215,\n",
              "                                             1: 5.770340450086555},\n",
              "                               n_estimators=500, random_state=42)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "model_BRF = imblearn.ensemble.BalancedRandomForestClassifier(n_estimators=500, \n",
        "                                                             class_weight=class_weight, \n",
        "                                                             random_state=42)\n",
        "model_BRF.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nD88ACY5hwdu",
        "outputId": "dc278354-3e05-4238-b6f9-76e16011f799"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8771738409996033\n"
          ]
        }
      ],
      "source": [
        "f1_15 = float(f1_m(Y_test, K.round(model_BRF.predict_proba(X_test)[:,1])))\n",
        "print(f1_15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKIRLKD1aW_p"
      },
      "source": [
        "Лучше, чем взвешенный лес ранее"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knz3TmGUVZl-"
      },
      "source": [
        "Можно попробовать сделать ансамбль из нескольких простых классификаторов. В целом, не самые плохие результаты показывают алгоритм k ближайших соседей, одинокое дерево. Может, вместе они повысят качество результата?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Mtf1GnTwtVD"
      },
      "outputs": [],
      "source": [
        "boosting_model._estimator_type = \"classifier\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zdEj_kRxeyS",
        "outputId": "058b9297-e808-4f22-82b9-09503be7c31c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "getattr(tree, \"_estimator_type\", None) == \"classifier\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# если не хочется запускать коды выше\n",
        "neigh = KNeighborsClassifier(n_neighbors=3) \n",
        "tree = tree.DecisionTreeClassifier(class_weight=class_weight, random_state=42)\n",
        "etc = ExtraTreesClassifier(n_estimators=200, class_weight=class_weight, \n",
        "                           random_state=42)\n",
        "model_BRF = imblearn.ensemble.BalancedRandomForestClassifier(n_estimators=500, \n",
        "                                                             class_weight=class_weight, \n",
        "                                                             random_state=42)\n",
        "svm = SVC(kernel='rbf', probability=True, \n",
        "          class_weight=class_weight, random_state=42) "
      ],
      "metadata": {
        "id": "y-2_vBg0i2h6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "XyCiY71FVyGK"
      },
      "outputs": [],
      "source": [
        "estimators_lst = [('dt', tree), ('knn', neigh), ('etc', etc), ('brf', model_BRF), ('cat', boosting_model)]\n",
        "# estimators_lst = [('dt', tree), ('knn', neigh), ('etc', etc), ('brf', model_BRF), ('svm', svm)]\n",
        "\n",
        "voting_hard = VotingClassifier(estimators=estimators_lst, voting='hard')\n",
        "voting_soft = VotingClassifier(estimators=estimators_lst, voting='soft')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "v_vTHywXV_Jg"
      },
      "outputs": [],
      "source": [
        "for model in [voting_hard, voting_soft]:\n",
        "    model.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Наличие метода опорных векторов ощутимо замедляет работу данного ансамбля... (прошло три часа)"
      ],
      "metadata": {
        "id": "aZtzVnP9LMHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f1_13 = float(f1_m(Y_test, K.round(voting_hard.predict(X_test))))\n",
        "print(f1_13)"
      ],
      "metadata": {
        "id": "W4ypXxVOmMNA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1425606a-9e72-48da-8d79-e8ee2785fd3c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9351514577865601\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1_14 = float(f1_m(Y_test, K.round(voting_soft.predict_proba(X_test)[:,1])))\n",
        "print(f1_14)"
      ],
      "metadata": {
        "id": "6DOe0EBJmNqP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "643c3b59-4b9e-43b7-c1ea-390a73862c58"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.931991457939148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Результат ансамбля ниже, чем для отдельного метода опорных векторов, входящего в него.\n",
        "\n",
        "Ниже пример результата с недообученным градиентным бустингом вместо svm."
      ],
      "metadata": {
        "id": "2CBrhU43Ru0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# estimators_lst = [('dt', tree), ('knn', neigh), ('etc', etc), ('brf', model_BRF), ('cat', boosting_model)] # catboost с 300 деревьями"
      ],
      "metadata": {
        "id": "nAROp2W2RcPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWHRjfQ1yG8r",
        "outputId": "506deabb-efbd-4303-b5ee-24cce26fda9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9322853088378906\n"
          ]
        }
      ],
      "source": [
        "f1_13 = float(f1_m(Y_test, K.round(voting_hard.predict(X_test))))\n",
        "print(f1_13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5MrW1TByJNt",
        "outputId": "90fb0d5b-1653-44be-8c58-2ac6e9a56f4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9274217486381531\n"
          ]
        }
      ],
      "source": [
        "f1_14 = float(f1_m(Y_test, K.round(voting_soft.predict_proba(X_test)[:,1])))\n",
        "print(f1_14)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvPqH5Mi9iBU"
      },
      "source": [
        "В целом, получилось неплохо, но, возможно, не стоило делать ансамбль из очень похожих по сути классификаторов. 4 из 5 основаны на деревьях решений. Но скажу, что добавление несбалансированного леса и затем кат. бустинга увеличило последовательно качество классификатора."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A1mXn7TWv_S"
      },
      "source": [
        "Итоговая оценка"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BChdN8QOqWc"
      },
      "source": [
        "Из лучших выбраны полносвязная нейронная сеть, метод опорных векторов, а также градиентный бустинг от Яндекса."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHOw2O58WxqA",
        "outputId": "80e632de-a1d8-443d-eb66-8987190cb3f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9750153422355652 0.9503176808357239 0.9684813022613525\n",
            "Средняя f1 мера составляет 0.9646047751108805\n",
            "Количество баллов равно 21.460477511088058\n"
          ]
        }
      ],
      "source": [
        "print(f1_1, f1_5, f1_7)\n",
        "print(\"Средняя f1 мера составляет\", (f1_1+f1_5+f1_7)/3)\n",
        "print(\"Количество баллов равно\", max(min(22,(f1_1+f1_5+f1_7)*100/3-75),0))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}